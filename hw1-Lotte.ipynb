{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "758d05f39ab671358f17858c55f7161c",
     "grade": false,
     "grade_id": "cell-c9cd9e550239e812",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 1 (Total Points: 260) <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "\n",
    "**Submission instructions**:\n",
    "- Only the code `TODO: Implement this!` denotes that these sections are graded.\n",
    "- For Part 1: You can use the `nltk`, `numpy` and `matplotlib` libraries here. Other libraries, e.g., `gensim` or `scikit-learn`, may not be used. For Part 2: `gensim` is allowed in addition to the imported libraries in the next code cell\n",
    "- The notebook you submit has to have the student ids, separated by underscores (E.g., `12341234_12341234_12341234.ipynb`). \n",
    "- This will be parsed by a regexp, so please double check your filename.\n",
    "- Only one member of each group has to submit the file to canvas.\n",
    "- Make sure to check that your notebook runs before submission. A quick way to do this is to restart the kernel and run all the cells.  \n",
    "- Please do not delete/add new cells. Removing cells can lead to grade deduction.\n",
    "- Note, that you are not allowed to use Google Colab.\n",
    "\n",
    "\n",
    "**Learning Goals**:\n",
    "- [Part 1, Term-based matching](#part1) (175 points):\n",
    "    - Learn how to load a dataset and process it.\n",
    "    - Learn how to implement several standard IR methods (TF-IDF, BM25, QL) and understand their weaknesses & strengths.\n",
    "    - Learn how to evaluate IR methods.\n",
    "- [Part 2, Semantic-based matching](#part2) (85 points):\n",
    "    - Learn how to implement vector-space retrieval methods (LSI, LDA).\n",
    "    - Learn how to use LSI and LDA for re-ranking\n",
    "\n",
    "    \n",
    "**Resources**: \n",
    "- **Part 1**: Sections 2.3, 4.1, 4.2, 4.3, 5.3, 5.6, 5.7, 6.2, 7, 8 of [Search Engines: Information Retrieval in Practice](https://ciir.cs.umass.edu/downloads/SEIRiP.pdf)\n",
    "- **Part 2**: [LSI - Chapter 18](https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf) from [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/) book and the [original LDA paper](https://jmlr.org/papers/volume3/blei03a/blei03a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c55bfe94ff1f564dd595547e516c4c6e",
     "grade": false,
     "grade_id": "cell-f5357fabdb9660e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# imports \n",
    "# TODO: Ensure that no additional library is imported in the notebook. \n",
    "# TODO: Only the standard library and the following libraries are allowed:\n",
    "# TODO: You can also use unlisted classes from these libraries or standard libraries (such as defaultdict, Counter, ...).\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from functools import partial\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "#from IPython.html import widgets\n",
    "from collections import namedtuple\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ef0139c6fbc22b41520721a9275c1e1",
     "grade": false,
     "grade_id": "cell-7428e12ed184408b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Part 1: Term-based Matching (175 points) <a class=\"anchor\" id=\"part1\"></a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In the first part, we will learn the basics of IR from loading and preprocessing the material, to implementing some well known search algorithms, to evaluating the ranking performance of the implemented algorithms. We will be using the CACM dataset throughout the assignment. The CACM dataset is a collection of titles and abstracts from the journal CACM (Communication of the ACM).\n",
    "\n",
    "Table of contents:\n",
    "- [Section 1: Text Processing](#text_processing) (15 points)\n",
    "- [Section 2: Indexing](#indexing) (10 points)\n",
    "- [Section 3: Ranking](#ranking) (80 points)\n",
    "- [Section 4: Evaluation](#evaluation) (40 points)\n",
    "- [Section 5: Analysis](#analysis) (30 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9050c97b011a926b9e4cf6831eff5bd",
     "grade": false,
     "grade_id": "cell-4b24825cf4ae55ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Section 1: Text Processing (15 points)<a class=\"anchor\" id=\"text_processing\"></a>\n",
    "\n",
    "[Back to Part 1](#part1)\n",
    "\n",
    "In this section, we will load the dataset and learn how to clean up the data to make it usable for an IR system. \n",
    "The points of this section are earned for the following implementations:\n",
    "- `read_cacm_docs` (4 points): Reads in the CACM documents.\n",
    "- `read_queries` (3 points): Reads in the CACM queries.\n",
    "- `load_stopwords` (1 point): Loads the stopwords.\n",
    "- `tokenize` (4 points): Tokenizes the input text.\n",
    "- `stem_token` (3 points): Stems the given token. \n",
    "\n",
    "We are using the [CACM dataset](http://ir.dcs.gla.ac.uk/resources/test_collections/cacm/), which is a small, classic IR dataset, composed of a collection of titles and abstracts from the journal CACM. It comes with relevance judgements for queries, so we can evaluate our IR system. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1de1b97f9ee233ad2348359f0c158eb7",
     "grade": false,
     "grade_id": "cell-45651364e7af6d5a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 1.1 Read the CACM documents (4 points)\n",
    "\n",
    "\n",
    "The following cell downloads the dataset and unzips it to a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d43c9ad6e77cc01ce4cef0c34824930",
     "grade": false,
     "grade_id": "cell-bbc3030bb3fe7e02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    folder_path = os.environ.get(\"IR1_DATA_PATH\")\n",
    "    if not folder_path:\n",
    "        folder_path = \"./datasets/\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    file_location = os.path.join(folder_path, \"cacm.zip\")\n",
    "    \n",
    "    # download file if it doesn't exist\n",
    "    if not os.path.exists(file_location):\n",
    "        \n",
    "        url = \"https://surfdrive.surf.nl/files/index.php/s/M0FGJpX2p8wDwxR/download\"\n",
    "\n",
    "        with open(file_location, \"wb\") as handle:\n",
    "            print(f\"Downloading file from {url} to {file_location}\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            for data in tqdm(response.iter_content()):\n",
    "                handle.write(data)\n",
    "            print(\"Finished downloading file\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(folder_path, \"train.txt\")):\n",
    "        \n",
    "        # unzip file\n",
    "        with zipfile.ZipFile(file_location, 'r') as zip_ref:\n",
    "            zip_ref.extractall(folder_path)\n",
    "        \n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31609b0d61d0c74cbd69bc43e47c23be",
     "grade": false,
     "grade_id": "cell-a7dd9a9bf98ede05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "You can see a brief description of each file in the dataset by looking at the README file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1783f9999c7d06955e8da815de62ba5f",
     "grade": false,
     "grade_id": "cell-9b6ff1a17124711f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in this directory with sizes:\r\n",
      "          0 Jun 19 21:01 README\r\n",
      "\r\n",
      "    2187734 Jun 19 20:55 cacm.all              text of documents\r\n",
      "        626 Jun 19 20:58 cite.info             key to citation info\r\n",
      "                                                (the X sections in cacm.all)\r\n",
      "       2668 Jun 19 20:55 common_words           stop words used by smart\r\n",
      "       2194 Jun 19 20:55 make_coll*             shell script to make collection\r\n",
      "       1557 Jun 19 20:55 make_coll_term*        ditto (both useless without\r\n",
      "                                                smart system)\r\n",
      "       9948 Jun 19 20:55 qrels.text             relation giving\r\n",
      "                                                    qid did 0 0\r\n",
      "                                                to indicate dument did is\r\n",
      "                                                relevant to query qid\r\n",
      "      13689 Jun 19 20:55 query.text             Original text of the query\r\n"
     ]
    }
   ],
   "source": [
    "##### Read the README file \n",
    "!cat ./datasets/README\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6a5333ae8e7f69d81900e714cc43852",
     "grade": false,
     "grade_id": "cell-73351431869fda76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "We are interested in 4 files:\n",
    "- `cacm.all` : Contains the text for all documents. Note that some documents do not have abstracts available. \n",
    "- `query.text` : The text of all queries\n",
    "- `qrels.text` : The relevance judgements\n",
    "- `common_words` : A list of common words. This may be used as a collection of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c78e4256a08889ce3d0d985ea799b0a9",
     "grade": false,
     "grade_id": "cell-b44dd14079f278ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".I 1\r\n",
      ".T\r\n",
      "Preliminary Report-International Algebraic Language\r\n",
      ".B\r\n",
      "CACM December, 1958\r\n",
      ".A\r\n",
      "Perlis, A. J.\r\n",
      "Samelson,K.\r\n",
      ".N\r\n",
      "CA581203 JB March 22, 1978  8:28 PM\r\n",
      ".X\r\n",
      "100\t5\t1\r\n",
      "123\t5\t1\r\n",
      "164\t5\t1\r\n",
      "1\t5\t1\r\n",
      "1\t5\t1\r\n",
      "1\t5\t1\r\n",
      "205\t5\t1\r\n",
      "210\t5\t1\r\n",
      "214\t5\t1\r\n",
      "1982\t5\t1\r\n",
      "398\t5\t1\r\n",
      "642\t5\t1\r\n",
      "669\t5\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "165\t6\t1\r\n",
      "196\t6\t1\r\n",
      "196\t6\t1\r\n",
      "1273\t6\t1\r\n",
      "1883\t6\t1\r\n",
      "324\t6\t1\r\n",
      "43\t6\t1\r\n",
      "53\t6\t1\r\n",
      "91\t6\t1\r\n",
      "410\t6\t1\r\n",
      "3184\t6\t1\r\n"
     ]
    }
   ],
   "source": [
    "##### The first 45 lines of the CACM dataset forms the first record\n",
    "# We are interested only in 3 fields. \n",
    "# 1. the '.I' field, which is the document id\n",
    "# 2. the '.T' field (the title) and\n",
    "# 3. the '.W' field (the abstract, which may be absent)\n",
    "!head -45 ./datasets/cacm.all\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aff7cbe5a15a0e3329dc223e3e31abf3",
     "grade": false,
     "grade_id": "cell-c4bf2e263ec553d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "**Implementation (4 points):**\n",
    "Write a function to read the `cacm.all` file. Note that each document has a variable number of lines. The `.I` field denotes a new document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45fef5d5b543ee439176d7fd0a9d20be",
     "grade": false,
     "grade_id": "cell-b736116eb419c624",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (4 points)\n",
    "def read_cacm_docs(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads in the CACM documents. The dataset is assumed to be in the folder \"./datasets/\" by default\n",
    "        Returns: A list of 2-tuples: (doc_id, document), where 'document' is a single string created by \n",
    "            appending the title and abstract (separated by a \"\\n\"). \n",
    "            In case the record doesn't have an abstract, the document is composed only by the title\n",
    "    \"\"\"\n",
    "    \n",
    "    # Path to cacm.all\n",
    "    path = root_folder + \"cacm.all\"\n",
    "    \n",
    "    # Specify lists for output\n",
    "    doc_id = []\n",
    "    doc_title = []\n",
    "    document_list = []\n",
    "    \n",
    "    # Open and split document into tokens\n",
    "    with open(path, 'r') as file:\n",
    "        doclist = file.read().split(\"\\n\")\n",
    "        \n",
    "    # Set counter for tracking index tracking\n",
    "    counter = 0\n",
    "    \n",
    "    # Loop through doc to get I., T. and W.\n",
    "    for i in range(len(doclist)):\n",
    "        if doclist[i][0:2] == \".I\":\n",
    "            counter += 1\n",
    "            doc_id.append(int(doclist[i][3:]))\n",
    "        if doclist[i] == \".T\":\n",
    "            doc_title.append(doclist[i+1])\n",
    "            j = i+2\n",
    "            while doclist[j] not in [\".B\", \".A\", \".X\"]:\n",
    "                if doclist[j] == '.W':\n",
    "                    j += 1\n",
    "                    doc_title[counter-1] += \" \\n\"\n",
    "                    continue\n",
    "                else:\n",
    "                    doc_title[counter-1] += \" \" + doclist[j]\n",
    "                    j += 1\n",
    "    \n",
    "    return list(zip(doc_id, doc_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Preliminary Report-International Algebraic Language'),\n",
       " (2, 'Extraction of Roots by Repeated Subtractions for Digital Computers'),\n",
       " (3, 'Techniques Department on Matrix Program Schemes'),\n",
       " (4, 'Glossary of Computer Engineering and Programming Terminology'),\n",
       " (5, 'Two Square-Root Approximations'),\n",
       " (6, 'The Use of Computers in Inspection Procedures'),\n",
       " (7, 'Glossary of Computer Engineering and Programming Terminology'),\n",
       " (8, 'On The Equivalence and Transformation of Program Schemes'),\n",
       " (9, 'Proposal for an UNCOL'),\n",
       " (10, 'Glossary of Computer Engineering and Programming Terminology'),\n",
       " (11,\n",
       "  'The Problem of Programming Communication with Changing Machines A Proposed Solution-Part 2'),\n",
       " (12, 'Error Estimation in Runge-Kutta Procedures'),\n",
       " (13, 'Glossary of Computer Engineering and Programming Terminology'),\n",
       " (14,\n",
       "  'The Problem of Programming Communication with Changing Machines A Proposed Solution (Part 1)'),\n",
       " (15, 'Recursive Curve Fitting Technique'),\n",
       " (16, \"Secant Modification of Newton's Method\"),\n",
       " (17, 'On Programming of Arithmetic Operations'),\n",
       " (18, 'Simple Automatic Coding Systems'),\n",
       " (19, 'Glossary of Computer Engineering and Programming Terminology'),\n",
       " (20,\n",
       "  'Accelerating Convergence of Iterative Processes \\n A technique is discussed which, when applied to an iterative procedure for the solution of an equation, accelerates the rate of convergence if the iteration converges and induces convergence if the iteration diverges.  An illustrative example is given.'),\n",
       " (21, 'Algebraic Formulation of Flow Diagrams'),\n",
       " (22,\n",
       "  'Unusual Applications Department--Automatic Implementation of Computer Logic'),\n",
       " (23,\n",
       "  'Binary and Truth-Function Operations on a Decimal Computer with an Extract Command'),\n",
       " (24, 'An Improved Decimal Redundancy Check'),\n",
       " (25, 'General Purpose Programming Systems'),\n",
       " (26, 'A Subroutine Method for Calculating Logarithms'),\n",
       " (27, 'Note On Empirical Bounds For Generating Bessel Functions'),\n",
       " (28, 'Request for Methods or Programs'),\n",
       " (29, 'Need for an Algorithm'),\n",
       " (30,\n",
       "  'Algorithm for Analyzing Logical Statements to Produce a Truth Function Table'),\n",
       " (31, 'IBM 704 Code-Nundrums'),\n",
       " (32, 'Variable-Width Tables with Binary-Search Facility'),\n",
       " (33, 'A Programmed Binary Counter For The IBM Type 650 Calculator'),\n",
       " (34, 'Tables for Automatic Computation'),\n",
       " (35, 'A Machine Method for Square-Root Computation'),\n",
       " (36, 'A Queue Network Simulator for the IBM 650 and Burroughs 220'),\n",
       " (37, 'Impact of Computer Developments'),\n",
       " (38, 'A Proposed Interpretation in ALGOL'),\n",
       " (39,\n",
       "  'The Secant Method for Simultaneous Nonlinear Equations  \\n A procedure for the simultaneous solution of a system of not-necessarily-linear equations,  a generalization of the secant method for a single function of one variable, is given.'),\n",
       " (40,\n",
       "  'Fingers or Fists? (The Choice of Decimal or Binary Representation) \\n The binary number system offers many advantages over a decimal representation for a high-performance,  general-purpose computer.  The greater simplicity of a binary arithmetic unit and the greater compactness  of binary numbers both contribute directly to arithmetic speed.  Less obvious and perhaps more important  is the way binary addressing and instruction formats can increase the overall performance.  Binary addresses  are also essential to certain powerful operations which are not practical with decimal instruction formats.   On the other hand, decimal numbers are essential for communicating between man and the computer.  In  applications requiring the processing of a large volume of inherently decimal input and output data,  the time for decimal-binary conversion needed by a purely binary computer may be significant.  A slower  decimal adder may take less time than a fast binary adder doing an addition and two conversions.  A careful  review of the significance of decimal and binary addressing and both binary and decimal data arithmetic,  supplemented by efficient conversion instructions.'),\n",
       " (41, 'Some Notes on Computer Research in Eastern Europe'),\n",
       " (42, 'A New Method of Computation of Square Roots Without Using Division'),\n",
       " (43, 'A Technique for Handling Macro Instructions'),\n",
       " (44, 'RUNCIBLE-Algebraic Translation on a Limited Computer'),\n",
       " (45, 'Flow Outlining-A Substitute for Flow Charting'),\n",
       " (46,\n",
       "  'Multiprogramming STRETCH: Feasibility Considerations \\n The tendency towards increased parallelism in computers is noted.  Exploitation of this parallelism  presents a number of new problems in machine design and in programming systems.  Minimum requirements  for successful concurrent execution of several independent problem programs are discussed.  These requirements  are met in the STRETCH system by a carefully balanced combination of built-in and programmed logic.   Techniques are described which place the burden of the programmed logic on system programs (supervisory  program and compiler) rather than on problem programs.'),\n",
       " (47, 'Russian Visit to U.S. Computers'),\n",
       " (48,\n",
       "  'Shift-Register Code for Indexing Applications \\n In this communication the use of a shift-register code with n = 10 is described for calling  64 wireless telemetering stations in a fixed cyclical order.  A high degree of redundancy is used, permitting  a single-error correcting code (\"minimum-distance-three\" code) with 64 10-bit code words to be employed  as the station identification code.  Embedding this in the shift-register code with period 1023 permits  the code to be employed without punctuation, each of the telemetering station receivers simply putting  received ones and zeros into a shift register.  Each time the given code combination arises identifying  the particular station (barring for tuitous error combinations of very low probability) it has been called.   The communication describes the properties and application of the code in some detail and the finding  of the particular example to be employed on URAL, the Soviet-built drum computer donated to the Indian  Statistical Institute by the United Nations Technical Aid Administration (UNTAA).'),\n",
       " (49, 'Scientific and Business Applications (Oracle Curve Plotter)'),\n",
       " (50, 'Statistical Programs for the IBM 650-Part II'),\n",
       " (51, 'On the Construction of Micro-Flowcharts'),\n",
       " (52,\n",
       "  'An Efficient Method for Generating Uniformly Distributed Points on the Surface on an n-Dimensional  Sphere (Corrigendum)'),\n",
       " (53, 'Recommendations of the SHARE ALGOL Committee'),\n",
       " (54, 'SALE, a Simple Algebraic Language for Engineers'),\n",
       " (55, 'An Algebraic Translator'),\n",
       " (56, 'Proposed Standard Flow Chart Symbols'),\n",
       " (57, 'J.E.I.D.A. and Its Computer Center'),\n",
       " (58,\n",
       "  'LEM-1, Small Size General Purpose Digital Computer Using Magnetic (Ferrite) Elements \\n The paper examines some of the questions of development and construction of a general purpose  digital computer using contactless magnetic (ferrite) and capacitive \"DEZU\" (long duration capacitive  memory) elements, developed at the Laboratory of Electrical Modeling VINITYI AN SSSR, under the supervision  of Professor L.I. Gutenmacher.'),\n",
       " (59,\n",
       "  'Survey of Progress and Trend of Development and Use of Automatic Data Processing in Business and Management control Systems of the Federal Government, as of December 1957-III'),\n",
       " (60, 'The Alpha Vector Transformation of a System of Linear Constraints'),\n",
       " (61, 'IBM 709 Tape Matrix Compiler'),\n",
       " (62, 'Multi-Dimensional Least-Squares Polynomial Curve Fitting'),\n",
       " (63,\n",
       "  'Octal Diagrams of Binary Conception and Their Applicability to Computer Design Logic \\n This paper dates back the genesis of binary conception circa 5000 years ago, and octal diagrams  about 4800 years ago, as derived by the Chinese ancients.  It analyzes the applicability of binary trinities  of the octal diagrams to modern electronic-digital-computer design logic.'),\n",
       " (64, 'Remarks on ALGOL and Symbol Manipulation '),\n",
       " (65, 'ALGOL Sub-Committee Report - Extensions'),\n",
       " (66, 'A Proposal for a Generalized Card Code for 256 Characters'),\n",
       " (67, 'Central-European Computers'),\n",
       " (68,\n",
       "  'The Role of the University in Computers, Data Processing and Related Fields \\n A study was made of university programs in the United States in the fields of computers, data  processing, operations research, and other closely related fields.  University policies, organization,  administration, faculties, students, researches, curricula, equipment, and financing were investigated.   An integrated university program is recommended reflecting the conviction that many present activities  related to computers will develop into disciplines and as such are the legitimate province of the university  scholar.  Details on a recommended Graduate School of \"Computer Sciences\" are given.'),\n",
       " (69,\n",
       "  \"Statistical Programs for the IBM 650-Part I \\n A collection is given of brief descriptions of statistical programs now in use in university  computing centers which have IBM 650's.\"),\n",
       " (70,\n",
       "  'Construction of a Set of Test Matrices \\n This paper develops the equations and properties of a set of test matrices which are useful  in the determination of the accuracy of routines for finding the inverse, determinant and/or eigenvalues  of a matrix.'),\n",
       " (71,\n",
       "  \"Proposal for a Feasible Programming System \\n This paper proposes designing a programming facility (itself involving a digital computer and  a program) which will assist the preparation of large-scale real-time programs.  This facility is to  be capable of preparing programs for any of a variety of machines having characteristics similar to those  of the facility's computer.  One of the basic assumptions is that there will be enough random-access  storage available to avoid the necessity for segmenting a constructed program in any fashion other than  a trivial one.  While this assumption is somewhat unrealistic, it is intended to provide an opportunity  to concentrate on the other aspects of program construction.  The programming system should stress the  discovery in source program statements of as many errors as possible, before attempting to construct  an object program.  Among the computer characteristics which are advocated are a program interrupt scheme,  a large set of characters, and indirect addressing.\"),\n",
       " (72, 'An Educational Program in Computing '),\n",
       " (73, 'A Real Time Data Assimilator'),\n",
       " (74, 'A High-Speed Sorting Procedure'),\n",
       " (75, 'Parameter Estimation for Simple Nonlinear Models'),\n",
       " (76,\n",
       "  'Binary Conversion, With Fixed Decimal Precision, Of a Decimal Fraction'),\n",
       " (77, 'On GAT and the Construction of Translators'),\n",
       " (78,\n",
       "  \"Remarks on the Practical Solution of Characteristic Value Problems \\n This paper is concerned with the practical solution of characteristic value problem for an  ordinary differential equation.  It is at once apparent that sequential computers, be they digital or  analog, solve initial value problems, rather than boundary value problems, and some mathematical process  must be found to compensate for the machine's inadequacy.  (Compensating for machine imperfection is,  of course, the normal activity of the numerical analyst.)  A number of other papers have applied particular  devices to particular problems.  The purpose of this note is to establish a mathematical framework or  model for these practical procedures and thus assist in the use and extension of the ideas in other particular  problems.\"),\n",
       " (79,\n",
       "  'Programming for a Machine With an Extended Address Calculational Mechanism'),\n",
       " (80,\n",
       "  'A Technique for Computing Critical Rotational Speeds of Flexible Shafts on an Automatic Computer'),\n",
       " (81, 'NORC High-Speed Printer'),\n",
       " (82,\n",
       "  'Handling Identifiers as Internal Symbols in Language Processors \\n Substitution of computer-oriented symbols for programmer-oriented symbols in language processors  is examined and a feasible method for doing so is presented.'),\n",
       " (83, 'A Visit to Computation Centers in the Soviet Union'),\n",
       " (84,\n",
       "  'Survey of Progress and Trend of Development and Use of Automatic Data Processing in Business and Management Control Systems of the Federal Government, as of December 1957-II (Part 2 see CA590406)'),\n",
       " (85, 'Error Analysis in Floating Point Arithmetic'),\n",
       " (86,\n",
       "  'Survey of Progress and Trend of Development and Use of Automatic Data Processing in Business  and Management Control Systems of the Federal Government, as of December 1957'),\n",
       " (87,\n",
       "  'A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres'),\n",
       " (88,\n",
       "  'An Efficient Method for Generating Uniformly Distributed Points on the Surface of an n-Dimensional Sphere'),\n",
       " (89,\n",
       "  'A Routine to Find the Solution of Simultaneous Linear Equations with Polynomial Coefficients'),\n",
       " (90,\n",
       "  'Binary Arithmetic for Discretely Variable Word Length in a Serial Computer'),\n",
       " (91, 'A Mathematical Procedure for Machine Division'),\n",
       " (92,\n",
       "  'A Checklist of Intelligence for Programming Systems \\n A remarkable variation exists in the degree of sophistication of various programming systems.   A particular manifestation is the jungle of assorted devices for reproducing limited human decision  procedures.  An attempt is made here to begin a systematic classification of the various devices for  educating the computer to take over the decision-making functions of one or many human operators, both  those that have been demonstrated feasible to date and those that are highly desirable for the future.'),\n",
       " (93,\n",
       "  'From Formulas to Computer Oriented Language \\n A technique is shown for enabling a computer to translate simple algebraic formulas into a  three address computer code.'),\n",
       " (94,\n",
       "  'An Iterative Method for Fitting the Logistic Curve \\n An iterative method is given for finding a logistic curve of best least squares fit to a set  of two-dimensional points.'),\n",
       " (95,\n",
       "  'Elimination of Special Functions from Differential Equations \\n A set of ordinary differential equations which contains mathematical functions requiring the  use of subroutines for numerical solution by electronic computer, tabular data for numerical solution  by hand calculation or function generators when analog methods are applied can sometimes be expanded  to an equivalent set of equations which do not contain the functions.  This is practical if these functions  satisfy sufficiently simple differential equations.  Thus among those functions which can be eliminated  by this procedure are the trigonometric, inverse trigonometric, exponential, and many other transcendental  functions.'),\n",
       " (96,\n",
       "  'On Computing Radiation Integrals \\n The relative merit and cost of four ways of evaluating typical radiation integrals containing  spherical Bessel functions are investigated.  These methods are desk machine evaluation of a finite series,  integration of the appropriate differential equation by a Reeves Electronic Analog Computer and by a  Litton 40 IBM 704 computer.  Results are generally applicable to equations separated from a Helmholtz  or wave equation.'),\n",
       " (97,\n",
       "  'Signal Corps Research and Development on Automatic Programming of Digital Computers'),\n",
       " (98,\n",
       "  'The Arithmetic Translator-Compiler of the IBM FORTRAN Automatic Coding System'),\n",
       " (99, 'Possible Modifications to the International Algebraic Language'),\n",
       " (100, 'Recursive Subscripting Compilers and List-Types Memories'),\n",
       " (101, 'Nuclear Reactor Codes'),\n",
       " (102, 'A Comparison of 650 Programming Methods'),\n",
       " (103,\n",
       "  'COPE (Console Operator Proficiency Examination)* \\n Each year electronic computers become more sophisticated, and the programs they must process  become more complex.  Because of this,dependence of those in computing on the skill and experience of  operators is increasing.  At the same time, selection and training of qualified operators grows more  difficult.  To meet the need for a quick, accurate, uniform operator test and training aid, the authors  have developed COPE (Console Operator Proficiency Examination), outlined below.  While this examination  is programmed specifically for the IBM 705 Model II with two Tape Record Coordinators, similar programs  could be developed for other computers.'),\n",
       " (104,\n",
       "  'Digital Simulation of Discrete Flow Systems* \\n The discrete flow systems discussed are characterized by the movement of randomly arriving  items along interacting channels.  Programing a digital computer to simulate such systems utilizes some  techniques not common in other approaches to physical problems.  The principal portion of the paper is  a discussion of two simulation studies that illustrate some of the programming problems involved. One  is of an extensive package-handling plant, with the objective being optimization of parameters such as  storage capacities and processing rates.  In the other, air traffic flow and control procedures are simulated  to compare the effects of alternative control decisions.'),\n",
       " (105, 'Two Methods for Word Inversion on the IBM 709'),\n",
       " (106,\n",
       "  'A Method for Overlapping and Erasure of Lists \\n An important property of the Newell-Shaw-Simon scheme for computer storage of lists is that  data having multiple occurrences need not be stored at more than one place in the computer.  That is,  lists may be \"overlapped.\"  Unfortunately, overlapping poses a problem for subsequent erasure.  Given  a list that is no longer needed, it is desired to erase just those parts that do not overlap other lists.   In LISP, McCarthy employs an elegant but inefficient solution to the problem.  The present paper describes  a general method which enables efficient erasure.  The method employs interspersed reference counts to  describe the extent of the overlapping.'),\n",
       " (107, 'Multiple Precision Arithmetic'),\n",
       " (108, 'Programmed Error Correction in Project Mercury'),\n",
       " (109, 'A Note on Approximating e^x'),\n",
       " (110, 'Fibonaccian Searching'),\n",
       " (111,\n",
       "  'On Programming the Numerical Solution of Polynomial Equations \\n Numerical techniques are presented for computing the roots of polynomial equations.  By applying  the recommended scaling and inversion rules, the basic Bairstow and Newton-Raphson iterative techniques  can be applied with great reliability.  Both a high degree of accuracy and rapid convergence are realized.   Numerical examples are shown to illustrate the pitfalls and to show how these are circumvented by application  of the recommended procedures.'),\n",
       " (112, 'Numerical Solution of the Polynomial Equation (Algorithm 30)'),\n",
       " (113, 'Survey of Coded Character Representation'),\n",
       " (114, 'Survey of Punched Card Codes'),\n",
       " (115, 'Optimizers: Their Structure'),\n",
       " (116,\n",
       "  'The Sumador Chino \\n On a recent motor trip through Mexico, the writer came across on adding device which was referred  to as a sumador chino (Chinese adder).  A survey of the more available literature on the history of mathematics  and on instruments of calculation has uncovered no reference to such a device.  The purpose of this communication  is to enlist the help of other members in bringing to light whatever may be known concerning the evolution  and present status of the sumador chino.'),\n",
       " (117,\n",
       "  'An Estimation of the Relative Efficiency of Two Internal Sorting Methods'),\n",
       " (118, 'Character Scanning on the IBM 7070'),\n",
       " (119, 'Note on Eigenvalue Computation'),\n",
       " (120, 'A Simple Technique for Coding Differential Equations'),\n",
       " (121, 'Over-all Computation Control and Labelling'),\n",
       " (122, 'Least Squares Fitting of a Great Circle Through Points on a Sphere'),\n",
       " (123,\n",
       "  'Compilation for Two Computers with NELIAC \\n NELIAC, a compiler based on ALGOL, was developed at the U.S. Navy Electronics Laboratory, San  Diego,California, as a\"boot-strap\" compiler for the Remington Rand Univac COUNTESS computer. This compiler  was used to generate a version of itself which, running as a COUNTESS program, generated machine code  for the Control Data Corporation CDC-1604.  All three versions of NELIAC accepted essentially identical  input language.'),\n",
       " (124,\n",
       "  'An Algorithm for the Assignment Problem \\n The assignment problem is formulated and briefly discussed.  An efficient algorithm for its  solution is presented in ALGOL code.  An empirical relation between solution time and the size of the  problem is given, based on extensive experiments carried out on a digital computer.'),\n",
       " (125, 'Polynomial Transformer (Algorithm 29)'),\n",
       " (126, 'Least Squares Fit By Orthogonal polynomials (Algorithm 28)'),\n",
       " (127, 'ASSIGNMENT (Algorithm 27)'),\n",
       " (128, 'ROOTFINDER III (Algorithm 26)'),\n",
       " (129, 'ROOTFINDER II (Algorithm 15)'),\n",
       " (130, 'Real Zeros of an Arbitrary Function (Algorithm 25)'),\n",
       " (131, 'Solution of Tri-Diagonal Linear Equations (Algorithm 24)'),\n",
       " (132, 'Math Sort (Algorithm 23)'),\n",
       " (133, 'Riccati-Bessel Functions of First And Second Kind (Algorithm 22)'),\n",
       " (134, 'Bessel Function for a Set of Integer Orders(Algorithm 21)'),\n",
       " (135, 'Digital Computers in Universities-IV'),\n",
       " (136, 'A Note on the Calculation of Interest'),\n",
       " (137, 'Evaluating Numbers Expressed as Strings of English Words'),\n",
       " (138,\n",
       "  'Some Thoughts on Reconciling Various Character Set Proposals (Corrigenda)'),\n",
       " (139, 'Binomial Coefficients (Algorithm 19)'),\n",
       " (140, 'Crout with Pivoting (Algorithm 16)'),\n",
       " (141, 'Some Thoughts on Parallel Processing'),\n",
       " (142, 'Comments on a Technique for Counting Ones '),\n",
       " (143,\n",
       "  'A List of Computer Systems Programs for the IBM 650, DATATRON 205, and UNIVAC SS-80'),\n",
       " (144,\n",
       "  'Do It by the Numbers-Digital Shorthand \\n Present communications systems transmit single characters in groups of coded pulses between  simple terminal equipments.  Since English words form only a sparse set of all possible alphabetic combinations,  present methods are inefficient when computer systems are substituted for these terminals.  Using numeric  representations of entire words or common phrases (rather than character-by-character representations)  requires approximately one-third of present transmission time.  This saving is reflected in overall costs.   Other benefits accrue in code and language translation schemes. Provision is made for transmission of  purely numeric and/or binary streams, and for single character-transmission of non-dictionary words such  as the names of people or places.'),\n",
       " (145, 'Automatic Graders for Programming Classes'),\n",
       " (146,\n",
       "  'The Use of Computers in Engineering Classroom Instruction  \\n On April 29-30, the Computer Committee of the College of Engineering, University of Michigan,  which acts as a steering committee for The Ford Foundation Project on the Use of Computers in Engineering  Education, held a special conference to discuss certain timely topics pertinent to the Ford Project.   This report contains a condensed transcription of the key ideas offered by the conference attendees  on selected topics. '),\n",
       " (147, 'Report on a Conference of University Computing Center Directors'),\n",
       " (148, 'Digital Computers in Universities-III'),\n",
       " (149,\n",
       "  'A Decision Rule for Improved Efficiency in Solving Linear Programming Problems with the Simplex  Algorithm'),\n",
       " (150, 'Rational Interpolation by Continued Fractions (Algorithm 18)'),\n",
       " (151, 'TRDIAG (Algorithm 17)'),\n",
       " (152, 'CROUT With Pivoting (Algorithm 16)'),\n",
       " (153, 'Comments from a FORTRAN User'),\n",
       " (154, 'Rapidly Convergent Expressions for Evaluating e^x'),\n",
       " (155, 'Trie Memory'),\n",
       " (156, 'An Introductory Problem in Symbol Manipulation for the Student'),\n",
       " (157, 'Digital Computers in Universities -II'),\n",
       " (158, 'ROOTFINDER II (Algorithm 15)'),\n",
       " (159, 'ROOTFINDER (Algorithm 2)'),\n",
       " (160, 'ROOTFINDER II (Algorithm 15)'),\n",
       " (161, 'Abbreviating Words Systematically (Corrigendum)'),\n",
       " (162, 'A Variant Technique for Counting Ones'),\n",
       " (163, 'Counting Ones on the IBM 7090 '),\n",
       " (164, 'A Short Study of Notation Efficiency'),\n",
       " (165, 'NELIAC-A Dialect of ALGOL'),\n",
       " (166,\n",
       "  'Programming Compatibility in a Family of Closely Related Digital Computers'),\n",
       " (167, 'Combining ALGOL Statement Analysis with Validity Checking'),\n",
       " (168,\n",
       "  'Multiprogram Scheduling Parts 3 and 4 Scheduling Algorithm and External Constraints'),\n",
       " (169, 'The Multilingual Terminology Project'),\n",
       " (170, 'Some Thoughts on Reconciling Various Character Set Proposals'),\n",
       " (171, 'Digital Computers in Universities (Part I) '),\n",
       " (172, 'Complex Exponential Integral (Algorithm 13)'),\n",
       " (173, 'ATLAS a new concept in large computer design'),\n",
       " (174,\n",
       "  'Interval Estimation of the Time in One State to Total Time Ratio in a DoubleExponential Process'),\n",
       " (175,\n",
       "  'The Solution of Simultaneous Ordinary Differential Equations Using a General Purpose Digital  Computer'),\n",
       " (176, 'Symbol Manipulation by Threaded Lists (Corrigendum)'),\n",
       " (177,\n",
       "  'Solution of Polynomial Equation by Bairstow Hitchcock Method, A. A. Grau Communications ACM,  February, 1960 (Algorithm)'),\n",
       " (178, 'ROOTFINDER (Algorithm)'),\n",
       " (179, 'Evaluation of the Legendre Polynomial Pn(X) by Recursion (Algorithm)'),\n",
       " (180, 'Evaluation of the Laguerre Polynomial Ln(X) by Recursion (Algorithm)'),\n",
       " (181, 'Evaluation of the Hermite Polynomial Hn(X) by Recursion (Algorithm)'),\n",
       " (182,\n",
       "  'Evaluation of the Chebyshev Polynomial Tn(X) by Recursion (Algorithm) '),\n",
       " (183, 'Conversion Between Floating Point Representations'),\n",
       " (184, 'A Short Method for Measuring Error in a Least-Squares Power Series'),\n",
       " (185,\n",
       "  'Multiprogram Scheduling Parts 1 and 2.  Introduction and Theory* \\n In order to exploit fully a fast computer which possesses simultaneous processing abilities,  it should to a large extent schedule its own workload.  The scheduling routine must be capable of extremely  rapid execution if it is not to prove self-defeating.  The construction of a schedule entails determining  which programs are to be run concurrently and which sequentially with respect to each other.  A concise  scheduling algorithm is described which tends to minimize the time for executing the entire pending workload  (or any subset of it), subject to external constraints such as precedence, urgency, etc.  The algorithm  is applicable to a wide class of machines.'),\n",
       " (186, 'An Algorithm Defining ALGOL Assignment Statements (Addendum)'),\n",
       " (187, 'Compiling Connectives'),\n",
       " (188, 'The Department of Computer Mathematics at Moscow State University'),\n",
       " (189, 'The Future of Automatic Digital Computers'),\n",
       " (190, 'Bendix G-20 System'),\n",
       " (191, 'Abbreviating Words Systematically'),\n",
       " (192, 'A Technique for Counting Ones in a Binary Computer'),\n",
       " (193, 'A Start at Automatic Storage Assignment'),\n",
       " (194, 'Divisionless Computation of Square Roots Through Continued Squaring'),\n",
       " (195, 'What is a Code?'),\n",
       " (196, 'Report on the Algorithmic Language ALGOL 60'),\n",
       " (197, 'An Imaginary Number System'),\n",
       " (198, 'A High-Speed Multiplication Process for Digital Computers'),\n",
       " (199, 'Euclidian Algorithm (Algorithm 7)'),\n",
       " (200, 'Bessel Function I, Asymptotic Expansion (Algorithm 6)'),\n",
       " (201, 'Bessel Funtion I, Series Expansion (Algorithm 5)'),\n",
       " (202,\n",
       "  'A Control System For Logical Block Diagnosis With Data Loading \\n This paper describes a section of an integrated diagnostic monitor system which facilitates  the checking of sections of instructions or subroutines anywhere in the object program.  A new method  of specifying all diagnostic operations in a format similar to a computer program makes the system convenient  to use and relatively simple to understand.  The paper also describes a number of other novel diagnostic  features which can be included in the system.'),\n",
       " (203, 'Decoding Combinations of the First n Integers Taken k at a Time'),\n",
       " (204, 'Proving Theorems by Pattern Recognition I'),\n",
       " (205,\n",
       "  'Macro Instruction Extensions of Compiler Languages \\n Macroinstruction compilers constructed from a small set of functions can be made extremely  powerful.  In particular, conditional assembly, nested definitions, and parenthetical notation serve  to make a compiler capable of accepting very general extensions to its ground language.'),\n",
       " (206, 'Symbol Manipulation in XTRAN'),\n",
       " (207, 'Syntactic and Semantic Augments to ALGOL'),\n",
       " (208, 'An Introduction to Information Processing Language V'),\n",
       " (209, 'Symbol Manipulation by Threaded Lists'),\n",
       " (210,\n",
       "  'Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I'),\n",
       " (211, 'Share Standard Flow Chart Symbols'),\n",
       " (212, 'Bisection Routine (Algorithm 4)'),\n",
       " (213, 'Numerical Inversion of Laplace Transforms'),\n",
       " (214, 'An Algorithm Defining ALGOL Assignment Statements'),\n",
       " (215, 'The Execute Operations-A Fourth Mode of Instruction Sequencing'),\n",
       " (216, 'A Note on the Use of the Abacus in Number Conversion'),\n",
       " (217, 'Soviet Computer Technology-1959'),\n",
       " (218, 'Computer Preparation of a Poetry Concordance'),\n",
       " (219, 'Marriage-with Problems'),\n",
       " (220, 'A New Method of Computation of Square Roots Without Using Division'),\n",
       " (221, 'The Basic Side of Tape Labeling'),\n",
       " (222,\n",
       "  'Coding Isomorphisms \\n The coding of external symbols into symbols internal to a compute can sometimes be carried  out in such a way that relevant informational properties are preserved, but in a form much more easily  dealt with.  A case in point is presented.'),\n",
       " (223, 'Selfcipher: Programming'),\n",
       " (224,\n",
       "  'Sequential Formula Translation \\n The syntax of an algorithmic language such as ALGOL is conveniently described as a sequence  of states indicated by an element called cellar.  Transitions are controlled by admissible state-symbol  pairs which may be represented by a transition matrix. This description of syntax furnishes at the same  time an extremely simple rule for translating into machine programs statements in the algorithmic language.   Sequential treatment, however, is not feasible in the case of certain optimizing processes such as recursive  address calculation.'),\n",
       " (225, 'A Techniquefor Handling Macro Instructions (Corrigendum)'),\n",
       " (226,\n",
       "  'Solution of Polynomial Equation by Bairstow-Hitchcock Method (Algorithm 3)'),\n",
       " (227, 'ROOTFINDER (Algorithm 2)'),\n",
       " (228, 'QUADI (Algorithm 1)'),\n",
       " (229, 'A Terminology Proposal'),\n",
       " (230, 'A Proposal for Character Code Compatibility'),\n",
       " (231, 'A Proposal for a Set of Publication Standards for Use by the ACM'),\n",
       " (232, 'A High-Speed Sorting Procedure'),\n",
       " (233, 'Abstracts-Additional Nuclear Reactor Codes'),\n",
       " (234, 'A SAP-Like Assembly Program for the IBM 650'),\n",
       " (235, 'Two Think Pieces'),\n",
       " (236,\n",
       "  'Soviet Cybernetics and Computer \\n This article records observations on Soviet research and technology in cybernetics and computer  science, made by the author during a visit to the Soviet Union as a delegate to the IFAC Congress on  Automatic Control held in Moscow in the summer of 1960.'),\n",
       " (237, 'Computer Production of Peek-A-Boo Sheets'),\n",
       " (238, 'Simulation and Analysis of Biochemical Systems'),\n",
       " (239,\n",
       "  'Inefficiency of the Use of Boolean Functions for Information Retrieval Systems'),\n",
       " (240, 'Processing Magnetic Tape Files with Variable Blocks'),\n",
       " (241,\n",
       "  'Machine Calculation of Moments of a Probability Distribution \\n A method is presented for the calculation on a machine of the moments of a probability distribution,  necessitating little more than n additions and n references to memory for each moment, instead of the  minimum of n multiplication, 2n additions, and 2n references to memory required by the most straightforward  method (where n is the number of entries in the probability distribution).  The method is directly applicable  when a tabulated distribution exists, as when it has been computed by repeated convolution; but in this  case it conserves both time and accuracy.'),\n",
       " (242,\n",
       "  'Notes on Geometric Weighted Check Digit Verification \\n This note describes a method for utilizing geometric weight modulus 11 checking digits on a  computer which does not have either multiplication or division.  In addition some attempt has been made  to show some limitations of this system.'),\n",
       " (243,\n",
       "  'N-Dimensional Codes for Detecting and Correcting Multiple Errors \\n The paper introduces a new family of codes for detecting and correcting multiple errors in  a binary-coded message.  The message itself is arranged (conceptually) into a multidimensional rectangular  array.  The processes of encoding and error detection are based upon parity evaluations along prescribed  dimensions of the array.  Effectiveness of the codes is increased by introducing a \"system check bit\",  which is essentially a parity check on the other parity bits.  Only three-dimensional codes are discussed  in this paper with parity evaluations along the horizontal, the vertical, and one main diagonal.  However,  the family of codes is not restricted to three dimensions, as evidenced by the discussion by Minnick  and Ashenhurst on a similar multidimensional single-bit selection plan used for another purpose [6].   A four-dimensional code, correcting three and detecting four errors, has been developed; the extension  to higher-dimensional codes with greater correction power is straightforward.'),\n",
       " (244, 'Incomplete Elliptic Integrals (Algorithm 73)'),\n",
       " (245,\n",
       "  'A Set of Associate Legendre Polynomials of the Second Kind (Algorithm 62)'),\n",
       " (246, 'Least-Squares Fit by Orthogonal Polynomials (Algorithm 28)'),\n",
       " (247, 'Incomplete Elliptic Integrals (Algorithm 73)'),\n",
       " (248,\n",
       "  'What is Proprietary In Mathematical Programming?-Impressions of a Panel Discussion \\n A panel discussion on \"What is Proprietary in Mathematical Programming?\" was sponsored by the  Special Interest Committee on Mathematical Programming of the ACM during a Hall of Discussion/on September  7th at the 16th National ACM meeting in Los Angeles.  This note consists solely of the impressions garnered  by the moderator of the panel and does not necessarily represent the position of any of the panelists  or other participants in the discussion.'),\n",
       " (249,\n",
       "  \"Specification Languages for Mechanical Languages and Their Processors*-A Baker's Dozen\"),\n",
       " (250, 'An Engineering Application of Logic-Structure Tables'),\n",
       " (251,\n",
       "  'Ballistic Cam Design  \\n This paper presents a digital computer program for the rapid calculation of manufacturing data  essential to the design of preproduction cams which are utilized in ballistic computers of tank fire  control systems.  The cam profile generated introduces the superelevation angle required by tank main  armament for a particular type ammunition.'),\n",
       " (252,\n",
       "  'Programming a Duplex Computer System \\n This paper describes a method of duplex-computer programming that has been used with two computers  in a military defense system.  The method combines special programs with a basic data processing program  package.  The duplex operation gives the system greater reliability.  After achieving the required level  of integration, both computers do similar processing on the same inputs and continually cross-check the  intermediate and final results.'),\n",
       " (253,\n",
       "  \"On a Program for Ray-Chaudhuri's Algorithm for a Minimum Cover of an Abstract Complex\"),\n",
       " (254,\n",
       "  'SMALGOL-61 \\n Prior to and during the 1961 Western Joint Computer Conference, several people in the Joint  Users Groups had expressed interest in defining a \"smalgol\" language.  This is to be an ALGOL language  for use with compilers on relatively small size computers.  A preliminary report resulted.  At the ACM  National Conference four months later, after considering several counter proposals, a final version was  agreed upon by a subcommittee.  The recommendations of the Subcommittee for a standard subset of ALGOL  60 for use on small computers is presented here.'),\n",
       " (255, 'Augmentation (Algorithm 68)'),\n",
       " (256, 'A Set of Test Matrices (Algorithm 52)'),\n",
       " (257, 'Invert (Algorithm 42)'),\n",
       " (258, 'Composition Generator (Algorithm 72)'),\n",
       " (259, 'Permutation (Algorithm 71)'),\n",
       " (260, 'Interpolation By Aitken (Algorithm 70)'),\n",
       " (261, 'Tape Splitting'),\n",
       " (262, 'MAP'),\n",
       " (263, 'Library Loading with Alternate Routine Selection'),\n",
       " (264, 'A Generalized Polyphase Merge Algorithm '),\n",
       " (265,\n",
       "  'Low Level Language Subroutines for Use Within Fortran \\n This paper describes some subroutines, coded in symbolic languages and for use within Fortran  coded programs, to deal with \"special arithmetic\" (e.g. multi-precision arithmetic), symbol manipulation,  bit manipulation and expanded character set input-output, and visual display.'),\n",
       " (266, 'Fitting Spheres by the Method of Least Squares'),\n",
       " (267, 'Some Proposals for Improving the Efficiency of ALGOL 60'),\n",
       " (268, 'Stochastic Evaluation of a Static Storage Allocation'),\n",
       " (269, 'Core Allocation Based on Probability'),\n",
       " (270, 'Techniques for Storage Allocation Algorithms '),\n",
       " (271, 'A Semi-Automatic Storage Allocation System at Loading Time'),\n",
       " (272,\n",
       "  'A Storage Allocation Scheme for ALGOL 60 \\n A storage allocation scheme for a machine with a 2048 instruction core store and a magnetic  drum is described.  The use of the drum for storing program blocks and/or data must be directed by the  programmer through auxiliary information in the ALGOL program.  The administrative routines controlling  the storage at run time are described in full.  A detailed example is given.'),\n",
       " (273, 'Experience in Automatic Storage Allocation '),\n",
       " (274,\n",
       "  'Dynamic Storage Allocation in the Atlas Computer, Including an Automatic Use of a Backing Store'),\n",
       " (275, 'Dynamic Storage Allocation for an Information Retrieval System'),\n",
       " (276,\n",
       "  'Program Organization and Record Keeping for Dynamic Storage Allocation \\n The material presented in this paper is part of the design plan of the core allocation portion  of the ASCII-MATIC Programming System.  Project ASCII-MATIC is concerned with the application of computer  techniques to the activities of certain headquarters military intelligence operations of the U.S. Army.'),\n",
       " (277,\n",
       "  'Problems of Storage Allocation in a Multiprocessor Multiprogrammed System'),\n",
       " (278,\n",
       "  \"A General Formulation of storage Allocation \\n Formalization of a general computer storage allocation process is attempted.  With a given  computer M is associated a fictitious computer M' essentially identical to M except in respect to possession  of unbounded primary storage.  Mappings of the total storage set (internal and external) of M into the  direct address set of M' are introduced.  A program sequence P for M' is termed M-admissible (relative  to a specific execution time period) if there is a mapping underwhich P and its effective data referents  are all located in the direct address set of M.  Storage allocation is considered as a process of establishing  for an arbitrary M' program  a sequence of mappings, a decoupling of the program into M-admissible subprograms  and a linking set of interludes.  An existence proof in terms of a completely interpretive M program  as indicated.  Some special cases are discussed.  Various restrictions on generality of M' programs are  considered under which more practical realization of allocation processes becomes tractable.\"),\n",
       " (279, 'The Case for Dynamic storage Allocation '),\n",
       " (280, 'A Preplanned Approach to a Storage Allocating Compiler'),\n",
       " (281,\n",
       "  'Putting a Hex on e^x \\n Recent notes on approximate natural antilogy have not considered indirect formulations for  describing e^x.  In this note we produce a particular family of very fast, high precision and eminently  practical exponential evaluation formulas derived from one such formulation.'),\n",
       " (282,\n",
       "  'Optimum Tape-Writing Procedures \\n Consider a magnetic tape system with a read check after writing.  Where an error occurs in  writing a record, a programmed error routine may either bypass some or all of the area on tape or try  to rewrite the record on the same area.  This paper evaluates these two procedures on the basis of expected  loss of computer time and develops a decision rule for selecting the optimum procedure.  The rule depends  critically on the number of times the tape being written will be used in the future.  In the case where  the optimum procedure is to bypass an area, a second decision-the size of the area to be bypassed-is  necessary.  A formula is developed to determine the optimum area to be bypassed for each procedure.'),\n",
       " (283, 'Inversion of a Complex Matrix'),\n",
       " (284,\n",
       "  'Manipulation of Algebraic Expressions \\n An algorithm for algebraically manipulating expressions of the form SUM{CiPi, i=1,...,n}; has  been developed in conjunction with the development of programs for systems analysis problems.  This algorithm  enablesus to derive over-all system transfer functions from algebraically described block diagrams of  any linear continuous multi-loop feedback system.  The machine representation of the derived expression,  is, by virtue of the algorithm, in a form which simplifies the task of compiling.  The algorithm was  developed for a particular purpose in connection with system analysis studies.  However, its application  as a mathematical device extends far beyond the confines of the original problem.'),\n",
       " (285, 'Solution of Tridiagonal Matrices'),\n",
       " (286, 'An Iterative Method for Inversion of Power Series'),\n",
       " (287, 'The Generalized Important Event Technique'),\n",
       " (288, 'A Syntactical Chart of ALGOL 60'),\n",
       " (289, 'Critical Path Scheduling (Algorithm 40)'),\n",
       " (290, 'Chain Tracing (Algorithm 69)'),\n",
       " (291, 'Use of MOBOL in PreparingRetrieval Programs'),\n",
       " (292, 'An Information Retrieval Language for Legal Studies'),\n",
       " (293,\n",
       "  'The Applied Mathematics Laboratory of the David W. Taylor Model Basin'),\n",
       " (294, 'An Imaginary Number System'),\n",
       " (295,\n",
       "  'Rational Approximations for the Error Function and for Similar Functions'),\n",
       " (296, 'A Note on Multiple Precision Arithmetic'),\n",
       " (297, 'A Note on Fitting Great Circles by Least Squares'),\n",
       " (298,\n",
       "  'A 48-Bit Pseudo-Random Number Generator \\n A new 48-bit pseudo-random number generator, suitable for several computers, was tested statistically  for randomness to determine its adequacy for use in Monte Carlo programs.  Frequency tests, distributions  of certain low-order moments, runs up and down, and runs above and below the mean were applied to one-half  million generated numbers lying within the interval (0,1) and to three sets of integers obtained from  specified bits within the generated numbers.  These tests substantiated the randomness of all numbers  except for the set of integers coming from the least significant bits.'),\n",
       " (299, 'A Generalized Polyphase Merge Algorithm'),\n",
       " (300,\n",
       "  'COBOL: A Sample Problem \\n A simplified Merchandise Control problem has been chosen for presenting COBOL to users and  potential users of computing systems.  A mythical department store, \"E. Language Bros., Inc.\", is programming  in the COBOL language one of the many runs on its computer.'),\n",
       " (301, 'A Set of Test Matrices (Algorithm 52)'),\n",
       " (302, 'Augmentation (Algorithm 68)'),\n",
       " (303,\n",
       "  \"Some Basic Terminology Connected With Mechanical Languages and Their Processors \\n The suggestions in this paper are part of the terminology used in work for the University of  Pennsylvania's Office of computer Research and Education.  The work is jointly supported by the National  Science Foundation and the Air Force Office of Scientific Research.\"),\n",
       " (304, 'Nth Roots of a Complex Number (Algorithm 53)'),\n",
       " (305, 'CRAM (Algorithm 67)'),\n",
       " (306, 'INVRS (Algorithm 66)'),\n",
       " (307, 'FIND (Algorithm 65)'),\n",
       " (308, 'QUICKSORT (Algorithm 64)'),\n",
       " (309, 'PARTITION (Algorithm 63)'),\n",
       " (310,\n",
       "  'A Set of Associate Legendre Polynomials of the Second Kind (Algorithm 62)'),\n",
       " (311, 'Procedures for Range Arithmetic (Algorithm 61)'),\n",
       " (312, 'A Further Note on Approximating e^x'),\n",
       " (313, 'An Iterative Method for Inversion of Power Series'),\n",
       " (314, 'A Divisionless Method of Integer Conversion'),\n",
       " (315, 'Solution of Tridiagonal Matrices'),\n",
       " (316, 'An Algorithm for Equivalence Declarations'),\n",
       " (317,\n",
       "  'On The Approximation of Curves by Line Segments Using Dynamic Programming'),\n",
       " (318, 'Combat Vehicle Firing Stability (Active Suspension)'),\n",
       " (319,\n",
       "  'On a Class of Iteration Formulas and Some Historical Notes \\n The class of iteration formulas obtainable by rational approximations of \"Euler\\'s formula\"  is derived with the corresponding error estimates.  Some historical notes on iterative procedures are  followed by a derivation of Euler\\'s formula with the associated error estimate in a new notation which  simplifies the error estimate and suggests generalizations.  The final section considers the Pade approximants  to the \"Euler polynomial\" and shows how a number of known formulas may be derived from this unified approach.   There is a short discussion of the \"best\" formula.'),\n",
       " (320,\n",
       "  'Logic-Structure Tables \\n Logic tables are an excellent way of developing and expressing the logic required in procedures,  operations, systems and circuits.  A set of rules for writing and using logic tables is explained by  means of some simple examples.  Then the logic structure of a vending machine is given in which two logic  tables are used.  Logic tables are two-dimensional in nature, enabling us to fully express and consider  both the sequential and parallel aspects of logic.  They can be compiled directly into a computer program  and so eliminate the need for flow charting and hand coding.'),\n",
       " (321,\n",
       "  'ALGOL 60 Confidential \\n The ALGOL 60 Report,* when first encountered, seems to describe a very complex language which  will be difficult to learn.  The \"metalinguistic formulae\" admirably serve the purpose of precisely specifying  a language, but they are certainly not very readable for a beginner.  However, experience has shown that  once the report is explained it is in fact easy to learn ALGOL and to write algorithms in it.  The language  is so general and powerful it can handle an enormous class of problems.  It is not hard to learn those  parts of ALGOL present in other compiler languages: how to write assignment and go to and for statements,  etc.  Indeed, a lot of the unnecessary restrictions imposed by other compiling languages have finally  been lifted.  But ALGOL also allows many unobvious things to be written, as we will see later, and herein  lies a problem: ALGOL seems to have become too general.  So many restrictions have been lifted that a  lot of technical details crop up which are hard to learn and to use correctly.  In this paper some of  the more obscure features of the language are considered and their usefulness is discussed.  Remarks  are based on the authors\\' interpretations of the ALGOL 60 Report.'),\n",
       " (322,\n",
       "  'Operational Compatibility of Systems-CONVENTIONS \\n The General Standards Committee of the SHARE organization has devoted considerable effort to  the problem of operating a computer efficiently in view of the growing number of programming systems  available.  Each of these programming systems has been coded to utilize a fixed set of hardware components  without recognizing the fact that others may be occupying a storage medium required by the first.  These  incompatibilities are currently resolved by manually setting up the computer for each system as required.   The following set of conventions is being considered to minimize computer set-up time.  They are of  sufficiently broad interest that we feel other computer users should be aware of them. -George F. Ryckman,  Chairman'),\n",
       " (323, 'The State of Digital Computer Technology in Europe'),\n",
       " (324, 'Romberg Integration (Algorithm 60)'),\n",
       " (325, 'Numerical Solution of the Polynomial Equation (Algorithm 30)'),\n",
       " (326, 'MATHSORT (Algorithm 23)'),\n",
       " (327, 'Zeros of a Real Polynomial by Resultant Procedure (Algorithm 59)'),\n",
       " (328, 'Matrix Inversion (ALgorithm 58)'),\n",
       " (329,\n",
       "  'Automatic Abstracting and Indexing Survey and Recommendations \\n In preparation for the widespread use of automatic scanners which will read documents and transmit  their contents to other machines for analysis, this report presents a new concept in automatic analysis:  the relative-frequency approach to measuring  the significance of words, word groups, and sentences.   The relative-frequency approach is discussed in detail, as is its application to problems of automatic  indexing and automatic abstracting.  Included in the report is a summary of automatic analysis studies  published as of the date of writing.  Conclusions are that point toward more sophisticated mathematical  and linguistic techniques for the solution of problems of automatic analysis.'),\n",
       " (330, 'A Method for Evaluating the Area of the Normal Function'),\n",
       " (331,\n",
       "  'Successive Approximations and Computer Storage Problems in Ordinary Differential Equations'),\n",
       " (332,\n",
       "  'An Indirect Chaining Method for Addressing on Secondary Keys \\n Methods for entering random access files on the basis of one key are briefly surveyed.  The  widely used chaining method, based on a pseudo-random key transformation, is reviewed in more detail.   An efficient generalization of the chaining method which permits recovery on additional keys is then  presented.'),\n",
       " (333, 'Design of an Improved* Transmission/Data Processing Code'),\n",
       " (334, 'Division and Square Root in the Quater-Imaginary Number System'),\n",
       " (335,\n",
       "  \"Some Numerical Experiments Using Newton's Method for Nonlinear Parabolic and EllipticBoundary-Value  Problems \\n Using a generalization of Newton's method, a nonlinear parabolic equation of the form U(t)-U(xx)=g(U)  and a nonlinear elliptic equation U(xx)+U(yy)=exp(U) are solved numerically Comparison of these results  with results obtained using the Picard iteration procedure show that in many cases the quisi linearization  method offers substantial advantages in both time and accuracy.\"),\n",
       " (336,\n",
       "  'A Practical Technique for the Determination of the Optimum Relaxation Factor of the Successive  Over-Relaxation Method'),\n",
       " (337, 'Further Survey of Punched Card Codes'),\n",
       " (338, 'GROUT II (Algorithm 43)'),\n",
       " (339, 'Real Exponential Integral (Algorithm 20)'),\n",
       " (340, 'Legendre Polynomial (Algorithm 13)'),\n",
       " (341, 'Chebyschev Polynomial (Algorithm 10)'),\n",
       " (342, 'Solution of Polynomial Equation by Barstow-Hitchcock (Algorithm 3)'),\n",
       " (343, 'On Frequently Occurring Errors in ALGOL 60 Programs (Algorithm 25)'),\n",
       " (344, 'Ber or Bei Function (Algorithm 57)'),\n",
       " (345, 'Complete Elliptic Integral of the Second Kind (Algorithm 56)'),\n",
       " (346, 'Complete Elliptic Integral of the First Kind (Algorithm 55)'),\n",
       " (347, 'Gamma Function for Range 1 to 2 (Algorithm 54)'),\n",
       " (348, 'Nth Roots of a Complex Number (Algorithm 53)'),\n",
       " (349, 'A Set of Test Matrices'),\n",
       " (350,\n",
       "  'Adjust Inverse of a Matrix When an Element is Perturbed (Algorithm 51)'),\n",
       " (351, 'Inverse of a Finite Segment of the Hilbert Matrix (Algorithm 50)'),\n",
       " (352, 'Spherical Neumant Function (Algorithm 49)'),\n",
       " (353, 'Logarithm of A Complex Number (Algorithm 48)'),\n",
       " (354,\n",
       "  'Associated Legendre Functions of the First Kind for Real or Imaginary Arguments (Algorithm 47)'),\n",
       " (355, 'Exponential of a Complex Number (Algorithm 46)'),\n",
       " (356, 'INTEREST (Algorithm 45)'),\n",
       " (357, 'Bessel Functions Computed Recursively (Algorithm 44)'),\n",
       " (358, 'Crout with Pivoting II (Algorithm 43)'),\n",
       " (359, 'INVERT (Algorithm 42)'),\n",
       " (360, 'Evaluation of Determinant (Algorithm 41)'),\n",
       " (361, 'Programmed Error Correction on a Decimal Computer'),\n",
       " (362, 'Table Look-At Techniques'),\n",
       " (363, 'On Approximating Transcendental Numbers by Continued Fractions'),\n",
       " (364, 'On the Compilation of Subscripted Variables '),\n",
       " (365, 'Bessel Functions of Integral Order and Complex Argument'),\n",
       " (366, 'Eigenvalues of a Symmetric 3 x 3 Matrix'),\n",
       " (367,\n",
       "  'Topological Ordering of a List of Randomly-Numbered Elements of a Network \\n A network of directed line segments free of circular elements is assumed.  The lines are identified  by their terminal nodes and the nodes are assumed to be numbered by a non-topological system.  Given  a list of these lines in numeric order, a simple technique can be used to create at high speed a list  in topological order.'),\n",
       " (368, 'Real Zeros of an Arbitrary Function (Algorithm 25)'),\n",
       " (369, 'Crout with Pivoting (Algorithm 16)'),\n",
       " (370, 'Bisection Routine (Algorithm 4)'),\n",
       " (371, 'Remarks on Algorithms 2 and 3, Algorithm 15 and Algorithms 25 and 26'),\n",
       " (372, 'Critical Path Scheduling (Algorithm 40)'),\n",
       " (373, 'Correlation Coefficients with Matrix Multiplication (Algorithm 39)'),\n",
       " (374, 'Telescope2 (Algorithm 38)'),\n",
       " (375, 'Telescope1 (Algorithm 37)'),\n",
       " (376, 'Tchebycheff (Algorithm 36)'),\n",
       " (377, 'SIEVE (Algorithm 35)'),\n",
       " (378,\n",
       "  'A Generalized Technique for Symbol Manipulation and Numerical Calculation '),\n",
       " (379, 'Bitwise Operations'),\n",
       " (380,\n",
       "  'Comparison of Iterative Methods for the Calculation of nth Roots \\n Three iterative methods for calculation of nth roots (including one proposed by the author)  are compared in two ways: (1) Theoretical convergence estimates are given.  (2) A new macrocompiler which  estimates machine running time is used to compare the running time of the three methods for a variety  of input data.'),\n",
       " (381, 'An Alternate Form of the \"UNCOL Diagram\"'),\n",
       " (382, 'Statistical Programs at the University of North Carolina'),\n",
       " (383, 'On Finding Minimum Routes in a Network With Turn Penalties'),\n",
       " (384, 'Gamma Function (Algorithm 34)'),\n",
       " (385, 'FACTORIAL (Algorithm 33)'),\n",
       " (386, 'MULTINT (Algorithm 32)'),\n",
       " (387, 'Gamma Function (Algorithm 31)'),\n",
       " (388,\n",
       "  'Solution of Polynomial Equations by Bairstow Hitchcock Method (Algorithm 3)'),\n",
       " (389, 'Real Exponential Integral (Algorithm 20)'),\n",
       " (390, 'Complex Exponential Integral (Algorithm 13)'),\n",
       " (391, 'The BKS System for the Philco-2000 '),\n",
       " (392, 'Comment on A Paper on Parallel Processing'),\n",
       " (393, 'Two Subroutines for Symbol Manipulation with an Algebraic Compiler'),\n",
       " (394, 'Multiple Programming Data Processing'),\n",
       " (395, 'Multiple-Precision Division '),\n",
       " (396,\n",
       "  '   Automation of Program  Debugging \\n    Automatic Debugging can substantially reduce lead-time between the coding and the effective use of a complex program. It also enforces analysis of debugging criteria, resulting in verifiably accurate programs. The programmer specifies the program to be debugged, memory areas, set of input data, maximum repetition of loops, and checkpoint information for each set of data. The executive debugging program the runs the program to be debugged, performing checking functions and creating a trace record of its own later analysis and location of errors. Applications are quite flexible, and the system can be used alone or in conjunction with other debugging techniques.'),\n",
       " (397,\n",
       "  'A Card Format for Reference Files in Information Processing \\n This paper proposes a card format suitable for a variety of reference files in information  processing.  An 80-column IBM card is divided into two fields-reference material field (columns 1-67)  and identification field (columns 68-80).  The format for the reference material is flexible, while the  format for the identification is rigid.  The reference material includes basically an index, title, source,  class, summary and cross reference for each entry.  The identification includes basically codes for a  matrix of descriptors, an entry number, and the kind, major interest, and source of the reference.  The  identification also provides a choice to identify material for personal as well as general files.  Since  this card format is sufficient to identify the material normally associated with reference files for  books, articles, programming terms, hardware terms, equipment, machine systems, abbreviations, etc., it  is suitable as a standard for card reference files in information processing.'),\n",
       " (398, 'The SLANG System'),\n",
       " (399,\n",
       "  'Compiling Techniques for Boolean Expressions and Conditional Statements in ALGOL 60'),\n",
       " (400,\n",
       "  'Comments on the Implementation of Recursive Procedures and Blocks in ALGOL 60'),\n",
       " (401, 'Allocation of Storage for Arrays in ALGOL 60'),\n",
       " (402, 'Dynamic Declarations'),\n",
       " (403,\n",
       "  'Thunks -- A Way of Compiling Procedure Statements with Some Comments on Procedure Declarations'),\n",
       " (404, 'A Syntax Directed Compiler for ALGOL 60'),\n",
       " (405,\n",
       "  'An Algorithm for Coding Efficient Arithmetic Operations \\n Most existing formula translation schemes yield inefficient coding.  A method is described  which reduces the number of store and fetch operations, evaluates constant subexpressions during compilation,  and recognizes many equivalent subexpressions.'),\n",
       " (406,\n",
       "  'The Use of Threaded Lists in Constructing a Combined ALGOL and Machine-Like Assembly Processor'),\n",
       " (407,\n",
       "  'MADCAP: A Scientific Compiler for a Displayed Formula Textbook Language'),\n",
       " (408, 'The Internal Organization of the MAD Translator'),\n",
       " (409,\n",
       "  'CL-1, An Environment for a Compiler \\n A flexible, large-scale programming system to facilitate the solution of information processing  problems and to provide intercommunication between programs and/or programmers has been developed and  realized on the IBM 709/7090 computer.  The system is based on a master file concept and has provisions  for accepting, storing, and retrieving both descriptions and instances of large and complex data sets,  as well as algorithms defined on these data sets.  Both data and algorithms may be expressed in a family  of command and descriptive languages.  The concept of distinct data descriptions and the content and  use of such descriptions are discussed in some detail.'),\n",
       " (410, 'The CLIP Translator'),\n",
       " (411, 'Use of Magnetic Tape for Data Storage in the ORACLE-ALGOL Translator'),\n",
       " (412, 'Recursive Processes and ALGOL Translation'),\n",
       " (413, 'A Basic Compiler for Arithmetic Expressions'),\n",
       " (414,\n",
       "  'IBM 1440 Data Processing System Features Five New Units \\n The IBM 1440 data processing system, announced recently by the International Business Machines  Corporation, not only features the 1311 disk storage drive with interchangeable disk packs but four other  newly developed units.'),\n",
       " (415, 'The Use of Digital Computers in Western Germany'),\n",
       " (416, 'Multiple Shooting Method for Two-Point Boundary Value Problems'),\n",
       " (417,\n",
       "  'Legal Implications of Computer Use \\n This paper points out a variety of ways computer systems used in business and industry can  be involved in legal entanglements and suggests that computer specialists have a responsibility to call  for assistance in forestalling or minimizing those entanglements during the planning stage.  Techniques  are suggested for making legal clearance effective with the least burden on the new technology and for  achieving a favorable legal climate for it generally.  Computer specialists also are alerted to potential  opportunities to interpret to lawyers the technical aspects of computer systems involved in legal situations.'),\n",
       " (418, 'RANDOM (Algorithm 133)'),\n",
       " (419, 'Magic Square (Algorithm 118)'),\n",
       " (420, 'PERM (Algorithm 115)'),\n",
       " (421, 'Position of Point Relative to Polygon (Algorithm 112)'),\n",
       " (422, 'COMBINATION (Algorithm 94)'),\n",
       " (423, 'Matrix Inversion (Algorithm 58)'),\n",
       " (424, 'Gamma Function (Algorithm 31)'),\n",
       " (425, 'Complete Elliptic Integral (Algorithm 149)'),\n",
       " (426, 'Term of Magic Square (Algorithm 148)'),\n",
       " (427, 'PSIF (Algorithm 147)'),\n",
       " (428, 'Multiple Integration (Algorithm 146)'),\n",
       " (429, \"Adaptive Nimerical Integration by Simpson's Rule (Algorithm 145)\"),\n",
       " (430, 'TREESORT2 (Algorithm 144)'),\n",
       " (431, 'TREESORT1 (Algorithm 143)'),\n",
       " (432, 'Triangular Regression (Algorithm 142)'),\n",
       " (433, 'Fixed-World-Length Arrays in Variable-Word-Length Computers'),\n",
       " (434, 'Character Manipulation in 1620 Fortran II'),\n",
       " (435,\n",
       "  'A Decision Matrix as the Basis for a Simple Data Input Routine \\n Currently a great deal of time and effort is being spent on the development of bigger and better  compiler languages, multiprogram executive systems, etc.  Since the implementation of  of new methods  and procedures is not instantaneous, but rather occurs by an evolutionary process, we should be concerned  also with the problem of maintaining, improving and incorporating new ideas into existing systems.  It  is with this somewhat neglected area that the author is interested.  A method employing a decision matrix  is presented for the handling of a standard systems programming problem,that of providing a data input  routine.'),\n",
       " (436, 'Evaluation of Polynomials by Computer'),\n",
       " (437, 'Compiling Matrix Operations'),\n",
       " (438,\n",
       "  'Mechanical Pragmatics: A Time-Motion Study of a Miniature Mechanical Linguistic System'),\n",
       " (439,\n",
       "  'On-Line Digital Computer for Measurement of a Neurological Control System'),\n",
       " (440,\n",
       "  'Record Linkage \\n Special difficulties are encountered in devising reliable systems for searching and updating  any large files of documents that must be identified primarily on the basis of names and other personal  particulars.  The underlying problem is that of making nearly maximum use of items of identifying information  that are individually unreliable but that may collectively be of considerable discriminating power.   Rules that can be applied generally to name retrieval systems have been developed in a methodological  study of the linkage of vital and health records into family groupings for demographic research purposes.   These rules are described, and the ways in which information utilization for matching may be optimized  are discussed.'),\n",
       " (441,\n",
       "  \"Topological Sorting of Large Networks \\n Topological Sorting is a procedure required for many problems involving analysis of networks.   An example of one such problem is PERT.  The present paper presents a very general method for obtaining  topological order.  It permits treatment of larger networks than can be handled on present procedures  and achieves this with greater efficiency.  Although the procedure can be adapted to any machine, it  is discussed in terms of the 7090.  A PERT network of 30,000 activities can be ordered in less than one  hour of machine time.  The method was developed as a byproduct of procedures needed by Westinghouse,  Baltimore.  It has not been programmed and at present there are no plans to implement it.  In regard  to the techniques described, Westinghouse's present and anticipated needs are completely served by the  Lockheed program, which is in current use.\"),\n",
       " (442, 'Crout with Equilibration and Iteration (Algorithm 135)'),\n",
       " (443, 'Complex Number to a Real Power (Algorithm 106)'),\n",
       " (444, 'Evaluation of Jacobi Symbol (Algorithm 99)'),\n",
       " (445, 'COMBINATION (Algorithm 94)'),\n",
       " (446, \"Simpson's Integration (Algorithm 84)\"),\n",
       " (447, 'Certification of the Calculation of Easter'),\n",
       " (448, 'Path Matrix (Algorithm 141)'),\n",
       " (449, 'Matrix Inversion(Algorithm 140)'),\n",
       " (450, 'Solution of the Diophantine Equation (Algorithm 139)'),\n",
       " (451, 'Nesting of for Statement II (Algorithm 138)'),\n",
       " (452, 'Nesting of for Statement I (Algorithm 137)'),\n",
       " (453, 'Enlargement of a Group (Algorithm 136)'),\n",
       " (454, 'Crout with Equilibration and Iteration (Algorithm 135)'),\n",
       " (455, 'Exponentiation of Series (Algorithm 134)'),\n",
       " (456, 'RANDOM (Algorithm 133)'),\n",
       " (457, 'Quantum Mechanical Integrals Over all Slater-Type Integrals'),\n",
       " (458, 'Coefficient Determination (Algorithm 131)'),\n",
       " (459, 'PERMUTE (Algorithm 130)'),\n",
       " (460, 'MINIFUN (Algorithm 129)'),\n",
       " (461, 'Coding of Medical Case History Data for Computer Analysis'),\n",
       " (462,\n",
       "  'Computer Pattern Recognition Techniques: Electrocardiographic Diagnosis \\n The use of programmed digital computers as general pattern classification and recognition devices  is one phase of the current lively interest in artificial intelligence.  It is important to choose a  class of signals which is, at present, undergoing a good deal of visual inspection by trained people  for the purpose of pattern recognition.  In this way comparisons between machine and human performance  may be obtained.  A practical result also serves as additional motivation.  Clinical electrocardiograms  make up such a class of signals.  The approach to the problem presented here centers upon the use of  multiple adaptive matched filters that classify normalized signals.  The present report fives some of  the background for the application of this method.'),\n",
       " (463, 'On Ambiguity in Phrase Structure Languages'),\n",
       " (464,\n",
       "  'Syntactic Analysis by Digital Computer \\n This paper provides an account of the Shadow language that is used to describe syntax and of  a corresponding subroutine that enables a computer to perform syntactic analysis.  The input to this  subroutine consists of a string to be analyzed and a description of the syntax that is to be used.  The  syntax is expressed in the Shadow language.  The output consists of a trace table that expresses the  results of the syntactic analysis in a tabular form.  Several versions of the subroutine and some associated  programs have been in use now for over three years.  The present account of the language and the subroutine  contains a summary of material that has been described previously in unpublished reports and also some  additional discussion of the work in relation to the more general questions of problem-oriented languages  and string transformations.'),\n",
       " (465, 'PERM (Algorithm 115)'),\n",
       " (466, 'General Order Arithmetic (Algorithm 93)'),\n",
       " (467, 'Permutation Generator (Algorithm 87)'),\n",
       " (468, 'Incomplete Elliptic Integrals (Algorithm 73)'),\n",
       " (469, 'Critical Path Scheduling (Algorithm 40)'),\n",
       " (470, 'Summation of Fourier Series (Algorithm 128)'),\n",
       " (471, 'ORTHO (Algorithm 127)'),\n",
       " (472, \"Gauss' Method (Algorithm 126)\"),\n",
       " (473, 'WEIGHTCOEFF (Algorithm 125)'),\n",
       " (474, 'Input Data Organization in Fortran'),\n",
       " (475, 'A Test Matrix for Inversion Procedures'),\n",
       " (476, 'Further Remarks on Sampling a Tape File-II'),\n",
       " (477, 'Further Remarks on Sampling a Tape File-I'),\n",
       " (478, 'Implementing a Stack'),\n",
       " (479,\n",
       "  'A Dispersion Pass Algorithm for the Polyphase Merge \\n This paper presents a new manner of dispersing strings for a Polyphase merge.  If the number  of strings dispersed is between two levels acceptable by Polyphase merge, a more economical technique  of reaching the next level for Polyphase merge is shown and proved.'),\n",
       " (480, 'Quick Calculation of Jacobian Elliptic Functions (Corrigendum)'),\n",
       " (481, 'A One-Day Look At Computing'),\n",
       " (482, 'TALL-A List Processor for the Philco 200 Computer'),\n",
       " (483,\n",
       "  'On the Nonexistence of a Phrase Structure Grammar for ALGOL 60 \\n ALGOL 60 is defined partly by formal mechanisms of phrase structure grammar, partly by informally  stated restrictions.  It is shown that no formal mechanisms of the type used are sufficient to define  ALGOL 60.'),\n",
       " (484, 'Hankel Function (Algorithm 124)'),\n",
       " (485, 'Real Error Function, ERF(x) (Algorithm 123)'),\n",
       " (486, 'Tridiagonal Matrix (Algorithm 122)'),\n",
       " (487, 'NORMDEV (Algorithm 121)'),\n",
       " (488, 'A Heuristic for Page Turning In a Multiprogrammed Computer'),\n",
       " (489, 'Current Status of IPL-V for the Philco 2000 Computer (June 1962)'),\n",
       " (490, 'Programmed Methods for Printer Graphical Output'),\n",
       " (491, 'Use of Multiprogramming in the Design of a Low Cost Digital Computer'),\n",
       " (492,\n",
       "  'Analysis of a File Addressing Method \\n This paper presents a new file addressing method based on the calculation of an address from  the identification of a record.  For large recirculating type files, it seems to be more advantageous  than customary ones.  The probability distribution of the displacement of records from their calculated  address, which is one less than the number of probes required to address a record, is computed on the  basis of a Markov chain model.  For the reader not interested in the mathematics, the introduction and  the summary should be sufficient.'),\n",
       " (493, 'The Property Classification Method of File Design and Processing'),\n",
       " (494,\n",
       "  'A Finite Sequentially Compact Process for the Adjoints of Matrices Over Arbitrary Integral Domains'),\n",
       " (495,\n",
       "  'A Procedure for Inverting Large Symmetric Matrices \\n In the least squares method for simultaneous adjustment of several parameters, the coefficients  of the normal equations are the elements of a symmetric positive-definite matrix.  In order to solve  the normal equations and evaluate the precision measures of the resulting parameters, inversion of this  matrix of coefficients is required.  Many available procedures for matrix inversion do not take advantage  of the symmetry.  Thus, when programmed for a high-speed computer, all n^2 elements must be stored and  manipulated, whereas only (n + 1)/2 of them are independent.  In order to allow a computer of given memory  capacity to handle a larger matrix, the following procedure for inverting a symmetric matrix has been  devised.'),\n",
       " (496, 'A Set of Matrices for Testing Computer Programs'),\n",
       " (497,\n",
       "  \"Further Remarks on Line Segment Curve-Fitting Using Dynamic Programming \\n In a recent paper, Bellman showed how dynamic programming could be used to determine the solution  to a problem previously considered by Stone.  The problem comprises the determination, given N, of the  N points of subdivision of a given interval (a,B) and the corresponding line segments, that give the  best least squares fit to a function g(x) in the interval.  Bellman confined himself primarily to the  analytical derivation, suggesting briefly, however, how the solution of the equation derived for each  particular point of subdivision u(i) could be reduced to a discrete search.  In this paper, the computational  procedure is considered more fully, and the similarities to some of Stone's equations are indicated.   It is further shown that an equation for u(i) involving no minimization may be found.  In addition,  it is shown how Bellman's method may be applied to the curve-fitting problem when the additional constraints  are added that the ends of the line segments must be on the curve.\"),\n",
       " (498, 'Magic Square (Algorithm 117 & 118)'),\n",
       " (499, 'Permutation Generator (Algorithm 87)'),\n",
       " (500, 'PERMUTE (Algorithm 86)'),\n",
       " (501, 'JACOBI (Algorithm 85)'),\n",
       " (502, \"Simpson's Integration (Algorithm 84)\"),\n",
       " (503,\n",
       "  'Rational Roots of Polynomials with Integer Coefficients (Algorithm 78)'),\n",
       " (504, 'FACTORS (Algorithm 75)'),\n",
       " (505, 'Composition Generator (Algorithm 72)'),\n",
       " (506, 'PERMUTATION (Algorithm 71)'),\n",
       " (507, 'Partition, Quicksort, Find (Algorithm 63, 64, 65)'),\n",
       " (508, 'Matrix Inversion (Algorithm 58)'),\n",
       " (509, 'Matrix Inversion (Algorithm 58)'),\n",
       " (510, 'Ber or Bei Function (Algorithm 57)'),\n",
       " (511, 'A Set of Test Matrices (Algorithm 52)'),\n",
       " (512, 'Telescope 1 (Algorithm 37)'),\n",
       " (513, 'SIEVE (Algorithm 35)'),\n",
       " (514, 'Binomial Coefficients (Algorithm 19)'),\n",
       " (515, 'Rational Interpolation by Continued Fractions (Algorithm 18)'),\n",
       " (516, 'Matrix Inversion II (Algorithm 120)'),\n",
       " (517, 'Evaluation of Pert Network (Algorithm 119)'),\n",
       " (518, 'Magic Square (Odd Order) (Algorithm 118)'),\n",
       " (519, 'Magic Square (Even Order) (Algorithm 117)'),\n",
       " (520, 'Complex Division (Algorithm 116)'),\n",
       " (521, 'PERM (Algorithm 115)'),\n",
       " (522, 'Generation of Partitions with Constraints (Algorithm 114)'),\n",
       " (523, 'TREESORT (Algorithm 113)'),\n",
       " (524, 'Position of Point Relative to Polygon (Algorithm 112)'),\n",
       " (525, 'A Computer Technique for Handling Analysis of Variance'),\n",
       " (526, 'Character Manipulation in Fortran'),\n",
       " (527,\n",
       "  'The Description List of Concepts \\n A concept is defined as a class of objects whose members can be distinguished by processing  its properties.  Property is defined to mean a partition of the set of all objects into disjoint classes.   The formal definition of a concept is recursive in nature.  A concept is described by a list structure.   A one-to-one correspondence is established between the recursive definition of a concept and its description  list structure.  Like the definition, the description list structure of a concept is also built up from  elementary list structures by a recursive process.  The list structures obtained this way are compared  with the description list structure discussed by the author in a previous publication.'),\n",
       " (528, 'FORTRAN for Business Data Processing'),\n",
       " (529, 'Regression and Coded Patterns in Data Editing'),\n",
       " (530, 'A Computer Method for Radiation Treatment Planning'),\n",
       " (531,\n",
       "  'Person-Matching by Electronic Methods \\n Record linkage in the updating of files is accomplished in many establishments through the  use of a preassigned number, such as payroll number, customer number, or social security number.  In  vital and health records, however, a unique number is generally not preassigned to an individual for  purposes of reporting services received to the health department.  In order to determine whether different  physician reports refer to the same individual, name and other identification must be compared.  This  is a laborious operation which is subject to various errors because of name misspellings, changes of  name upon marriage, and other problems.  We are interested in the maintenance of a psychiatric case register  in Maryland, where many of the reports from over a hundred psychiatric agencies refer to the same patient.   These records must be linked in order to provide unduplicated counts of individuals under care and longitudinal  records of psychiatric history.  An earlier paper [1] describes our general procedures for register maintenance  by use of a digital computer (Honeywell 800).  Here we present in more detail our initial procedures  for the person-matching process in order to elicit comments and suggestions from persons who have had  experience in matching.'),\n",
       " (532,\n",
       "  'On the Computation of Rational Approximations to Continuous Functions'),\n",
       " (533,\n",
       "  'Digital Synthesis of Correlated Stationary Noise \\n In this note we propose a method of generating stationary noise with a prescribed auto-covariance  function by digital methods.  The need for such a technique often arises in testing the performance of  data processing and engineering systems, where inputs corrupted with correlated noise (of a known form)  are required.  The technique is quite simple and produces strict-sense stationary noise which agrees  approximately with R(t), the prescribed auto-covariance function (acf), over an interval [-T(0), T(0)].   The method consists of approximating the spectral density by a periodic process with spectral lines,  and then synthesizing the periodic noise with random phases and appropriate amplitudes.  In order to  simplify discussion of the statistical properties of the noise generated, the technique is first presented  in terms of exact harmonic analysis.  In practice, discrete harmonic analysis as presented in the third  section is used.'),\n",
       " (534, 'Quick Calculation of Jacobian Elliptic Functions'),\n",
       " (535,\n",
       "  'Triangular Walk Pattern for the Down-hill Method of Solving a Transcendental Equation'),\n",
       " (536,\n",
       "  'Nonlinear Regression and the Solution of Simultaneous Equations \\n If one has a set of observables (Z1,...,Zm) which are bound in a relation with certain parameters  (A1,...,An) by an equation S(Z1,...;A1,...)=0, one frequently has the problem of determining a set of  values of the Ai which minimizes the sum of squares of differences between observed and calculated values  of a distinguished observable, say Zm.  If the solution of the above equation for Zm,  Zm=N(Z1,...;A1,...)  gives rise to a function N which is nonlinear in the Ai, then one may rely on a version of Gaussian regression  [1,2] for an iteration scheme that converges to a minimizing set of values.  It is shown here that this  same minimization technique may be used for the solution of simultaneous (not necessarily linear) equations.'),\n",
       " (537,\n",
       "  'A Machine Program for Theorem-Proving \\n The program of a proof procedure is discussed in connection with trial runs and possible improvements.'),\n",
       " (538, 'Quantum Mechanical Integrals of Slater-Type Orbitals (Algorithm 110)'),\n",
       " (539, 'Definite Exponential Integrals B (Algorithm 109)'),\n",
       " (540, 'Definite Exponential Integrals A (Algorithm 108)'),\n",
       " (541, \"Simpson's Integration (Algorithm 84)\"),\n",
       " (542, 'FACTORS (Algorithm 75)'),\n",
       " (543, 'Interpolation by Aitken (Algorithm 70)'),\n",
       " (544, ' Ber or Bei Function (Algorithm 57)'),\n",
       " (545,\n",
       "  'Adjust Inverse of a Matrix when an Element is Perturbed (Algorithm 51)'),\n",
       " (546, 'Logarithm of a Complex Number (Algorithm 48)'),\n",
       " (547, 'Gamma Function (Algorithm 34)'),\n",
       " (548, 'Molecular-Orbital Calculation of Molecular Interactions'),\n",
       " (549, 'Quantum Mechanical Integrals of Slater-Type Orbitals'),\n",
       " (550, 'Definite Exponential Integrals B (Algorithm 109)'),\n",
       " (551, 'Definite Exponential Integrals A (Algorithm 108)'),\n",
       " (552, \"Gauss's Method (Algorithm 107)\"),\n",
       " (553, 'Complex Number to a Real Power (Algorithm 106)'),\n",
       " (554, 'Newton Maehly, (Algorithm 105)'),\n",
       " (555, 'Reduction to Jacobi (Algorithm 104)'),\n",
       " (556, 'On Translation of Boolean Expressions'),\n",
       " (557, 'Simulation of Computer Timing Device'),\n",
       " (558,\n",
       "  'A Modified Inversion Procedure for Product Form of the Inverse Linear Programming Codes \\n This paper describes a new algorithm for the selection of the pivot row in matrix inversion  when using the product form of the inverse.  This algorithm has been developed for linear programming  codes; however, it would be valuable for the inversion of any non-dense matrix.  The procedures described  in this paper have been thoroughly tested and have been in operation on the Esso Research and Engineering  IBM 7090 computer for nine months.  Substantial computer cost savings have been realized because of this  procedure.'),\n",
       " (559,\n",
       "  'Solution of Eigenvalue Problems With Approximately Known Eigenvectors'),\n",
       " (560, 'Communication Between Independently Translated Blocks'),\n",
       " (561, 'Analytic Differentiation By Computer'),\n",
       " (562, 'AVINT (Algorithm 77)'),\n",
       " (563, 'Sorting Procedures (Algorithm 76)'),\n",
       " (564, 'CRAM (Algorithm 67)'),\n",
       " (565, 'INVRS (Algorithm 66)'),\n",
       " (566, 'Matrix Inversion (Algorithm 58)'),\n",
       " (567, 'Logarithm of a Complex Number (Algorithm 48)'),\n",
       " (568, 'Exponential of a Complex Number (Algorithm 46)'),\n",
       " (569, 'Binomial Coefficients (Algorithm 19)'),\n",
       " (570, \"Simpson's Rule Integrator (Algorithm 103)\"),\n",
       " (571, 'Permutation in Lexicographical Order (Algorithm 102)'),\n",
       " (572, 'Add Item to Chain-Linked List (Algorithm 100)'),\n",
       " (573, 'Remove Item From Chain-Linked List (Algorithm 101)'),\n",
       " (574, 'Evaluation of Jacobi Symbol (Algorithm 99)'),\n",
       " (575, 'Evaluation of Definite Complex Line Integrals (Algorithm 98)'),\n",
       " (576, 'Shortest Path (Algorithm 97)'),\n",
       " (577, 'ANCESTOR (Algorithm 96)'),\n",
       " (578, 'Generation of Partitions in Part-Count Form (Algorithm 95)'),\n",
       " (579, 'COMBINATION (Algorithm 94)'),\n",
       " (580, 'General Order Arithmetic (Algorithm 93)'),\n",
       " (581, 'A Note on Sampling a Tape-File'),\n",
       " (582, 'One Lost Bit'),\n",
       " (583, 'A Redundancy Check for ALGOL Programs'),\n",
       " (584, 'Report on the Algorithmic Language FORTRAN II'),\n",
       " (585,\n",
       "  'Initial Experience With an Operating Multiprogramming System \\n The Lewis Research Center has been using various forms and degrees of program simultaneity  in the operation of its modified Sperry-Rand Univac Scientific Model 1103 computer during the last five  years.  This simultaneity has evolved from an initial achievement of self-searching input and output  to the automatic time sharing of independently coded problems.  Several important machine and program  system modifications were necessary to accomplish this evolution.  Several additional modifications,  although not required, were added to facilitate ease of coding and operation.  All modifications had  to proceed at a relatively temperate pace to insure that the basic data-reduction work load of the computing  center was completed on schedule.  Some educationally valuable mistakes were made, and their suggested  cures often pointed the way to useful future improvements or emphasized some of the basic principles  of a multiprogramming system.  The material that follows is a description of the evolution of the programming  and hardware system which has developed into the present multiprogramming system at Lewis research Center.'),\n",
       " (586,\n",
       "  'Simultaneous System of Equations and Matrix Inversion Routine (Algorithm 92)'),\n",
       " (587, 'Romberg Integration (Algorithm 60)'),\n",
       " (588, 'Chebyshev Curve-Fit (Algorithm 91)'),\n",
       " (589, 'Evaluation of the Fresnel Cosine Integral (Algorithm 90)'),\n",
       " (590, 'Evaluation of the Fresnel Sine Integral (Algorithm 89)'),\n",
       " (591,\n",
       "  'Evaluation of Asymptotic Expression for the Fresnel Sine and Cosine Integrals (Algorithm 88)'),\n",
       " (592, 'COBOL Batching Problems'),\n",
       " (593, 'An Introduction to a Machine-Independent Data Division'),\n",
       " (594, 'An Advanced Input-Output System for a COBOL Compiler'),\n",
       " (595,\n",
       "  \"Guides to Teaching COBOL \\n The teaching of COBOL can be divided into three main subject areas.  They are the syntax of  COBOL, the use of such syntax in solving any given problem, and programming concepts.  It is generally  accepted that some knowledge of the hardware and computer logic must be possessed by the programmer.   The teaching problem arises in determining how thoroughly a student must know the hardware and logic  for that computer for which he will write COBOL programs.  Unfortunately, historical data concerning  students' programming proficiency is almost non-existent and, at best, difficult to measure.  How then  might we approach solving this problem?\"),\n",
       " (596,\n",
       "  'Floating-Point Arithmetic in COBOL \\n In this paper the basic operations of floating-point arithmetic are examined and COBOL procedures  for carrying these out are given, along with specification of working storage.  The paper concludes with  an example in which these procedures are used.'),\n",
       " (597, 'Modular Data Processing Systems Written in COBOL'),\n",
       " (598,\n",
       "  'The COBOL Librarian - A Key to Object Program Efficiency \\n Many answers to the question \"How may a COBOL Compiler be forced into the generation of an  efficient object program?\"  The purpose of this article is to present one possible answer: the creation  and full utilization of a well-constructed COBOL Library.'),\n",
       " (599, 'A Report Writer For COBOL'),\n",
       " (600, 'Syntactical Charts of COBOL 61'),\n",
       " (601, 'Interim Report on Bureau of Ships COBOL Evaluation Program'),\n",
       " (602, 'COBOL and Compatibility'),\n",
       " (603, 'Basic Elements of COBOL 61'),\n",
       " (604, 'Why COBOL?'),\n",
       " (605,\n",
       "  'Computer Simulation Of City Traffic \\n In simulating traffic flow on city streets, the National Bureau of Standards has used data  processing techniques to tabulate and make motion pictures of vehicle movements in the model.  Each vehicle  is assigned a digital identification giving points of entry and exit, type of vehicle, desired speed,  and actual speed, in proportions simulating field data.  Changes in the model can be made to observe  their consequences and to determine the ability of a real street to carry loads expected in the future.'),\n",
       " (606,\n",
       "  'A Method for Eliminating Ambiguity Due to Signal Coincidence in Digital Design'),\n",
       " (607, 'The Calculation of Easter...'),\n",
       " (608, 'Permutation (Algorithm 71)'),\n",
       " (609, 'Permutation (Algorithm 71)'),\n",
       " (610, 'SIEVE (Algorithm 35)'),\n",
       " (611, 'Permutation Generator (Algorithm 87)'),\n",
       " (612, 'Permute (Algorithm 86)'),\n",
       " (613, 'JACOBI (Algorithm 85)'),\n",
       " (614, \"Simpson's Integration (Algorithm 84)\"),\n",
       " (615,\n",
       "  'Addressing Multidimensional Arrays \\n A useful method of representing a function of n variables is to consider the function to assume  its values at selected points in n-dimensional space.  Although this picture is of value to the analyst,  the elements of an n-dimensional array must exist in conventional storage as a linear array or vector.   The means of performing the transformation of a set of indices locating on array element in n-space  to the location (address) of the element in its storage vector is the subject of this paper.  It is noted  that the index address transformation is computationally identical to the conversion of a number from  a fixed to a mixed radix number system.  Several ways of implementing the transformation are described.'),\n",
       " (616,\n",
       "  'An Information Algebra - Phase I Report-Language Structure Group of the CODASYL Development Committee \\n This report represents the results of the first phase of the work of the Language Structure  Group.  The goal of this work is to arrive at a proper structure for a machine-independent problem-defining  language, at the systems level of data processing.  The report is based, for the most part, on a mathematical  model called \"An Information Algebra\" developed primarily by R. Bosak.  It is hoped that this report  will be read (a) with avid interest by programming language designers and implementors, and all those  interested in developing a theoretical approach to data processing; (b) with interest and understanding  by professional programmers and systems analysts; and (c) with appreciation by the businessman-analyst-manager.   The authors have not attempted an exhaustive discourse in this report.  Rather, they have tried to present  a philosophy to the professional people who are vitally concerned with providing a working language for  the systems analyst\\'s use.  They trust that the ideas in this report will stimulate others to think along  similar lines.  Questions and comments will be welcomed, and can be addressed to any of the members of  the Language Structure Group:  Robert Bosak, System Development Corporation;  Richard F. Clippinger,  Honeywell EDP Division;  Carey Dobbs, Remington Rand Univac Division;  Roy Goldfinger (Chairman), IBM  Corporation;  Renee B. Jasper, Navy Management Office;  William Keating, National Cash Register;  George  Kendrick, General Electric Company;  Jean E. Sammet, IBM Corporation.'),\n",
       " (617,\n",
       "  'POSEIDON \\n Any computer that forms part of a control system-whether completely automatic or partly human-must  work at the same speed as the control system.  It must perform its calculations or data processing fast  enough for the results to be available at the required instants in the action of the control system.   This known as working in \"real time.\"'),\n",
       " (618,\n",
       "  'Computers- The Key to Total Systems Control: An Industrial Viewpoint \\n Man-Man-machine processes are characterized in five main types, and the markets for each type are  shown for 1950 and 1960 and estimated for 1970.'),\n",
       " (619,\n",
       "  \"Retrieval of Misspelled Names in an Airlines Passenger Record System \\n This paper discusses the limited problem of recognition and retrieval of a given misspelled  name from among a roster of several hundred names, such as the reservation inventory for a given flight  of a large jet airliner.  A program has been developed and operated on the Telefile (a stored-program  core and drum memory solid-state computer) which will retrieve passengers' records successfully, despite  significant misspellings either at original entry time or at retrieval time.  The procedure involves  an automatic scoring technique which matches the names in a condensed form. Only those few names most  closely resembling the requested name, with their phone numbers annexed, are presented for the agents  final manual selecton.  The program has successfully isolated and retrieved names which were subjected  to a number of unusual (as well as usual) misspellings.\"),\n",
       " (620, 'RATFACT (Algorithm 78)'),\n",
       " (621, 'Romberg Integration (Algorithm 60)'),\n",
       " (622, 'Optimal Classification of Objects (Algorithm 83)'),\n",
       " (623, 'Economising a Sequence 2 (Algorithm 82)'),\n",
       " (624, 'Economising a Sequence 1 (Algorithm 81)'),\n",
       " (625, 'Reciprocal Gamma Function of Real Argument (Algorithm 80)'),\n",
       " (626,\n",
       "  'A Method of Representation, Storage and Retrieval of 13 Random Codes in a 4-Digit Number or 16  Random Codes in a 5-Digit Number'),\n",
       " (627, 'Knotted List Structures'),\n",
       " (628,\n",
       "  'On a Floating-Point Number Representation For Use with Algorithmic Languages'),\n",
       " (629, 'On a Wired-In Binary-to-Decimal Conversion Scheme'),\n",
       " (630,\n",
       "  'An Evaluation of Autocode Readability \\n Of the many requirements of an autocode, the pair of requirements \"easy to read\" and \"easy  to write\" are not often compatible.  This paper argues that readability can be added automatically in  the translation process so that the programmer can enjoy the utmost economy of expression, while for  management a full and valid COBOL version is printed to give all the advantages of readability and compatibility.'),\n",
       " (631,\n",
       "  'Automatic-Programming-Language Translation Through Syntactical Analysis*'),\n",
       " (632, 'Vectorcardiographic Diagnosis With The Aid of ALGOL'),\n",
       " (633,\n",
       "  'Simulation and Analysis of Biochemical Systems (III. Analysis and Pattern Recognition)'),\n",
       " (634, 'Manipulation of Trees in Information Retrieval*'),\n",
       " (635, 'A Note on Multiplying Boolean Matrices'),\n",
       " (636, 'Tape Splitting in an Iterative Program'),\n",
       " (637,\n",
       "  'A NELIAC-Generated 7090-1401 Compiler \\n NELIAC systems for several different machines have been generated using the original NELIAC  system developed at the Naval Electronics Laboratory, San Diego, in 1958.  A basic \"bootstrap\" process  was used to generate all but the first, i.e. the systems were described in the NELIAC language and generated  by an existing NELIAC compiler.  This experience has shown there is no inherent difficulty in \"building  compilers with compilers\"; indeed, it pointed out many advantages in using a POL for constructing programming  systems.  This report presents the results of a project completed in May, 1961 in which the NELIAC system  was used to generate a compiler for the IBM 1401.  The 1401 compiler, which runs on the 7090 and produces  1401 programs, was described in the NELIAC language and generated with 7090 NELIAC system.  The reduction  in programming time and the improvement in documentation of the system were very significant.'),\n",
       " (638, 'SURGE: A Recoding of the COBOL Merchandise Control Algorithm'),\n",
       " (639, 'Difference Expression Coefficients (Algorithm 79)'),\n",
       " (640,\n",
       "  'Rational Roots of Polynomials with Integer Coefficients (Algorithm 78)'),\n",
       " (641, 'Interpolation, Differentiation, and Integration (Algorithm 77)'),\n",
       " (642, 'An Introduction to ALGOL '),\n",
       " (643,\n",
       "  'Simulation and Analysis of Biochemcial Systems (II. Solution of Differential Equations)'),\n",
       " (644,\n",
       "  'A String Language for Symbol Manipulation Based on ALGOL 60 \\n An artificial computer programming language is proposed for describing the manipulation of  strings of characters and symbols.  The concept of strings, introduced in the ALGOL 60 report, is extended  by adding: (1) the declaration of strings, substrings, and string arrays with explicit lengths; (2) the  ability to concatenate and shift strings; and (3) the ranking of symbols for comparing stings in Boolean  relations.  A primer or informal description of the language is followed by examples, a description of  experiments with the language on an IBM 704 computer, and a formal description which, taken with the  ALGOL 60 Report, defines the proposed string language.'),\n",
       " (645, 'INVRS (Algorithm 66)'),\n",
       " (646, 'Inverse of a Finite Segment of the Hilbert Matrix (Algorithm 50)'),\n",
       " (647, 'Numerical Solution of the Polynomial Equation (Algorithm 30)'),\n",
       " (648, 'Sorting Procedures (Algorithm 76)'),\n",
       " (649, 'FACTORS (Algorithm 75)'),\n",
       " (650, 'Curve Fitting with Constraints (Algorithm 74)'),\n",
       " (651, 'A Survey of Languages and Systems for Information Retrieval'),\n",
       " (652, 'Use of Semantic Structure in Information Systems'),\n",
       " (653,\n",
       "  'Translation of Retrieval Requests Couched in a \"Semiformal\" English-Like Language*'),\n",
       " (654, 'Language Problems Posed by Heavily Structured Data'),\n",
       " (655,\n",
       "  'COMIT as an IR Language \\n Many of the features that make COMIT a good all around symbol manipulation language also render  it well suited to various types of information retrieval programs.  Presented here is a general discussion  of this unique and different programming language and an examination of some of its applications.'),\n",
       " (656,\n",
       "  'An Information System With The Ability To Extract Intelligence From Data'),\n",
       " (657, 'Information Structures for Processing and Retrieving'),\n",
       " (658, 'Discussion-The Pros and Cons of a Special IR Language'),\n",
       " (659, 'Reversion of Series (Algorithm 193)'),\n",
       " (660, 'More Test Matrices for Determinants and Inverses (Pracnique)'),\n",
       " (661,\n",
       "  'Indexing and the Lambda-Notation \\n Some methods of indexing sequentially stored elements of sparse multi-dimensional arrays are  described in the scheme A notation.'),\n",
       " (662, 'Shuttle Sort (Algorithm 175)'),\n",
       " (663, 'Determinant (Algorithm 159)'),\n",
       " (664, 'Assignment (Algorithm 27)'),\n",
       " (665, 'Gauss-Seidel (Algorithm 220)'),\n",
       " (666, 'Topological Ordering for Pert Networks (Algorithm 219)'),\n",
       " (667, 'Kutta Merson (Algorithm 218)'),\n",
       " (668, 'Minimum Excess Cost Curve (Algorithm 217)'),\n",
       " (669, 'A Specification of JOVIAL'),\n",
       " (670,\n",
       "  \"Some Legal Implications of the Use of Computers in the Banking Business \\n The introduction of computers in to the banking business has a wide variety of legal implications  that merit careful attention at this very early stage.  The industry is highly regulated by government  and, hence, is subject to many statutes and regulations.  It also is affected by important common law  rules established by courts.  The legal ramifications involve not only the mechanization itself, but  also the very significant, economically attractive phenomenon of off premises processing.  It is essential  to identify and provide for many legal aspects right now, before systems and practices crystallize, in  order to avoid the later impact of unanticipated physical complications and expense.  The legal aspects  of computerization in the banking business are especially diverse.  In some states, there might be the  basic question whether banks are authorized by law to invest in the new facilities, either directly or  through cooperatives.  More challenging are questions relating to off-premises processors, particularly  with respect to the obligation not to disclose information concerning a bank's customers, the adequacy  of fidelity bond coverage, the extent of liability for improper refusal to pay a check, and susceptibility  to regulation by government agencies.  Also pertinent is the propriety of data processing by banks for  nonbank entities and particularly of the rendering of that service without charge for bank depositors.\"),\n",
       " (671,\n",
       "  \"TELEFILE-A Case Study of an On-Line Savings Bank Application \\n The development of an on-line computer system for a savings bank institution is traced from  the early conceptual needs of the bank to the consummation of design by The Teleregister Corporation.   Both bank and equipment criteria are specified which led to the development of the Telefile System of  The Teleregister Corporation.  Operation of the on-line and off-line programs are described and statistics  are cited for reliability and performance of the system.  Benefits to the bank are discussed from the  banker's point of view; an indication of future trends in the on-line savings bank field is also discussed.\"),\n",
       " (672, 'Recent Developments Affecting ADP in Tax Administration'),\n",
       " (673, 'Account Classification at Automating Banks'),\n",
       " (674,\n",
       "  'Application of IBM 1620 EDP Methods to the Calculation of the Formation Constants of Complex  Irons'),\n",
       " (675,\n",
       "  'Coding Clinical Laboratory Data For Automatic Storage and Retrieval \\n A series of clinical laboratory codes have been developed to accept and store urin analysis, blood  chemistry, and hematology test results for automatic data processing.  The codes, although constructed  as part of a computerized hospital simulation, have been able to handle the results of every laboratory  test that they have encountered.  The unique feature of these codes is that they can accept conventionally  recorded qualitative as well as quantitative test results.  Consequently, clinical test results need  not be arbitrarily stratified, standardized, or altered in any way to be coded.  This paper describes  how the codes were developed and presents a listing of the urin analysis codes.  Five criteria used in developing  the codes are outlined and the problem of multiple-synonymous terminology is discussed.  A solution to  the problem is described.  Flexible, computer-produced, composite laboratory reports are also discussed,  along with reproduction of such a report. The paper concludes that even though many problems remain unsolved,  the next ten years could witness the emergence of a practical automated information system in the laboratory.'),\n",
       " (676, 'On the Computation of a Certain Type of IncompleteBeta Functions'),\n",
       " (677,\n",
       "  'Length of Strings for a Merge Sort \\n Detailed statistics are given on the length of maximal sorted strings which result form the  first (internal sort) phase of a merge sort onto tapes.  It is shown that the strings produced by an  alternating method (i.e. one which produces ascending and descending strings alternately) tend to be  only three-fourths as long as those in a method which produces only ascending strings, contrary to statements  which have appeared previously in the literature.  A slight modification of the read-backward polyphase  merge algorithm is therefore suggested.'),\n",
       " (678,\n",
       "  'Optimizing Bit-time Computer Simulation \\n A major component of a bit-time computer simulation program is the Boolean compiler.  The compiler  accepts the Boolean functions representing the simulated computer\\'s digital circuits, and generates corresponding  sets of machine instructions which are subsequently executed on the \"host\" computer.  Techniques are  discussed for increasing the sophistication of the Boolean compiler so as to optimize bit-time computer  simulation.  The techniques are applicable to any general-purpose computer.'),\n",
       " (679,\n",
       "  'Recent Improvements in MADCAP \\n MADCAP is a programming language admitting subscripts, superscripts and certain forms of displayed  formulas.  The basic implementation of this language was described in a previous paper [MADCAP: A scientific  compiler for a displayed formula textbook language, Comm. ACM 4 (Jan. 61), 31-36].  This paper discusses  recent improvements in the language in three areas: complex display, logical control, and subprogramming.   In the area of complex display, the most prominent improvements are a notation for integration and for  the binomial coefficients.  In the area of logical control the chief new feature is a notation for variably  nested looping.  The discussion of subprogramming is focused on MADCAP\\'s notation for and use of \"procedures.\"'),\n",
       " (680, 'An Error-Correcting Parse Algorithm'),\n",
       " (681, 'Flexible Abbreviation of Words in a Computer Language'),\n",
       " (682, 'Recursive programming in FORTRAN II'),\n",
       " (683, 'A Serial Technique to Determine Minimum Paths'),\n",
       " (684, 'Interpolation, Differentiation, and Integration (Algorithm 77)'),\n",
       " (685, 'Euler Summation (Algorithm 8)'),\n",
       " (686, 'Smooth (Algorithm 216)'),\n",
       " (687, 'Shanks (Algorithm 215)'),\n",
       " (688, 'q-Bessel Functions In(t)(Algorithm 214)'),\n",
       " (689,\n",
       "  'Report of a Visit to Discuss Common Programming Languages in Czechoslovakia and Poland, 1963'),\n",
       " (690,\n",
       "  'USA Participation in an International Standard glossary on Information Processing'),\n",
       " (691,\n",
       "  'A Description of the APT Language \\n The APT (Automatically Programmed Tools) language for numerical control programming is described  using the metalinguistic notation introduced in the ALGOL 60 report.  Examples of APT usage are included.   Presented also are an historical summary of the development of APT and a statement concerning its present  status.  '),\n",
       " (692, 'On the Inverse of a Test Matrix'),\n",
       " (693,\n",
       "  'An Extension of Fibonaccian Search To Several Variables \\n A technique which uses Fibonaccian search concepts has been developed to solve optimization  problems involving unimodal functions of several variables.  The technique has not been proven to be  optimal in the sense that the one-dimensional Fibonaccian search is.  However, it is valuable for certain  kinds of calculations.'),\n",
       " (694,\n",
       "  'A Comparison of Disks and Tapes \\n The principal characteristics of current magnetic disks and tape units are summarized and compared.   Some of the characteristics of disk files are illustrated in a sorting example and compared to a tapesort.   The conclusion is presented that disk files are competitive to tapes in some important applications.'),\n",
       " (695,\n",
       "  'Use of the Disk File on Stretch \\n The paper begins by briefly describing the Stretch (IBM 7030) computer with special emphasis  given to the organization and operation of its input-output equipment.  Physical characteristics of the  two-disk system (4,194,304 72-bit words, 8 usec-per-word transmission rate, etc.) are noted.  Timing  limitations due to arm motion and disk rotation are discussed.  Applications of disk usage are discussed  separately for problem programs and for systems programs such as compilers and the supervisory program.  Approximately 260,000 words of disk storage are reserved for the storage of systems programs and the  subroutine library.  Problem programs, however, are not currently filed on the disk.  Certain programming  techniques are discussed for transmitting words between disk and core storage with minimum delaying and  interruption of the arithmetic unit.  Dumps on disk are considered for both recovery from computer malfunction  and for mathematical or physical developments during the calculation.  Some comments are made regarding  the reliability, economics, utility and weaknesses or limitations of the disk system.  Several possible  future applications are noted which appear to have disk connotations.'),\n",
       " (696,\n",
       "  \"An Automatic Data Acquisition and Inquiry System Using Disk Files \\n Lockheed Missiles and Space Company has installed a large-scale Automatic Data Acquisition  (ADA) system which ties together the Company's manufacturing facilities located in Van Nuys and Sunnyvale,  California.  The system includes over 200 remote Input Stations which collect and transmit Company operating  data to a central Data Processing Center.  Two RCA 301 EDP Systems are used to record and control the  flow of data transmitted to the Data Processing Center.  A large capacity RCA 366 Data Disc File is used  to store information required to provide up-to-date information in response to inquiries received from  remotely located Inquiry Stations.  In addition to storage of data on the disk files, the system automatically  records all incoming and outgoing data on magnetic tape to be used as input to the Company's conventional  off-line business data processing applications.\"),\n",
       " (697,\n",
       "  'A Numerical Method for the Determination of Moving Field Isodose Curves for Treatment Planning  in Radiotherapy'),\n",
       " (698,\n",
       "  \"DATA-DIAL: Two-Way Communication with Computers From Ordinary dial Telephones \\n An operating system is described which allows users to call up a remotely located computer  from ordinary dial telephones.  No special hardware or connections are required at the users' telephones.   Input to the computer is through the telephone dial;output from the computer is in spoken form.  Results  of a test with telephones in the Boston area are reported.\"),\n",
       " (699,\n",
       "  'A Contour-Map Program for X-Ray Crystallography \\n A FORTRAN program is described for use with the IBM 7090 system and an X, Y-plotter to produce  a contour map.  A matrix of points evenly spaced in each dimension is contoured.  Scale factors along  the axes may be different and the axes need not be perpendicular.'),\n",
       " (700, 'Hermite Interpolation (Algorithm 210)'),\n",
       " (701, 'Shuttle Sort (Algorithm 175)'),\n",
       " (702, 'Assign (Algorithm 173)'),\n",
       " (703, 'Assign (Algorithm 173)'),\n",
       " (704,\n",
       "  'Combinatorial of M Things Taken One At A Time Two At A Time, Up To N At A Time (Algorithm 161)'),\n",
       " (705, 'Combinatorial Of M Things Taken N At A Time (Algorithm 160)'),\n",
       " (706, 'Fourier Series Approximation (Algorithm 157)'),\n",
       " (707, 'Erf(x) (Algorithm 123)'),\n",
       " (708, 'Evaluation of the Fresnel Integrals (Algorithm 88, 89, 90)'),\n",
       " (709, 'Assignment (Algorithm 27)'),\n",
       " (710, 'Fresnel Integrals (Algorithm 213)'),\n",
       " (711, 'Frequency Distribution (Algorithm 212)'),\n",
       " (712, 'Hermite Interpolation (Algorithm 211)'),\n",
       " (713, 'Lagrangian Interpolation (Algorithm 210)'),\n",
       " (714, 'Gauss (Algorithm 209)'),\n",
       " (715, 'Discrete Convolution (Algorithm 208)'),\n",
       " (716, 'Stringsort (Algorithm 207)'),\n",
       " (717,\n",
       "  'Partitioning Algorithms for Finite Sets \\n The partitions of a set with n elements are represented by certain n-tuples of positive integers.   Algorithm are described which generate without repetitions the n-tuples corresponding to: (1) all partitions  of the given set, (2) all partitions of the given set into m or fewer sets (1 <= m <= n), and (3) all  partitions of the given set into exactly m sets (1 <= m <= n).'),\n",
       " (718,\n",
       "  'An Experiment in Automatic Verification of Programs  \\n How effective is a compiler at replacing explicit verification, and what is the cost of this  technique?'),\n",
       " (719,\n",
       "  'Variable Width Stacks \\n Character addressable, variable field computers permit ready establishment and manipulation  of variable width stacks.  Single machine commands may push variable field items down into such stacks  or pop them up.  The availability of a variety of field delimiters allows the machine to push down or  pop up more than one variable width item with one command.  Since these stacking operations can be made  the basis of compiler decoding algorithms the proper use of machines of this class for compilation has  advantages over machines with fixed-length words.'),\n",
       " (720, 'Format-Free Input in FORTRAN'),\n",
       " (721,\n",
       "  'Report on Proposed American Standard Flowchart Symbols for Information Processing \\n This paper presents the essential contents of the Proposed American Standard Flowchart Symbols  for Information Processing.  This is the first proposed standard prepared by Subcommittee X3.6 on Problem  Description and Analysis of the American Standards Association (ASA).'),\n",
       " (722, 'ALCOR Group Representation of ALGOL Symbols'),\n",
       " (723, 'ECMA Subset of ALGOL 60'),\n",
       " (724,\n",
       "  'A Profile of the Programmer \\n Synopsis: 549 members of the ACM participated in a study concerned primarily with the attitudes  of programmers toward their careers and jobs.  A very high percentage of programmers have apparently  entered their careers by accident; it has proven a happy choice for most and they expect to remain in  the field during the next five years.  Their principal job satisfactions relate to the nature of their  work, and mostfind their jobs offer high level of professional interest and good working conditions.   Salary and advancement prospects, however,are not as satisfactory.  More than half report a positive  attitude toward programmers and programming on the part of their organizations.  Turnover among themselves  is attributed primarily to poor management-salary is seen as the principal motivating factor in turnover  among other programmers.  Nature of the work offered and salary are principal determinants in accepting  a new job.  Programmers are less mobile than expected.  Programmers tend to see their colleagues in a  favorable light, on the whole.  Personalities seem to vary with function, systems programmers differing  from applications programmers.  Four principal problems for programming in the immediate future are listed  by participants: languages, personnel, various specific applications and techniques, and building programming  as a profession.'),\n",
       " (725, 'Group Participation Computer Demonstration'),\n",
       " (726,\n",
       "  'A General Program for the Analysis of Square and Rectangular Lattice Designs \\n This paper describes a general-purpose program that will handle those incomplete block designs  known as square and rectangular lattices.  Flow diagrams are given so that the method of calculation  may be programmed for any digital computer.'),\n",
       " (727,\n",
       "  'On the Approximate Solution of Delta(u)=F(u) \\n Three-dimensional Dirichlet problems for Delta(u)=F(u), Fu >= 0, are treated numerically by  an exceptionally fast, exceptionally accurate numerical method.  Programming details, numerous examples  and mathematical theory are supplied.Extension of the method in a natural way to n-dimensional problems  is indicated by means of a 4-dimensional example.'),\n",
       " (728,\n",
       "  'Computer-Drawn Flowcharts* \\n To meet the need for improved documentation of written computer programs, a simple system for  effective communication is presented, which has shown great promise.  The programmer describes his program  in a simple format, and the computer prepares flow charts and other cross-referenced listings from this  input.  The description can be kept up-to-date easily, and the final output clearly explains the original  program.  The system has also proved to be a valuable debugging and coding aid.'),\n",
       " (729, 'A Generalization of ALGOL'),\n",
       " (730,\n",
       "  'MIRFAG: A Compiler Based on Standard Mathematical Notation And Plain English \\n A pilot version of the compiler MIRFAG, now in operation, is described.  The chief features  of the system, which is intended for the solution of scientific problems, are the presentation of mathematical  formulas entirely in standard textbook notation.  The use of plain English for organizational instructions,  automatic error diagnosis indicating the actual location of the error in the uncompiled program, and  an attempt to minimize that fragmentation of the original problem statement which is a normal feature  of programming systems.'),\n",
       " (731,\n",
       "  'Symmetric List Processor  \\n A list processing system in which each list cell contains both a forward and a backward link  as well as a datum is described.  This system is intended for imbeding in higher level languages capable  of calling functions and subroutines coded in machine language.  The presentation is in the form of FORTRAN  programs depending on only a limited set of FORTRAN programs depending on only a limited set of \"primitive\"  machine language subroutines which are also defined.  Finally, a set of field, particularly character,  manipulation primitives are given to round out the system.'),\n",
       " (732, 'Monte Carlo Inverse (Algorithm 166)'),\n",
       " (733,\n",
       "  'Newton Interpolation with Forward Divided Differences (Algorithm 169)'),\n",
       " (734,\n",
       "  'Newton Interpolation with Backward Divided Differences (Algorithm 168)'),\n",
       " (735, 'Calculation of Confluent Divided Differences (Algorithm 167)'),\n",
       " (736, 'Modified Hankel Functions (Algorithm 163)'),\n",
       " (737, 'Exponentiation of Series (Algorithm 158)'),\n",
       " (738, 'Fourier Series Approximation (Algorithm 157)'),\n",
       " (739, 'MINIFUN (Algorithm 129)'),\n",
       " (740, 'INTEREST (Algorithm 45)'),\n",
       " (741, 'Evaluation of Determinant (Algorithm 41)'),\n",
       " (742, 'Evaluation of Determinant (Algorithm 41)'),\n",
       " (743, 'ARCCOSIN (Algorithm 206)'),\n",
       " (744, 'ATIVE (Algorithm 205)'),\n",
       " (745, 'STEEP2 (Algorithm 204)'),\n",
       " (746, 'STEEP1 (Algorithm 203)'),\n",
       " (747, 'Generation of Permutations in Lexicographical Order (Algorithm 202)'),\n",
       " (748, 'A Semi-Iterative Process for Evaluating Arctangents'),\n",
       " (749, 'Note onStochastic Matrices'),\n",
       " (750, 'PEI Matrix Eigenvectors'),\n",
       " (751, 'A Note on a Set of Test Matrices for Inversion'),\n",
       " (752, 'Closing Out a Print Tape'),\n",
       " (753,\n",
       "  'A Procedure for Converting Logic Table Conditions into an Efficient Sequence of Test Instructions'),\n",
       " (754, 'Ye Indiscreet Monitor'),\n",
       " (755,\n",
       "  'An Exponential Method of Numerical Integration of Ordinary Differential Equations \\n A formula for numerical integration is prepared, which involves an exponential term.  This  formula is compared to two standard integration methods, and it is shown that for a large class of differential  equations, the exponential formula has superior stability properties for large step sizes.  Thus this  formula may be used with a large step size to decrease the total computing time for a solution significantly,  particularly in those engineering problems where high accuracy is not needed.'),\n",
       " (756, 'A Computer Program for Editing the News'),\n",
       " (757, 'Simulation of a Traffic Network'),\n",
       " (758,\n",
       "  'Skeletal Structure of PERT and CPA Computer Programs \\n An introduction to the inner mechanics of PERT and CPA computer programs is provided.  The  major components of these programs as well as their purposes and interrelationships are outlined.'),\n",
       " (759,\n",
       "  'Continued Operation Notation for Symbol Manipulation and Array Processing \\n A brief account is given of a notational device that is very useful in the formal representation  of syntaxes, string relationships and string transformation procedures and also of computing procedures  that deal with arrays of functions of many variables.  The device consists of the use of certain \"continued  operation\" or \"collective\" symbols that are analogous to the summation symbol (Sigma) and continued multiplication  symbol (Pi) of conventional mathematics.'),\n",
       " (760, 'Dialects of FORTRAN'),\n",
       " (761,\n",
       "  'A Note on the Dangling Else in ALGOL 60 \\n Some revisions of ALGOL 60 are proposed, which not only eliminate certain ambiguous statements  but also add some convenience to the language.  A discussion of the background of the problem and a sketch  of a proof that the ambiguities have been removed is included.'),\n",
       " (762, 'Some Remarks on the Syntax of Symbolic Programming Languages'),\n",
       " (763, 'A Syntax Controlled Generator of Formal Language Processors'),\n",
       " (764, 'Reduction of a Matrix Containing Polynomial Elements (Algorithm 170)'),\n",
       " (765, 'Orthogonal Polynomial Least Squares Surface Fit (Algorithm 164)'),\n",
       " (766, 'XY-move Plotting (Algorithm 162)'),\n",
       " (767,\n",
       "  'Certification of Algorithm 161 Combinatorial of M Things Taken One at a Time, Two at a Time, Up to N at a Time [M. L. Wolfson and H. V. Wright, Comm. ACM, Apr. 1963]'),\n",
       " (768,\n",
       "  'Certification of Algorithm 160 Combinatorial of M Things Taken N at a Time [M. L. Wolfson and H. V. Wright, Comm. ACM, Apr. 1963]'),\n",
       " (769, 'Algebra of Sets (Algorithm 156)'),\n",
       " (770, 'Combination in Any Order (Algorithm 155)'),\n",
       " (771, 'Combination in Lexicographical Order (Algorithm 154)'),\n",
       " (772, 'GOMORY (Algorithm 153)'),\n",
       " (773, 'Matrix Inversion (Algorithm 140)'),\n",
       " (774, 'Jacobi (Algorithm 85)'),\n",
       " (775, 'Interpolation, Differentiation, and Integration (Algorithm 77)'),\n",
       " (776, 'Partition, Quicksort, and Find (Algorithm 62, 64, & 65)'),\n",
       " (777, 'A Set of Test Matrices (Algorithm 52)'),\n",
       " (778,\n",
       "  'Associated Legendre Functions of the First Kind for Real or Imaginary Arguments (Algorithm 47)'),\n",
       " (779, 'CROUT II (Algorithm 43)'),\n",
       " (780,\n",
       "  \"Algorithm 42 INVERT, Alg.107 Gauss's Method, Alg.120 Inversion II, and gjr\"),\n",
       " (781, 'Telescope 2 (Algorithm 38)'),\n",
       " (782, 'Telescope 1 (Algorithm 37)'),\n",
       " (783, 'Shellsort (Algorithm 201)'),\n",
       " (784, 'Normal Random (Algorithm 200)'),\n",
       " (785,\n",
       "  'Conversions Between Calendar Date And Julian day Number (Algorithm 199)'),\n",
       " (786, 'Adaptive Integration and Multiple Integration (Algorithm 198)'),\n",
       " (787, 'Matrix Division (Algorithm 197)'),\n",
       " (788,\n",
       "  \"Muller's Method for Finding Roots of an Arbitrary Function (Algorithm 196)\"),\n",
       " (789, 'Bandsolve (Algorithm 195)'),\n",
       " (790, 'Zersol (Algorithm 194)'),\n",
       " (791, 'Character Manipulation in 7090 Fortran'),\n",
       " (792,\n",
       "  'Multiple-Precision Binary-To-Decimal Integer Conversion Using Only Addition And Subtraction'),\n",
       " (793, 'Mapped List Structures'),\n",
       " (794,\n",
       "  'A List-Type Storage Technique for Alphameric Information \\n A method which is economic in terms of space and time is proposed for the storage and manipulation  of character strings of arbitrary length in a fixed word-length computer.  The method is illustrated  in an application to Algol-type identifiers in an Algol-like block structure.'),\n",
       " (795, 'Debugging Systems at the Source Language Level'),\n",
       " (796,\n",
       "  'SABRAG, A Time-Sharing Low-Cost Computer \\n The serial SABRAC computer designed and built in the Scientific Department of the Israel defense  Ministry has a 5000-location magnetic drum, main store.  To avoid a need to resort to optimum programming  techniques and to increase its overall efficiency the computer has also been given a 224-word ferrite  core store from which the program is obeyed.  Transfers between the core and drum stores and to and from  the twin paper-tape input and output channels are all available autonomously (concurrently, time-shared).   Multiplication and division orders are also autonomous, so that the machine may be executing up to three  orders simultaneously.  All functions naturally are interlocked.  A number of other advanced orders and  facilities are also incorporated.In particular, an \"Execute\" order permits a temporary jump for up  to four orders and a second modifier register permits double modification in general and relative addressing  of subroutines in particular.  Thus the overall effective speed of the machine is muchhigher than its  basic specification would lead one to expect and its design indicates one way in which the concepts of  time sharing may be incorporated in \"low-cost\" computers.'),\n",
       " (797, 'American Standard Code for Information Interchange'),\n",
       " (798, 'A Catalogue Entry Retrieval System'),\n",
       " (799,\n",
       "  'Design of a Separable Transition-Diagram Compiler* \\n A COBOL compiler design is presented which is compact enough to permit rapid, one-pass compilation  of a large subset of COBOL on a moderately large computer.  Versions of the same compiler for smaller  machines require only two working tapes plus a compiler tape.  The methods given are largely applicable  to the construction of ALGOL compilers.'),\n",
       " (800, 'The Linking Segment Subprogram Language and Linking Loader'),\n",
       " (801, 'Least Squares Solution with Constraints (Algorithm 177)'),\n",
       " (802, 'SYMINV2 (Algorithm 150)'),\n",
       " (803, 'Syminv2 (Algorithm 150)'),\n",
       " (804, 'Exponentiation of Series (Algorithms 134)'),\n",
       " (805, 'Newton Maehly (Algorithm 105)'),\n",
       " (806, 'Remark on Certification of Matrix Inversion Procedures'),\n",
       " (807, 'Reversion of Series (Algorithm 193)'),\n",
       " (808, 'Confluent Hypergeometric (Algorithm 192)'),\n",
       " (809, 'Hypergeometric (Algorithm 191)'),\n",
       " (810, 'Complex Power (Algorithm 190)'),\n",
       " (811, 'Smoothing 2 (Algorithm 189)'),\n",
       " (812, 'Smoothing 1 (Algorithm 188) '),\n",
       " (813, 'Differences and Derivatives (Algorithm 187)'),\n",
       " (814, 'Complex Arithmetic (Algorithm 186)'),\n",
       " (815, 'Normal Probability for Curve Fitting (Algorithm 185)'),\n",
       " (816, 'Erlang Probability for Curve Fitting (Algorithm 184)'),\n",
       " (817, 'Nexcom (Algorithm 152)'),\n",
       " (818, 'Realizing Boolean Connectives on The IBM 1620'),\n",
       " (819, 'Polynomial Evaluation Revised'),\n",
       " (820, 'Checking for Loops in Networks'),\n",
       " (821, 'Further Remarks on Sampling a Tape File-III'),\n",
       " (822,\n",
       "  'Real-Time Programming Specifications \\n Problems in the implementation of large real-time applications are treated, and suggested guidelines  for both program and file specifications are developed.  The problems delineated also occur in systems  programming.'),\n",
       " (823, 'A Syntactic Description of BC NELLIAC'),\n",
       " (824,\n",
       "  'DESCRIPTRAN-Automated Descriptive Geometry* \\n Descriptive geometry consists of procedures originally designed to solve 3-space geometry problems  by graphical constructions and measurement instead of by computation.  However, in addition to this it  unifies and simplifies the approach to many such problems.  When one can call subroutines that compute  new coordinates that correspond to those obtainable from the graphical constructions, there is the three-way  advantage of the approach of descriptive geometry, the accuracy of computation and the speed of the digital  computer.  DESCRIPTRAN makes it possible to program many problems in 3-space with a few statements; it  consists of 15 subroutines analogous to the procedures of descriptive geometry.'),\n",
       " (825,\n",
       "  'PIP: A Photo-Interpretive Program for the Analysis of Spark-Chamber Data* \\n An operating computer program that processes photographically recorded data is described.   The input to the program consists of spark-chamber photographs on which tracks of high-energy particles  are recorded.  The program automatically scans, measures and performs the preliminary interpretation  of these photographs.  In continuous operation a processing rate of 5,000 photographic frames per hour  is achieved.'),\n",
       " (826, 'Remarks on Fortran Subroutines for Time Series Analysis'),\n",
       " (827,\n",
       "  'Disk File Sorting \\n Sorting techniques using an IBM 1401 with a random access storage device are evaluated.'),\n",
       " (828,\n",
       "  'Incompressible flow Network Calculations \\n A general method for the calculation of flows and pressures in fluid flow networks is presented.   The method is applicable to computer use.'),\n",
       " (829, 'The External Language KLIPA For the URAL-2 Digital computer'),\n",
       " (830, 'CORC-The Cornell Computing Language'),\n",
       " (831, 'Real Error Function, ERF (Algorithm 123)'),\n",
       " (832, 'Curve Fitting with Constraints (Algorithm 74)'),\n",
       " (833, 'Reduction of a Symmetric Bandmatrix to Triple Diagonal Form'),\n",
       " (834, 'Nonrecursive Adaptive Integration (Algorithm 182)'),\n",
       " (835, 'Complementary Error Function-Large X (Algorithm 181)'),\n",
       " (836, 'Error Function-Large X (Algorithm 180)'),\n",
       " (837, 'Incomplete Beta Ratio (Algorithm 179)'),\n",
       " (838, 'Direct Search (Algorithm 178)'),\n",
       " (839, 'Least Squares Solution with Constraints (Algorithm 177)'),\n",
       " (840, 'Least Squares Surface Fit (Algorithm 176)'),\n",
       " (841, 'Shuttle Sort (Algorithm 175)'),\n",
       " (842, 'A Posteriori Bounds on a Zero of a Polynomial (Algorithm 174)'),\n",
       " (843, 'Assign (Algorithm 173)'),\n",
       " (844, '1410 Fortran Edit Feature'),\n",
       " (845, 'Another Test Matrix for Determinants and Inverses'),\n",
       " (846, 'Self-Inverse Conversion Table'),\n",
       " (847,\n",
       "  'A Penny-Matching Program \\n The logic of a penny-matching program written for the CSX-1 is described.'),\n",
       " (848,\n",
       "  'A Note on Range Transformations for Square Root and Logarithm \\n There was the germ of an idea in two previous papers [1,2] which no one seems to have picked  up in almost five years.  For certain functions it seems desirable to transform the argument to a short  range symmetric about 10.1 will give examples of this usage for the square root and logarithm function  for both binary and decimal machines.'),\n",
       " (849,\n",
       "  'Use of Tree Structures for Processing Files \\n In data processing problems, files are frequently used which must both be searched and altered.   Binary search techniques are efficient for searching large files, but the associated file organization  is not readily adapted to the file alterations.  Conversely, a chained file allocation permits efficient  alteration but cannot be searched efficiently. A file organized into a tree-like structure is discussed,  and it is shown that such a file may both be searched and altered with times proportional to slog(s)N,  where N is the number of file items and s is a parameter of the tree.  It is also shown that optimizing  the value of s leads to a search time which is only 25 per cent slower than the binary search.  The tree  organization employs two data chains and may be considered to be a compromise between the organizations  for the binary search and the chained file.  The relation of the tree organization to multidimensional  indexing and to the trie structure is also discussed.'),\n",
       " (850,\n",
       "  'Conversion, Reconversion and Comparison Techniques In Variable-Length Sorting \\n The logic is described for converting highly variable input records into a format that can  be easily and efficiently processed by a sorting program.  The internal record formats are discussed  in relation to (1) their conversion from input formats, (2) their reconversion to output formats, and  (3) comparison techniques between internal formats.'),\n",
       " (851,\n",
       "  'Design and Characteristics of a Variable-Length Record Sort Using New Fixed-Length Record Sorting  Techniques \\n This paper describes the application of several new techniques for sorting fixed-length records  to the problems of variable-length record sorting.  The techniques have been implemented on a Sylvania  9400 computer system with 32,000 fixed-length words of memory.  Specifically, the techniques sequence  variable-length records of unrestricted size, produce long initial strings of data, merge strings of  data at the power of T-1, where T is the number of work tapes in a system, and do not restrict the volume  of input data.'),\n",
       " (852, 'A Method of Comparing the Time Requirements of Sorting Methods'),\n",
       " (853, 'The COBOL Sort Verb'),\n",
       " (854,\n",
       "  'Some Characteristics of Sorting in Computing Systems Using Random Access Storage Devices  \\n The substantial differences in characteristics of random access storage and tape devices dictate  that concepts and objectives of computer program design be considered from the viewpoint of the external  file medium used.  This is particularly true in the case of sorting.  In a tape-oriented system, the  major sorting problem is that of minimizing merge time despite the limited orders of merge possible.   In contrast, sorting in a random access-oriented system encourages the selection of the optimum order  of merge from many possible orders.  The latter problem is discussed in this paper, along with criteria  developed for determining the optimum order of merge according to the various properties of random access  storage devices.  Attention is also given to the problem of key sorting versus record sorting and the  possibly serious disadvantage of key sorting on a random access system.'),\n",
       " (855,\n",
       "  'Organization and Structure of Dataon Disk File Memory Systems for Efficient Sorting and Other  Data Processing Programs \\n An approach to the organization and structure of data on Bryant Disc File Memory Systems for  sorting and performing other data processing functions is presented.  The following areas are covered:  characteristics of Bryant Disc File Systems on the Bendix G-20 and RCA 301; two proposed \"chaining\" structures  for data; and functions of a Disk File Executive Routine.  The concepts for sorting and performing file  maintenance processing using the proposed structure and executive routine are discussed.  Additionally,  it is shown that sorting can be accomplished without the use of disk storage work areas.'),\n",
       " (856,\n",
       "  'Sorting with Large Volume, Random Access, Drum Storage \\n An approach to sorting records is described using random access drum memory.  The Sort program  described is designed to be a generalized, self-generating sort, applicable to a variety of record statements.   This description is divided into three parts.  The first part presents the operating environment; the  second defines the general solution; the third part describes the internal sort-merge technique.'),\n",
       " (857,\n",
       "  'Sorting Nonredundant Files-Techniques Used in the FACT Compiler \\n Some typical file structures, including some called \"non-redundant,\" are examined,and the  methods used in FACT to sort such files are discussed.'),\n",
       " (858,\n",
       "  'A Tape File Merge Pattern Generator \\n A routine is presented which specifies the sequence of merge cycles to effect the merging of  sorted tape files.  The routine is designed to minimize elapsed computer time by varying the power of  the merge cycles, so as to use all the available tape drives, with its characteristic of assigning one  drive to a single-reel file and two drives to each multiple-reel file.'),\n",
       " (859, 'Computer Planned Collates'),\n",
       " (860,\n",
       "  'A Comparison Between the Polyphase and Oscillating Sort Techniques \\n A comparison between the Oscillating and Polyphase Sort techniques is developed for computer  systems having from four to ten tape drives.  The basis for the comparison is the total reading and writing  required for various number of input strings and tape drives for the two techniques.'),\n",
       " (861,\n",
       "  'Read-Backward Polyphase Sorting \\n Read-backward Polyphase sorting provides more efficient use of the tapes available to a sort  than most other sorting techniques.  Backward Polyphase produces a continuous merging process from n-1  tapes where n is the total number of tapes being used in the sorting process.  Any of the available presorting  techniques may be used in conjunction with the Polyphase merge sort provided that the presort has the  capability of producing both ascending and descending strings and distributing the strings on the various  tapes as required by the Polyphase Merge.'),\n",
       " (862, 'String Distribution for the Polyphase Sort'),\n",
       " (863, 'Multiphase Sorting'),\n",
       " (864, 'An Empirical Study of Minimal Storage Sorting'),\n",
       " (865,\n",
       "  'Internal and Tape Sorting Using the Replacement-Selection Technique \\n A general technique for sequencing unsorted records is presented.  The technique is shown to  be applicable for the first stage of a generalized sort program (the formation of initial strings) as  well as for sorting records within a memory storage (an internal sort).  It is shown that given N records  in memory storage, records are sequenced using 1+log2 N tests per record, that initial string lengths  will average 2N for random input records, and that reading, writing and processing can be accomplished  simultaneously if the computer permits such overlap.'),\n",
       " (866, 'Sorting on Computers'),\n",
       " (867,\n",
       "  'Least Squares Fitting of Planes to Surfaces Using Dynamic Programming \\n Dynamic programming has recently been used by Stone, by Bellman and by Gluss to determine the  closet fit of broken line segments to a curve in an interval under the constraint that the number of  segments is fixed.  In the present paper successive models are developed to extend the method to the  fitting of broken plane segments to surfaces z=g(x,y) defined over certain types of subareas of the (x,y)-space.   The first model considers a rectangular area, with the constraint that the plane segments are defined  over a grid in the (x,y)-space.  It is then shown how this model may be incorporated into an algorithm  that provides successive approximations to optimal fits for any type of closed area.  Finally, applications  are briefly described.'),\n",
       " (868, 'A Suggested Method of Making Fuller Use of Strings in ALGOL 60'),\n",
       " (869, 'Term of Magic Square (Algorithm 148)'),\n",
       " (870, 'Term of Magic Square (Algorithm 148)'),\n",
       " (871, 'PSIF (Algorithm 147)'),\n",
       " (872, \"Adaptive Numerical Integration by Simpson's Rule (Algorithm 145)\"),\n",
       " (873, 'Random (Algorithm 133)'),\n",
       " (874, 'Chebyshev Curvefit (Algorithm 91)'),\n",
       " (875, 'Incomplete Elliptic Integrals (Algorithm 73)'),\n",
       " (876, 'Complete Elliptic Integral (Algorithm 149)'),\n",
       " (877, 'Complete Elliptic Integral of the First Kind (Algorithm 55)'),\n",
       " (878, 'Reduction of a Matrix Containing Polynomial Elements (Algorithm 170)'),\n",
       " (879,\n",
       "  'Newton Interpolation with Forward Divided Differences (Algorithm 169)'),\n",
       " (880, 'Newton Interpolation with Backward Divided Differences'),\n",
       " (881, 'Calculation of Confluent Divided Differences (Algorithm 167)'),\n",
       " (882, 'Monte Carlo (Algorithm 166)'),\n",
       " (883, 'Complete Elliptic Integrals (Algorithm 165)'),\n",
       " (884, 'Orthogonal Polynomial Least Squares Surface Fit (Algorithm 164)'),\n",
       " (885, 'Modified Hankel Function (Algorithm 163)'),\n",
       " (886, 'XY-move Plotting (Algorithm 162)'),\n",
       " (887,\n",
       "  'Combinatorial of M Things Taken One at a Time, Two at a Time, Up to N at a Time (Algorithm 161)'),\n",
       " (888, 'Algorithm 160 Combinatorial of M Things Taken N at A Time'),\n",
       " (889, 'Official Actions and Responses to ALGOL As a Programming Language'),\n",
       " (890,\n",
       "  \"Selected Definitions  \\n A selection of the definitions prepared by the ACM Standards Committee's Subcommittee on Programming  Terminology is presented for review by the ACM membership.\"),\n",
       " (891,\n",
       "  \"Everyman's Information Retrieval System \\n The information retrieval problem whose solution is presented here was posed by a technical  library with limited bubget and personnel.  The solution, however, is quite general and is applicable  to many different types of retrieval problems.  Further,the method of solution makes it possible for  many groups who have previously dismissed an information retrieval program as expensive and difficult  (from a programming stand-point) to reconsider their position, for the present solution makes it possible  to install an information retrieval program in less than three months, and with relatively little equipment.\"),\n",
       " (892,\n",
       "  'RECOL-A Retrieval Command Language \\n An interrogation scheme is described for the retrieval and manipulation of data file records.   The language of the interrogation scheme allows for selecting file records with the are of logical condition  statements, defining record classes, associating file records, editing printed output, and summarizing  the results of the above operations.  Some examples of a typical file application and the more significant  features of a particular machine implementation are given.'),\n",
       " (893,\n",
       "  'Significance Arithmetic on a Digital Computer \\n The 7090 at NYU has been modified to include a \"Significance Mode\" of operation which is intended  to facilitate the identification of significant bits in the results of floating-point arithmetic operations.   The manner in which floating-point arithmetic is handled in this mode is discussed.  Several numerical  experiments using this mode are described and comparisons are made with the ordinary \"normalized mode.\"   Examples include power series evaluation, linear equations solution, determinant evaluation and matrix  inversion.'),\n",
       " (894,\n",
       "  'An Iterative Factorization Technique for Polynomials \\n An iterative technique is displayed whereby factors of arbitrary degree can be found for polynomials  in one variable.  Convergence is shown to occur always if a certain Jacobian does not vanish and if the  initial approximation to a factor is near enough to an actual factor.  The process is simply programmed,  and preliminary results indicate it to be well adapted to use with digital computers.  For factors of  degree two, the technique is similar to that of Bairstow, the present method being somewhat simpler.'),\n",
       " (895,\n",
       "  'A Computational Extension of the Variate Difference Method \\n Presented here is a computational extension of the variate difference method as developed by  G. Tintner [1].'),\n",
       " (896, 'Characteristic Values and Vectors of Defective Matrices'),\n",
       " (897,\n",
       "  'Note on the Proof of the Non-existence of a Phrase Structure Grammar for ALGOL 60'),\n",
       " (898, 'Random (Algorithm 133)'),\n",
       " (899, 'Magic Square (Algorithm 117 & 118)'),\n",
       " (900, 'Ancestor (Algorithm 79)'),\n",
       " (901, 'Difference Expression Coefficients (Algorithm 79)'),\n",
       " (902, 'Determinant (Algorithm 159)'),\n",
       " (903, 'Exponentiation of Series (Algorithm 134 )'),\n",
       " (904, 'Fourier Series Approximation (Algorithm 157)'),\n",
       " (905, 'Algebra of Sets (Algorithm 156)'),\n",
       " (906, 'Combination in any Order (Algorithm 155)'),\n",
       " (907, 'Combination in Lexicographical Order (Algorithm 154)'),\n",
       " (908, 'Test Matrix for Inversion '),\n",
       " (909, 'Arithmetizing Declarations (Corrigendum)'),\n",
       " (910, 'Selective Instruction Trap for the 7090'),\n",
       " (911, 'A Variant Method of File Searching'),\n",
       " (912,\n",
       "  'Addressing an Array Yi in k-Dimensions by Fortran for Analysis of Variance'),\n",
       " (913, 'Neliac'),\n",
       " (914, 'Jovial and Its Documentation'),\n",
       " (915, 'Documentation of IPL-V'),\n",
       " (916, 'FORTRAN'),\n",
       " (917, 'COMIT'),\n",
       " (918, 'COBOL'),\n",
       " (919, 'Documentation Problems: ALGOL 60'),\n",
       " (920, 'Toward Better Documentation of Programming Languages'),\n",
       " (921, 'Incomplete Elliptic Integrals (Algorithm 73)'),\n",
       " (922, 'Multint (Algorithm 32)'),\n",
       " (923, 'Gomory (Algorithm 153)'),\n",
       " (924, 'Nexcom (Algorithm 152)'),\n",
       " (925,\n",
       "  'Location of a Vector in a Lexicographically Ordered ListAlgorithm 151)'),\n",
       " (926, 'Syminv2 (Algorithm 150)'),\n",
       " (927, 'Linear Programming Applied to Ultraviolet Absorption Spectroscopy'),\n",
       " (928, 'Character Manipulation in FORTRAN'),\n",
       " (929, 'Glossary Construction'),\n",
       " (930, 'Decimal-to-Binary Conversion of Short Fields'),\n",
       " (931, 'Systematic Mistake Analysis of Digital Computer Programs'),\n",
       " (932, 'Matrix Inversion by Gauss-Jordan Inversion II (Algorithm 120)'),\n",
       " (933, 'Magic Squares (Algorithm 117 & 118)'),\n",
       " (934, \"Gauss's Method (Algorithm 107)\"),\n",
       " (935, 'Calculating Primes by Means of GPS (Algorithm)'),\n",
       " (936, 'A Set of Test Matrices (Algorithm 52)'),\n",
       " (937, 'Inverse of a Finite Segment of the Hilbert Matrix (Algorithm 50)'),\n",
       " (938, 'Invert (Algorithm 42)'),\n",
       " (939, 'Gamma Function (Algorithm 31)'),\n",
       " (940,\n",
       "  'Generating Discrete Random Variables in a Computer \\n This note is concerned with details of how to instruct a computer to choose one from many things  with assigned probabilities.  The method uses a uniform variable to direct the computer to a memory location;  if this is done by a sequence of appropriately chosen conditional probabilities, efficient use of memory  space and quite fast programs will result.'),\n",
       " (941,\n",
       "  \"A Recursive Program for the General n-Dimensional Integral \\n A general program is outlined for n-dimensional integration with variable limits.  The program  is of a recursive nature and uses Simpson's rule combined with repeated bisection to attain the required  accuracy.  It was developed in the Ferranti Mercury Autocode Scheme.\"),\n",
       " (942,\n",
       "  'FORTRAN Subroutines for Time Series Analysis \\n The authors have recently been concerned in a time-series study that constituted a fairly typical  piece of applied statistical research, involving extensive computations on a moderately large quantity  of data.  Wehave found that the many different numerical processes that were required could be built  up almost completely from a small number of basic operations, and a set of FORTRAN subroutines has been  written to perform these.  The main purpose of this note is to describe these subroutines, but since  the question of general statistical programs is topical [1], we include some general remarks.'),\n",
       " (943, 'Terms Frequently Combined in Problem Description'),\n",
       " (944,\n",
       "  'Storage and Search Properties of a Tree-Organized Memory System \\n A memory with list properties [1] may be used to construct numeric, alphabetic or alphanumeric  trees.  Such trees have information storage and retrieval properties applicable to problems involving  large quantities of data or to problems where the quantity, word length and distribution of stored information  is not known a priori, or changes rapidly during the processing.  The purpose of this paper is to examine  the storage and search properties of a tree-organized storage system assuming that a memory possessing  certain list properties is available.  Of prime interest is the application where a symbol table, dictionary  or similar file is to be stored and searched.'),\n",
       " (945, 'Arithmetizing Declarations: An Application to COBOL'),\n",
       " (946,\n",
       "  'Suggestions on ALGOL 60 (ROME) Issues - A Report by the American Standards Association Subcommittee X3.4.2'),\n",
       " (947, 'Supplement to the ALGOL 60 Report'),\n",
       " (948, 'Note on the Use of Procedures '),\n",
       " (949,\n",
       "  'Integer and Signed Constants in ALGOL \\n A few remarks are given on the relations between syntax and semantics in the programming languages.   The aim is to point out that, if it is true that the grammar of a context-free language should be conceived  not only as a strings-generating device but also as a method for expressing a meaning, then the grammar  of ALGOL is open to some criticism.'),\n",
       " (950,\n",
       "  'Parallel Methods for Integrating Ordinary Differential Equations \\n This paper is dedicated to the proposition that, in order to take full advantage for real-time  computations of highly parallel computers as can be expected to be available in the near future, much  of numerical analysis will have to be recast in a more \"parallel\" form.  By this is meant that serial  algorithms ought to be replaced by algorithm which consist of several subtasks which can be computed  without knowledge of the results of the other subtasks.  As an example, a method is proposed for \"parallelizing\"  the numerical integration of an ordinary differential equation, which process, by all standard methods,  is entirely serial.'),\n",
       " (951,\n",
       "  'Rational Chebyshev Approximations to the Bessel Function Integrals Kis(x) \\n The second Remes algorithm is used to approximate the integrals Kis by rational functions.  The related coefficients for the approximations of Ki1, Ki2, Ki3 are given for different precisions.'),\n",
       " (952, 'Another use of FORTRAN II Chaining'),\n",
       " (953, 'Scanning Text with a 1401'),\n",
       " (954, 'A Note on the Calculation of Probabilities in an F-Distribution'),\n",
       " (955, 'A Class of Matrices to Test Inversion Procedures'),\n",
       " (956, 'A Family of Test Matrices'),\n",
       " (957, 'Method for Partial Rewriting of Magnetic Tape'),\n",
       " (958, 'A Case of too Much Precision'),\n",
       " (959, 'Mark Sense and Port-A-Punch Programming Inputs'),\n",
       " (960, 'Curve Fitting with Format Fortran'),\n",
       " (961,\n",
       "  'Limited Bit Manipulation Using FORTRAN II \\n Techniques are developed for manipulating bits using only FORTRAN II.  These techniques allow  individual bits to be tested, certain fields to be shifted, and numbers coded in BCD to be converted  to Binary.'),\n",
       " (962,\n",
       "  \"Double-Precision Squares Root for The CDC-3600 \\n In January of 1960, the late Hans J. Maehly completed a summary of approximations to the elementary  functions for the CDC-1604 computer.  The approximations and techniques suggested by Maehly are equally  applicable to the second large computer in the CDC line, the 3600.  Unlike the 1604, however, the 3600  has built-in double-precision floating-point arithmetic.  The present work, largely inspired by the successes  of Maehly and his associates, concerns the extension of one of Maehly's ideas to a double-precision subroutine  for the 3600.\"),\n",
       " (963,\n",
       "  'Relative Effects of Central Processor and Input-Output Speeds Upon Throughput on the Large Computer \\n Presented in this paper is a technique for determining the relative effects of the internal  speed of the computer and the speed of the input-output units upon the overall speed of the system. Equations  are derived which permit the determination of these effects from hardware usage measurements.'),\n",
       " (964,\n",
       "  'Mechanization of Tedious Algebra-the e Coefficients of Theoretical Chemistry \\n A table of formulas for certain integrals involving Legendre functions has been constructed  mechanically by a program which performed algebraic operations.  The formulas are all rational algebraic  expressions in a single variable and were constructed by a recurrence procedure.  They are of interest  in molecular quantum chemistry.  Trivial coding techniques were used to write the relevant programs in  FORTRAN.  The results were photo composed on a Photon S-560 system, that was controlled by tapes which  were punched directly from the computer output, so avoiding manual keyboarding, transcription errors  and keyboarded correction.'),\n",
       " (965, 'Greatest Common Divisor (Algorithm 237 [A1])'),\n",
       " (966, 'Evaluation of Determinant (Algorithm 224 [F3])'),\n",
       " (967, 'Complementary Error Function (Algorithm 181 [S15])'),\n",
       " (968, 'Radical-Inverse Quasi-Random Point Sequence (Algorithm 247 [G5])'),\n",
       " (969, 'Graycode (Algorithm 246 [Z])'),\n",
       " (970, 'Treesort 3 (Algorithm [M1])'),\n",
       " (971,\n",
       "  'Time Sharing in a Traffic Control Program \\n The Toronto traffic signal control system consists of a variety of logically distinct computer  programs, all competing for machine time.  To satisfy these demands, a time-sharing program has been  written whose purpose is to execute, in the order of a predefined priority, the various subprograms within  the real-time system.  In this paper the more interesting aspects of the time-sharing program are outlined.'),\n",
       " (972,\n",
       "  'An Executive System Implemented as a Finite-State Automaton \\n The 473L command and control system used by the Air Force permits many operators to access  large data files through the use of a computer.  The man-machine interface is satisfied by several communication  consoles from which operators may enter queries and view replies.  A data link permits remote stations  to send messages, status reports and inventories directly to the computer.  The information received  over the on-line data link is used to update the data files which are stored on disk.  The 473L programming  system is divided into an Executive Control Program and five components with different processing priorities.   These priorities permit the system to be most sensitive to the console inputs and permit the operators  at all the consoles to time share the central processor.  The Executive Control Program provides for  the orderly transitions of control among the programming system components. The major emphasis of the  paper is on the technique of using the definition of a finite-state automaton for organizing the Executive  Control Program.'),\n",
       " (973,\n",
       "  'Estimation of Heart Parameters Using Skin Potential Measurements \\n A fundamental problem of vector cardiography is the estimation of the state of the heart on  the basis of skin potential measurements.  A mathematical model relating ventricular dipoles to surface  potentials is sketched.  Then it is shown that the inverse problem-that of determining electrical heart  parameters on the basis of skin potential measurements-may be viewed as a nonlinear multipoint boundary  value problem.  A feasible solution, employing quasilinearization and high-speed digital computers, is  given.'),\n",
       " (974,\n",
       "  'A Technique for Reading Gapless Tapes Makes Electrocardiograph Analysis Feasible on the IBM 7090  \\n To study arrhythmias and higher frequency components of the electrocardiogram, long series  of patient heart cycles must be examined before valid comparison of different heart beats can be made.  A technique is presented for the automatic analysis of long series heart cycles via a digital computer.'),\n",
       " (975,\n",
       "  'The New Program of Work for the International Standard Vocabulary in Computers and Information Processing'),\n",
       " (976, 'Fresnel Integrals (Algorithm 213 [S20])'),\n",
       " (977,\n",
       "  'Conversions Between Calendar Date and Julian Day Number (Algorithm 199 [Z])'),\n",
       " (978, 'Fresnel Integrals (Algorithm 244 [S20])'),\n",
       " (979, 'Logarithm of a Complex Number (Algorithm 243 [B3])'),\n",
       " (980,\n",
       "  'Multiple-Precision Arithmetic and the Exact Calculation of the 3-j, 6-j and 9-j Symbols \\n Described in this paper is a system of general-purpose multiple-precision fixed-point routines  and their use in subroutines which calculate exactly the quantum-mechanical 3-j, 6-j and 9-j symbols  of large arguments.'),\n",
       " (981,\n",
       "  'Rounding Problems in Commercial Data Processing \\n A common requirement in commercial data processing is that the sum of a set of numbers, rounded  in a generally understood manner, be equal to the sum of the numbers rounded individually.  Four rounding  procedures are described to accomplish this.  The particular procedure that is appropriate depends upon  whether the numbers being accumulated can vary in sign, whether their sum can vary in sign, and whether  the last number being summed can be recognized as such prior to its rounding.'),\n",
       " (982,\n",
       "  'An Inductive Approach to Language Translation \\n The possibility of natural language translation by means of fixed operations on example translations  is considered.  The conception of sentence translation which motivates the work is informally presented,  and the measurement of physical similarity in pairs of strings is discussed, a notion which plays a central  role in the proposed type of translator.  Experimental evidence is presented in support of the premise  upon which this conception is based.'),\n",
       " (983,\n",
       "  'Take-up reels for One-Inch Perforated Tape for Information Interchange (Proposed American Standard)'),\n",
       " (984, 'Report on Input-Output Procedures for ALGOL 60 (IFIP)'),\n",
       " (985, 'Report on SUBSET ALGOL 60 (IFIP)'),\n",
       " (986,\n",
       "  'Proposed Amendment to Proposed American Standard on Specification for General-Purpose Paper Cards  for Information Processing'),\n",
       " (987,\n",
       "  'FORTRAN vs. Basic FORTRAN (A Programming Language for Information Processing on Automatic Data  Processing Systems)'),\n",
       " (988,\n",
       "  'History and Summary of FORTRAN Standardization Development for the ASA'),\n",
       " (989, 'A Method of Syntax Specification'),\n",
       " (990,\n",
       "  'Constraint-Type Statements in Programming Languages \\n A proposal is made for including in a programming language statements which imply relations  between variables but which are not explicit assignment statements.  The compiler sets up a Newtonian  iteration making use for the purpose of a routine for formal differentiation.'),\n",
       " (991, 'Gamma Function with Controller Accuracy (Algorithm 225 [S14])'),\n",
       " (992, 'Gamma Function (Algorithm 221 [S14])'),\n",
       " (993, 'Kutta Merson (Algorithm 218 [D2])'),\n",
       " (994, 'Stringsort (Algorithm 207 [M1])'),\n",
       " (995, 'Steep1 (Algorithm 203 [E4])'),\n",
       " (996, 'Permutations of a Set with Repetitions (Algorithm 242 [G6])'),\n",
       " (997, 'Patent Protection of Computer Programs'),\n",
       " (998, 'Computer Programs are Patentable'),\n",
       " (999, 'Joint Inventorship of Computers'),\n",
       " (1000, 'Computer Patent Disclosures'),\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_cacm_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70472b5c277bb7c417de8da24e4e5261",
     "grade": true,
     "grade_id": "cell-a1c43818e0d3fd79",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Function check\n",
    "docs = read_cacm_docs()\n",
    "\n",
    "assert isinstance(docs, list)\n",
    "assert len(docs) == 3204, \"There should be exactly 3024 documents\"\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6fd095d2204cce3935444ca37c7c42da",
     "grade": false,
     "grade_id": "cell-5ed2ddc91f73c60e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 1.2 Read the CACM queries (3 points)\n",
    "\n",
    "Next, let us read the queries. They are formatted similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d26c0908e758acb9968b84056b1060a",
     "grade": false,
     "grade_id": "cell-5c7e8e7c4fc2757f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".I 1\r\n",
      ".W\r\n",
      " What articles exist which deal with TSS (Time Sharing System), an\r\n",
      "operating system for IBM computers?\r\n",
      ".N\r\n",
      " 1. Richard Alexander, Comp Serv, Langmuir Lab (TSS)\r\n",
      " \r\n",
      ".I 2\r\n",
      ".W\r\n",
      " I am interested in articles written either by Prieve or Udo Pooch\r\n",
      ".A\r\n",
      "Prieve, B.\r\n",
      "Pooch, U.\r\n",
      ".N\r\n",
      " 2. Richard Alexander, Comp Serv, Langmuir Lab (author = Pooch or Prieve)\r\n"
     ]
    }
   ],
   "source": [
    "##### The first 15 lines of 'query.text' has 2 queries\n",
    "# We are interested only in 2 fields. \n",
    "# 1. the '.I' - the query id\n",
    "# 2. the '.W' - the query\n",
    "!head -15 ./datasets/query.text\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da34d95924026ea55bf0a9b2070ec237",
     "grade": false,
     "grade_id": "cell-88e293507d2dcef6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "**Implementation (3 points):**\n",
    "Write a function to read the `query.text` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f98ac81b2ccb44fe3441e202e980c847",
     "grade": false,
     "grade_id": "cell-433e3ad5d0e2572a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (3 points)\n",
    "def read_queries(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads in the CACM queries. The dataset is assumed to be in the folder \"./datasets/\" by default\n",
    "        Returns: A list of 2-tuples: (query_id, query)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Path to query.text\n",
    "    path = root_folder + \"query.text\"\n",
    "    \n",
    "    # Open and split document into tokens\n",
    "    with open(path, 'r') as file:\n",
    "        queries = file.read().split(\"\\n\")\n",
    "    \n",
    "    # Define lists\n",
    "    query_id = []\n",
    "    query = []\n",
    "    \n",
    "    # Set counter for tracking index tracking\n",
    "    counter = 0\n",
    "    \n",
    "    # Loop through doc to get I. and W.\n",
    "    for i in range(len(queries)):\n",
    "        if queries[i][0:2] == \".I\":\n",
    "            counter += 1\n",
    "            query_id.append(queries[i][3:])\n",
    "        if  queries[i] == '.W':\n",
    "            query.append(queries[i+1][1:])\n",
    "            j = i+2\n",
    "            while queries[j] not in [\".N\", \".A\"]:\n",
    "                query[counter-1] = query[counter-1] + queries[j]\n",
    "                j += 1\n",
    "               \n",
    "    \n",
    "    return list(zip(query_id, query))\n",
    "        \n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0e7a9868bd4b92692c33732af640b8b",
     "grade": true,
     "grade_id": "cell-6ec540abce66c598",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Function check\n",
    "queries = read_queries()\n",
    "\n",
    "assert isinstance(queries, list)\n",
    "assert len(queries) == 64 and all([q[1] is not None for q in queries]), \"There should be exactly 64 queries\"\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "652f9159caea63888e06b54103bd7f32",
     "grade": false,
     "grade_id": "cell-1c31569491d7b782",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 1.3 Read the stop words (1 point)\n",
    "\n",
    "We use the common words stored in `common_words`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ad6f5bae6a792504c1c8513ae5751ad",
     "grade": false,
     "grade_id": "cell-34bdb63461418a96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\r\n",
      "about\r\n",
      "above\r\n",
      "accordingly\r\n",
      "across\r\n",
      "after\r\n",
      "afterwards\r\n",
      "again\r\n",
      "against\r\n",
      "all\r\n"
     ]
    }
   ],
   "source": [
    "##### Read the stop words file \n",
    "!head ./datasets/common_words\n",
    "##### Read the README file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f720edbacd79170351156a443e0018ff",
     "grade": false,
     "grade_id": "cell-4744bde0338895d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "**Implementation (1 point):**\n",
    "Write a function to read the `common_words` file (For better coverage, try to keep them in lowercase):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6a9e9bb3ad0f8dc2ec3c497a3149092",
     "grade": false,
     "grade_id": "cell-7357aa40f64e5bcb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (1 point)\n",
    "def load_stopwords(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Loads the stopwords. The dataset is assumed to be in the folder \"./datasets/\" by default\n",
    "        Output: A set of stopwords\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    path = root_folder + \"common_words\"\n",
    "    \n",
    "    # Open and split document into tokens\n",
    "    with open(path, 'r') as file:\n",
    "        stopwords = file.read().lower().split('\\n')[:-1]\n",
    "    \n",
    "    \n",
    "    return set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9211b9735b828d89d1a51d8743c448b4",
     "grade": true,
     "grade_id": "cell-2ca3ac162004de97",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Function check\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "assert isinstance(stopwords, set)\n",
    "assert len(stopwords) == 428, \"There should be exactly 428 stop words\"\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc7e419f35dfd3f507d8a7555ce2cde9",
     "grade": false,
     "grade_id": "cell-134b72872f4300cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 1.4 Tokenization (4 points)\n",
    "\n",
    "We can now write some basic text processing functions. \n",
    "A first step is to tokenize the text. \n",
    "\n",
    "**Note**: Use the  `WordPunctTokenizer` available in the `nltk` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f5564d3c75bf22fbf832b3a9b938f37",
     "grade": false,
     "grade_id": "cell-322be4c9499bdc4b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (4 points)\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "        Tokenizes the input text. Use the WordPunctTokenizer\n",
    "        Input: text - a string\n",
    "        Output: a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    return nltk.WordPunctTokenizer().tokenize(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f60b93aa03bf9eced64940252eb33fe3",
     "grade": true,
     "grade_id": "cell-7fbf48bf7541a622",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "##### Function check\n",
    "text = \"the quick brown fox jumps over the lazy dog\"\n",
    "tokens = tokenize(text)\n",
    "\n",
    "assert isinstance(tokens, list)\n",
    "assert len(tokens) == 9\n",
    "\n",
    "print(tokens)\n",
    "# output: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b1cc868db22735e5e193bc64ff67877",
     "grade": false,
     "grade_id": "cell-fd1b98ae61b697ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 1.5 Stemming (3 points)\n",
    "\n",
    "Write a function to stem tokens. \n",
    "Again, you can use the nltk library for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c4a6aa979d66158c7b6b992af43293a",
     "grade": false,
     "grade_id": "cell-e3f6c8e3f874b28d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # importing modules \n",
    "# from nltk.stem import PorterStemmer \n",
    "# from nltk.tokenize import word_tokenize \n",
    "   \n",
    "# ps = PorterStemmer() \n",
    "   \n",
    "# sentence = \"Programers program with programing languages\"\n",
    "# words = word_tokenize(sentence) \n",
    "   \n",
    "# for w in words: \n",
    "#     print(w, \" : \", ps.stem(w)) \n",
    "\n",
    "# TODO: Implement this! (3 points)\n",
    "def stem_token(token):\n",
    "    \"\"\"\n",
    "        Stems the given token using the PorterStemmer from the nltk library\n",
    "        Input: a single token\n",
    "        Output: the stem of the token\n",
    "    \"\"\"\n",
    "    \n",
    "    return nltk.PorterStemmer().stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd2611a46e7c2e92af438d7166cf2616",
     "grade": true,
     "grade_id": "cell-cd6863e6ee6ed205",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Function check\n",
    "\n",
    "assert stem_token('owned') == 'own'\n",
    "assert stem_token('itemization') == 'item'\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fcc93f08a6abcaffd5bd9475e4768e5",
     "grade": false,
     "grade_id": "cell-47c9f90498699110",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 1.6 Summary\n",
    "\n",
    "The following function puts it all together. Given a string, it tokenizes it and processes it according to the flags that you set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ff2d215ee8e0039c5a91fd3de12e6bd",
     "grade": false,
     "grade_id": "cell-dd0d3f46b30801da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Putting it all together\n",
    "def process_text(text, stem=False, remove_stopwords=False, lowercase_text=False):\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tokenize(text):\n",
    "        if remove_stopwords and token.lower() in stopwords:\n",
    "            continue\n",
    "        if stem:\n",
    "            token = stem_token(token)\n",
    "        if lowercase_text:\n",
    "            token = token.lower()\n",
    "        tokens.append(token)\n",
    "\n",
    "    return tokens\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02d433b18eb43654fa4306a7bf55b190",
     "grade": false,
     "grade_id": "cell-8d885bfd2edd43ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "Let's create two sets of preprocessed documents.\n",
    "We can process the documents and queries according to these two configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbe4ca667be6842fdcf512fbcad50c7f",
     "grade": false,
     "grade_id": "cell-d427365ee0fb21d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# In this configuration:\n",
    "# Don't preprocess the text, except to tokenize \n",
    "config_1 = {\n",
    "  \"stem\": False,\n",
    "  \"remove_stopwords\" : False,\n",
    "  \"lowercase_text\": True\n",
    "} \n",
    "\n",
    "\n",
    "# In this configuration:\n",
    "# Preprocess the text, stem and remove stopwords\n",
    "config_2 = {\n",
    "  \"stem\": True,\n",
    "  \"remove_stopwords\" : True,\n",
    "  \"lowercase_text\": True, \n",
    "} \n",
    "\n",
    "####\n",
    "doc_repr_1 = []\n",
    "doc_repr_2 = []\n",
    "for (doc_id, document) in docs:\n",
    "    doc_repr_1.append((doc_id, process_text(document, **config_1)))\n",
    "    doc_repr_2.append((doc_id, process_text(document, **config_2)))\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b60589aac19e80941d860d9b3f1e9a16",
     "grade": false,
     "grade_id": "cell-b1c102db61ae7495",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "--- \n",
    "\n",
    "## Section 2: Indexing (10 points)<a class=\"anchor\" id=\"indexing\"></a>\n",
    "\n",
    "[Back to Part 1](#part1)\n",
    "\n",
    "\n",
    "\n",
    "A retrieval function usually takes in a query document pair, and scores a query against a document.  Our document set is quite small - just a few thousand documents. However, consider a web-scale dataset with a few million documents. In such a scenario, it would become infeasible to score every query and document pair. In such a case, we can build an inverted index. From Wikipedia:\n",
    "\n",
    "> ... , an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, .... The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. ...\n",
    "\n",
    "\n",
    "Consider a simple inverted index, which maps from word to document. This can improve the performance of a retrieval system significantly. In this assignment, we consider a *simple* inverted index, which maps a word to a set of documents. In practice, however, more complex indices might be used.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "710fd943f45523ac36fcb887cc0d4d39",
     "grade": false,
     "grade_id": "cell-fa373192c1b7bb95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Term Frequency-index (10 points)\n",
    "In this assignment, we will be using an index created in memory since our dataset is tiny. To get started, build a simple index that maps each `token` to a list of `(doc_id, count)` where `count` is the count of the `token` in `doc_id`.\n",
    "For consistency, build this index using a python dictionary.\n",
    "    \n",
    "Now, implement a function to build an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c281c45196493b87e45e1b3fb4ddd6c9",
     "grade": false,
     "grade_id": "cell-077599b87e953209",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def build_tf_index(documents):\n",
    "    \"\"\"\n",
    "        Build an inverted index (with counts). The output is a dictionary which takes in a token\n",
    "        and returns a list of (doc_id, count) where 'count' is the count of the 'token' in 'doc_id'\n",
    "        Input: a list of documents - (doc_id, tokens)\n",
    "        Output: An inverted index. [token] -> [(doc_id, token_count)]\n",
    "    \"\"\"\n",
    "    inv_index = {}\n",
    "\n",
    "    for document in documents:\n",
    "        # Create dict to store token info for this document\n",
    "        hashtable = {}\n",
    "        doc_id = int(document[0])\n",
    "        # Go through all tokens and count them\n",
    "        for token in document[1]:\n",
    "            # Initialize or increase count\n",
    "            if token not in hashtable:\n",
    "                hashtable[token] = 1\n",
    "            else:\n",
    "                hashtable[token] += 1\n",
    "        # Go through tokens and append their count to the index\n",
    "        for token, count in hashtable.items():\n",
    "            if token not in inv_index:\n",
    "                inv_index[token] = []\n",
    "            inv_index[token].append((doc_id, count))\n",
    "    return inv_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15e7041b4323d2a290322de538ff7670",
     "grade": false,
     "grade_id": "cell-093aebfa504f96f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "Now we can build indexed documents and preprocess the queries based on the two configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e27540c1d8d77a3779a05f557f3f40c6",
     "grade": false,
     "grade_id": "cell-b2ff1676348b90a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Indexed documents based on the two configs\n",
    "\n",
    "# Create the 2 indices\n",
    "tf_index_1 = build_tf_index(doc_repr_1)\n",
    "tf_index_2 = build_tf_index(doc_repr_2)\n",
    "\n",
    "# This function returns the tf_index of the corresponding config\n",
    "def get_index(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: tf_index_1,\n",
    "        2: tf_index_2\n",
    "    }[index_set]\n",
    "\n",
    "####\n",
    "#### Preprocessed query based on the two configs\n",
    "\n",
    "# This function preprocesses the text given the index set, according to the specified config\n",
    "def preprocess_query(text, index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    if index_set == 1:\n",
    "        return process_text(text, **config_1)\n",
    "    elif index_set == 2:\n",
    "        return process_text(text, **config_2)\n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c50e4b4c68261ff1184277bcf3c38d8",
     "grade": true,
     "grade_id": "cell-fc7c7232d5d2ee46",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 1), (7, 1), (10, 1), (13, 1), (19, 1), (22, 1), (23, 1), (37, 1), (40, 3), (41, 1)]\n"
     ]
    }
   ],
   "source": [
    "##### Function check\n",
    "\n",
    "print(tf_index_1['computer'][:10])\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02de27463e2a393f46278d6fe3160481",
     "grade": true,
     "grade_id": "cell-ff06bd11204db250",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(670, 1), (675, 1), (1621, 3), (1681, 1), (2145, 1), (2339, 1), (2572, 1), (2583, 1), (2739, 1), (3012, 1)]\n"
     ]
    }
   ],
   "source": [
    "##### Function check\n",
    "\n",
    "print(tf_index_2['computer'][:10])\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "982112dca3f9a75e871bea74fe1adab4",
     "grade": false,
     "grade_id": "cell-89eba71f04310291",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "## Section 3: Ranking  (80 points) <a class=\"anchor\" id=\"ranking\"></a>\n",
    "\n",
    "[Back to Part 1](#part1)\n",
    "\n",
    "Now that we have cleaned and processed our dataset, we can start building simple IR systems. \n",
    "\n",
    "For now, we consider *simple* IR systems, which involve computing scores from the tokens present in the document/query. More advanced methods are covered in later assignments.\n",
    "\n",
    "We will implement the following methods in this section:\n",
    "- [Section 3.1: Bag of Words](#bow) (10 points)\n",
    "- [Section 3.2: TF-IDF](#tfidf) (15 points)\n",
    "- [Section 3.3: Query Likelihood Model](#qlm) (35 points)\n",
    "- [Section 3.4: BM25](#bm25) (20 points)\n",
    "\n",
    "**Scoring policy:**\n",
    "Your implementations in this section are scored based on the expected performance of your ranking functions.\n",
    "You will get a full mark if your implementation meets the expected performance (measured by some evaluation metric).\n",
    "Otherwise, you may get partial credit.\n",
    "For example, if your *Bag of words* ranking function has 60% of expected performance, you will get 4 out of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97c13d2e2cad60df4a70dcb61ea7c30e",
     "grade": false,
     "grade_id": "cell-3daf70a60e393adf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "--- \n",
    "\n",
    "### Section 3.1: Bag of Words (10 points)<a class=\"anchor\" id=\"bow\"></a>\n",
    "\n",
    "Probably the simplest IR model is the Bag of Words (BOW) model.\n",
    "Implement a function that scores and ranks all the documents against a query using this model.   \n",
    "\n",
    "Note that you can use either the count of the token or 'binarize' it i.e set the value equal to 1 if the token appears.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1178c8a8908b0e4dc07add7aa49b7fde",
     "grade": false,
     "grade_id": "cell-de9cf0459c4b9324",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "def bow_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query. \n",
    "        Note: You have to use the `get_index` function created in the previous cells\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    \n",
    "    index = get_index(index_set)\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    scores = {}\n",
    "    for q in processed_query:\n",
    "        for doc, count in index[q]:\n",
    "            if doc not in scores:\n",
    "                scores[doc] = 0\n",
    "            scores[doc] += count\n",
    "            \n",
    "    return [(d, float(s)) for d, s in sorted(scores.items(), key=lambda t: t[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "644d6d33cf5075c6d9878d4fe24c6213",
     "grade": true,
     "grade_id": "cell-9f6aceae6dd9125f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Results:\n",
      "Rank 0(6.0): An Information Algebra - Phase I Report-Language S...\n",
      "Rank 1(6.0): Rejuvenating Experimental Computer Science \\n This...\n",
      "Rank 2(3.0): ALGOL 60 Confidential \\n The ALGOL 60 Report,* whe...\n",
      "Rank 3(2.0): Automatic Abstracting and Indexing Survey and Reco...\n",
      "Rank 4(2.0): A String Language for Symbol Manipulation Based on...\n"
     ]
    }
   ],
   "source": [
    "#### Function check\n",
    "\n",
    "docs_by_id = dict(docs)\n",
    "def print_results(docs, len_limit=50):    \n",
    "    for i, (doc_id, score) in enumerate(docs):\n",
    "        doc_content = docs_by_id[doc_id].strip().replace(\"\\n\", \"\\\\n\")[:len_limit] + \"...\"\n",
    "        print(f\"Rank {i}({score:.2}): {doc_content}\")\n",
    "\n",
    "test_bow = bow_search(\"report\", index_set=1)[:5]\n",
    "print(f\"BOW Results:\")\n",
    "print_results(test_bow)\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d54c91b913d7a54b25f3358c969fa45",
     "grade": true,
     "grade_id": "cell-4eed3abf233d9b58",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58bcc16d36dc079b7f41f9572423c62b",
     "grade": true,
     "grade_id": "cell-4d65a2d7090c466c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e4561970c4eba3e5ce43b142f858af",
     "grade": true,
     "grade_id": "cell-dedf36ab5853ce20",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5c2318004c5ce534b67c7a36bbc7c31",
     "grade": false,
     "grade_id": "cell-a5c09c79ac1f2871",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Section 3.2: TF-IDF (15 points) <a class=\"anchor\" id=\"tfidf\"></a>\n",
    "\n",
    "Before we implement the tf-idf scoring functions, let's first write a function to compute the document frequencies of all words.  \n",
    "\n",
    "#### 3.2.1 Document frequency (5 points)\n",
    "Compute the document frequencies of all tokens in the collection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11cdde77378ded394d7922fe36bbc5d1",
     "grade": false,
     "grade_id": "cell-9a2369f32e864b8a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "\n",
    "def compute_df(documents):\n",
    "    \n",
    "    \"\"\"\n",
    "        Compute the document frequency of all terms in the vocabulary\n",
    "        Input: A list of documents\n",
    "        Output: A dictionary with {token: document frequency)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = dict()\n",
    "    \n",
    "    for document in documents:\n",
    "        for token in set(document):\n",
    "            if token in df:\n",
    "                df[token] += 1\n",
    "            else:\n",
    "                df[token] = 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "154985511d7925c5793a1f97dea81880",
     "grade": false,
     "grade_id": "cell-4c3bddd0b73ac90e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Compute df based on the two configs\n",
    "\n",
    "# get the document frequencies of each document\n",
    "df_1 = compute_df([d[1] for d in doc_repr_1])\n",
    "df_2 = compute_df([d[1] for d in doc_repr_2])\n",
    "\n",
    "def get_df(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: df_1,\n",
    "        2: df_2\n",
    "    }[index_set]\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "110cc180068cf3f77c682ee3de2a117c",
     "grade": true,
     "grade_id": "cell-79e8a6db1e5fc46f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "#### Function check\n",
    "\n",
    "print(df_1['computer'])\n",
    "print(df_2['computer'])\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6646996a2d016ef86e7cedd2f79a29d1",
     "grade": false,
     "grade_id": "cell-52f6acc487e1b96d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "#### 3.2.2 TF-IDF search (10 points)\n",
    "Next, implement a function that computes a tf-idf score given a query.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b33c859693d1b383bcb53acea340027f",
     "grade": false,
     "grade_id": "cell-2fb5ba34b2994cd9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "\n",
    "def tfidf_search(query, index_set):\n",
    "    \n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using tf-idf. \n",
    "        Note #1: You have to use the `get_index` (and the `get_df`) function created in the previous cells\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    \n",
    "    index = get_index(index_set)\n",
    "    df = get_df(index_set)\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    # Make a set of all involved documents\n",
    "    documents = set([z[0] for z in [y for x in index.values() for y in x]])\n",
    "    \n",
    "    # Make dict of all involved documents\n",
    "    tf = dict({key: 0 for key in documents})\n",
    "    \n",
    "    # Update dict with Inverse document frequency\n",
    "    df.update({n: np.log(len(documents)/df[n]) for n in df.keys()})\n",
    "    \n",
    "    # Multiply tf * idf and sum to calculate score\n",
    "    for token in processed_query:\n",
    "        if not token in index.keys():\n",
    "            continue\n",
    "        for doc in index[token]:\n",
    "            tf[doc[0]] += doc[1]*df[token]\n",
    "    \n",
    "    # Sort and return dict\n",
    "    return sorted(tf.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10b9fd39f3c9a9f3ca274c020ad79323",
     "grade": true,
     "grade_id": "cell-bc68aeeacf42beb3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Results:\n",
      "Rank 0(2.3e+01): An Information Algebra - Phase I Report-Language S...\n",
      "Rank 1(2.3e+01): Rejuvenating Experimental Computer Science \\n This...\n",
      "Rank 2(1.2e+01): ALGOL 60 Confidential \\n The ALGOL 60 Report,* whe...\n",
      "Rank 3(7.8): Automatic Abstracting and Indexing Survey and Reco...\n",
      "Rank 4(7.8): A String Language for Symbol Manipulation Based on...\n"
     ]
    }
   ],
   "source": [
    "#### Function check\n",
    "test_tfidf = tfidf_search(\"report\", index_set=1)[:5]\n",
    "print(f\"TFIDF Results:\")\n",
    "print_results(test_tfidf)\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03441f97df9db4996d0f260fcc974e9d",
     "grade": true,
     "grade_id": "cell-c7702fa8179fadb9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7272cfcbb457a74d23bfec1f85d22d5f",
     "grade": true,
     "grade_id": "cell-3284f50ac29abbaa",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcbcc867713612b90395d653db8fd5e3",
     "grade": true,
     "grade_id": "cell-d908c80a3155354b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdffc83f0eaea937cf64a212e7e9af8d",
     "grade": false,
     "grade_id": "cell-f5d923459ba21733",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "--- \n",
    "\n",
    "### Section 3.3: Query Likelihood Model (35 points) <a class=\"anchor\" id=\"qlm\"></a>\n",
    "\n",
    "In this section, you will implement a simple query likelihood model. \n",
    "\n",
    "\n",
    "#### 3.3.1 Naive QL (15 points)\n",
    "\n",
    "First, let us implement a naive version of a QL model, assuming a multinomial unigram language model (with a uniform prior over the documents). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7ae2b7d818b428b4638c1c9206d2aca",
     "grade": false,
     "grade_id": "cell-98505778f7b68e7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Document length for normalization\n",
    "\n",
    "def doc_lengths(documents):\n",
    "    doc_lengths = {doc_id:len(doc) for (doc_id, doc) in documents}\n",
    "    return doc_lengths\n",
    "\n",
    "doc_lengths_1 = doc_lengths(doc_repr_1)\n",
    "doc_lengths_2 = doc_lengths(doc_repr_2)\n",
    "\n",
    "def get_doc_lengths(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: doc_lengths_1,\n",
    "        2: doc_lengths_2\n",
    "    }[index_set]\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cedd08303a914243fefdb6b876977ca1",
     "grade": false,
     "grade_id": "cell-8bcf2b804d636c2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (15 points)\n",
    "def naive_ql_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using a naive QL model. \n",
    "        Note #1: You have to use the `get_index` (and get_doc_lengths) function created in the previous cells\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    index = get_index(index_set)\n",
    "    doc_lengths = get_doc_lengths(index_set)\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b550d15bdad28354c336020a00c33d56",
     "grade": true,
     "grade_id": "cell-5a83ac12ecde8578",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5fb2f4783055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#### Function check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_naiveql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaive_ql_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"report\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Naive QL Results:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_naiveql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-bdf666939c1f>\u001b[0m in \u001b[0;36mnaive_ql_search\u001b[0;34m(query, index_set)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprocessed_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### Function check\n",
    "test_naiveql = naive_ql_search(\"report\", index_set=1)[:5]\n",
    "print(f\"Naive QL Results:\")\n",
    "print_results(test_naiveql)\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "799df1d81c63fe90edbb6c218fc707fb",
     "grade": true,
     "grade_id": "cell-80f4bf2137f997bb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "875a4a517d27e20625d41783cebec118",
     "grade": true,
     "grade_id": "cell-5ce2993458a8ce51",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5c4e1c3852e71a46f32825b122f1b71",
     "grade": true,
     "grade_id": "cell-7753bdb54e292f3d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4d4332d4356e89ce0240f6b80e1899a",
     "grade": true,
     "grade_id": "cell-54e476e2f96e64bb",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d739dc91a22bd48897f603885f95a74",
     "grade": false,
     "grade_id": "cell-5414dfd69dab8b94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "#### 3.3.2 QL (20 points)\n",
    "Now, let's implement a QL model that handles the issues with the naive version. In particular, you will implement a QL model with Jelinek-Mercer Smoothing. That means an interpolated score is computed per word - one term is the same as the previous naive version, and the second term comes from a unigram language model. In addition, you should accumulate the scores by summing the **log** (smoothed) probability which leads to better numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8c6abf937ad333e628f1db891f2e29e",
     "grade": false,
     "grade_id": "cell-bb1f506409771257",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "def ql_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using a QL model \n",
    "        with Jelinek-Mercer Smoothing (set smoothing=0.1). \n",
    "        \n",
    "        \n",
    "        Note #1: You have to use the `get_index` (and get_doc_lengths) function created in the previous cells\n",
    "        Note #2: You might have to create some variables beforehand and use them in this function\n",
    "        \n",
    "        \n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    index = get_index(index_set)\n",
    "    doc_lengths = get_doc_lengths(index_set)\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b12a7f3355193a257fd9f5f69a66562",
     "grade": true,
     "grade_id": "cell-850e9d6369bcec32",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Function check\n",
    "test_ql_results = ql_search(\"report\", index_set=1)[:5]\n",
    "print_results(test_ql_results)\n",
    "print()\n",
    "test_ql_results_long = ql_search(\"report \" * 10, index_set=1)[:5]\n",
    "print_results(test_ql_results_long)\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e40be645140389c115849856145f5b59",
     "grade": true,
     "grade_id": "cell-958cdcf6fd6899b7",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41d4aff001df17e7963ba79b45810b30",
     "grade": true,
     "grade_id": "cell-384dc23a0c251f6e",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "893e8c5a073abc8ebb763d267b91bc02",
     "grade": true,
     "grade_id": "cell-7218966cba5097cc",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f99cb6f8b1f5830aaed8f06712ff846e",
     "grade": true,
     "grade_id": "cell-481ab073259ae53f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c02f14705d679579b1aa9f78f54779d5",
     "grade": false,
     "grade_id": "cell-f44088bfdac1dc90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "--- \n",
    "\n",
    "### Section 3.4: BM25 (20 points) <a class=\"anchor\" id=\"bm25\"></a>\n",
    "\n",
    "In this section, we will implement the BM25 scoring function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e57efe06ea92af1c83784a42eb3d86e0",
     "grade": false,
     "grade_id": "cell-15640fc9b5d00a3c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)\n",
    "def bm25_search(query, index_set, k_1=1.5, b=0.75):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using BM25. Use k_1 = 1.5 and b = 0.75\n",
    "        Note #1: You have to use the `get_index` (and `get_doc_lengths`) function created in the previous cells\n",
    "        Note #2: You might have to create some variables beforehand and use them in this function\n",
    "        \n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    \n",
    "    index = get_index(index_set)\n",
    "    df = get_df(index_set)\n",
    "    doc_lengths = get_doc_lengths(index_set)\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    scores = {}\n",
    "    N = len(doc_lengths)\n",
    "    avdl = sum(doc_lengths.values()) / N\n",
    "    for q in processed_query:\n",
    "        inv_list = index[q]\n",
    "        n = len(inv_list)\n",
    "        \n",
    "        for doc_id, f in inv_list:\n",
    "            if doc_id not in scores:\n",
    "                scores[doc_id] = 0\n",
    "                \n",
    "            dl = doc_lengths[doc_id]\n",
    "            K = k_1*((1-b) + b*dl/avdl)\n",
    "            first_part = 1. / ((n + 0.5) / (N - n + 0.5))\n",
    "            second_part = ((k_1+1)*f) / (K+f)\n",
    "            score = np.log(first_part * second_part)\n",
    "            scores[doc_id] += score\n",
    "    \n",
    "    return [(d, float(s)) for d, s in sorted(scores.items(), key=lambda t: t[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4be9de5d4e94637960d83725422bea6c",
     "grade": true,
     "grade_id": "cell-d10536bca72c74b1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0(4.4): A Report Writer For COBOL...\n",
      "Rank 1(4.4): A CRT Report Generating System...\n",
      "Rank 2(4.4): Preliminary Report-International Algebraic Languag...\n",
      "Rank 3(4.4): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(4.4): ALGOL Sub-Committee Report - Extensions...\n"
     ]
    }
   ],
   "source": [
    "#### Function check\n",
    "test_bm25_results = bm25_search(\"report\", index_set=1)[:5]\n",
    "print_results(test_bm25_results)\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31b550d6a46ae4f8ede88788799ac2b9",
     "grade": true,
     "grade_id": "cell-60f6ec5052712d79",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3da2ec16bfe781916e71755d65aa2983",
     "grade": true,
     "grade_id": "cell-5d17524043a5abcc",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7b563d54fa84c20909c0ae107010541",
     "grade": true,
     "grade_id": "cell-ff8e704eda1184e3",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b013f90974b396630a8831d6f1d7e5f7",
     "grade": true,
     "grade_id": "cell-a52310500a2543cb",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fcf67cb7d5e8b26cb9bf1f0aa42c847",
     "grade": false,
     "grade_id": "cell-8b2b412c81d62f2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 3.5. Test Your Functions\n",
    "\n",
    "The widget below allows you to play with the search functions you've written so far. Use this to test your search functions and ensure that they work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfeb204b390acc0794dbdcac92b0cf2c",
     "grade": false,
     "grade_id": "cell-c9c2bb76354e8d97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Highlighter function\n",
    "# class for results\n",
    "ResultRow = namedtuple(\"ResultRow\", [\"doc_id\", \"snippet\", \"score\"])\n",
    "# doc_id -> doc\n",
    "docs_by_id = dict((d[0], d[1]) for d in docs)\n",
    "\n",
    "def highlight_text(document, query, tol=17):\n",
    "    import re\n",
    "    tokens = tokenize(query)\n",
    "    regex = \"|\".join(f\"(\\\\b{t}\\\\b)\" for t in tokens)\n",
    "    regex = re.compile(regex, flags=re.IGNORECASE)\n",
    "    output = \"\"\n",
    "    i = 0\n",
    "    for m in regex.finditer(document):\n",
    "        start_idx = max(0, m.start() - tol)\n",
    "        end_idx = min(len(document), m.end() + tol)\n",
    "        output += \"\".join([\"...\",\n",
    "                        document[start_idx:m.start()],\n",
    "                        \"<strong>\",\n",
    "                        document[m.start():m.end()],\n",
    "                        \"</strong>\",\n",
    "                        document[m.end():end_idx],\n",
    "                        \"...\"])\n",
    "    return output.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "def make_results(query, search_fn, index_set):\n",
    "    results = []\n",
    "    for doc_id, score in search_fn(query, index_set):\n",
    "        highlight = highlight_text(docs_by_id[doc_id], query)\n",
    "        if len(highlight.strip()) == 0:\n",
    "            highlight = docs_by_id[doc_id]\n",
    "        results.append(ResultRow(doc_id, highlight, score))\n",
    "    return results\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1673a21725fb47cf96c748fb69164625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search Bar')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Set this to the function you want to test!\n",
    "# this function should take in a query (string)\n",
    "# and return a sorted list of (doc_id, score) \n",
    "# with the most relevant document in the first position\n",
    "search_fn = bm25_search\n",
    "index_set = 1\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "def handle_submit(sender):\n",
    "    print(f\"Searching for: '{sender.value}'\")\n",
    "    \n",
    "    results = make_results(sender.value, search_fn, index_set)\n",
    "    \n",
    "    # display only the top 5\n",
    "    results = results[:5]\n",
    "    \n",
    "    body = \"\"\n",
    "    for idx, r in enumerate(results):\n",
    "        body += f\"<li>Document #{r.doc_id}({r.score}): {r.snippet}</li>\"\n",
    "    display(HTML(f\"<ul>{body}</ul>\"))\n",
    "    \n",
    "\n",
    "text.on_submit(handle_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b315c280048a50e8bad7d6250f6f39d1",
     "grade": false,
     "grade_id": "cell-8d46fe8e4f3d8cdb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 4: Evaluation (40 points) <a class=\"anchor\" id=\"evaluation\"></a>\n",
    "\n",
    "[Back to Part 1](#part1)\n",
    "\n",
    "Before we jump in and implement an algorithm for retrieval, we first have to learn how to evaluate such a system. In particular, we will work with offline evaluation metrics. These metrics are computed on a dataset with known relevance judgements.\n",
    "\n",
    "Implement the following evaluation metrics. \n",
    "\n",
    "1. Precision (7 points)\n",
    "2. Recall (7 points)\n",
    "3. Mean Average Precision (12 points)\n",
    "4. Expected Reciprocal Rank (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82e90076c51893e31fec5c40c49d1828",
     "grade": false,
     "grade_id": "cell-3419fd3bc663d7cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 4.1 Read relevance labels (2 points)\n",
    "\n",
    "Let's take a look at the `qrels.text` file, which contains the ground truth relevance scores. The relevance labels for CACM are binary - either 0 or 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c11025f5a222796f2882c73c1634799",
     "grade": false,
     "grade_id": "cell-6b738366059dde9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "!head ./datasets/qrels.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e77d7cc6aae69fdbadb5ac9f78f8a560",
     "grade": false,
     "grade_id": "cell-10e16bff2753ffbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "**Implementation (2 points):**\n",
    "The first column is the query_id and the second column is the document_id. You can safely ignore the 3rd and 4th columns. Write a function to read in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef347565a0990ddbd049835105753d59",
     "grade": false,
     "grade_id": "cell-ee5253a4ef602fce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (2 points)\n",
    "def read_qrels(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads the qrels.text file. \n",
    "        Output: A dictionary: query_id -> [list of relevant documents]\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a258a834392cc8cbf8af5d4ccdee81f",
     "grade": true,
     "grade_id": "cell-72215605fbe24f65",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Function check\n",
    "qrels = read_qrels()\n",
    "\n",
    "assert len(qrels) == 52, \"There should be 52 queries with relevance judgements\"\n",
    "assert sum(len(j) for j in qrels.values()) == 796, \"There should be a total of 796 Relevance Judgements\"\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c9e7428a52e291a2cdf92a379730d4c",
     "grade": false,
     "grade_id": "cell-176a6fb2939d0420",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "**Note:** For a given query `query_id`, you can assume that documents *not* in `qrels[query_id]` are not relevant to `query_id`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b26a818c7b4f7ad490e00b35ea0edd69",
     "grade": false,
     "grade_id": "cell-bd8341b72cdd89bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 4.2 Precision (7 points)\n",
    "Implement the `precision@k` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad3cc3234a361d269ecb6b59cc447e9f",
     "grade": false,
     "grade_id": "cell-494bd0cce108ed67",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (7 points)\n",
    "def precision_k(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "        Compute Precision@K\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), \n",
    "                    with the most relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "            k: the cut-off\n",
    "        Output: Precision@K\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "964b214dbfff8fc53cada864019ee863",
     "grade": true,
     "grade_id": "cell-e7ff0d91c319ca64",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Function check\n",
    "qid = queries[0][0]\n",
    "qtext = queries[0][1]\n",
    "print(f'query:{qtext}')\n",
    "results = bm25_search(qtext, 2)\n",
    "precision = precision_k(results, qrels[qid], 10)\n",
    "print(f'precision@10 = {precision}')\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fd3b3998197c7097a40348500affb68",
     "grade": false,
     "grade_id": "cell-afd95f865bc7191e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 4.3 Recall (7 points)\n",
    "Implement the `recall@k` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2020e5741ae03b3fe35817ed8f4ccaa",
     "grade": false,
     "grade_id": "cell-c323fc8c3f8a7cf8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (7 points)\n",
    "def recall_k(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "        Compute Recall@K\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "            k: the cut-off\n",
    "        Output: Recall@K\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56b6e0b8522f8b2dffbfb3206b2efa84",
     "grade": true,
     "grade_id": "cell-b25172161aef165c",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Function check\n",
    "qid = queries[10][0]\n",
    "qtext = queries[10][1]\n",
    "print(f'query:{qtext}')\n",
    "results = bm25_search(qtext, 2)\n",
    "recall = recall_k(results, qrels[qid], 10)\n",
    "print(f'recall@10 = {recall}')\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d43c63fa38f333e0b52d41639b2655f7",
     "grade": false,
     "grade_id": "cell-77fd2e7a39a74739",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 4.4 Mean Average Precision (12 points)\n",
    "Implement the `map` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aae2c62f2ffd76f5b6c004e9519b9f14",
     "grade": false,
     "grade_id": "cell-e50925fa9093a30d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (12 points)\n",
    "def average_precision(results, relevant_docs):\n",
    "    \"\"\"\n",
    "        Compute Average Precision (for a single query - the results are \n",
    "        averaged across queries to get MAP in the next few cells)\n",
    "        Hint: You can use the recall_k and precision_k functions here!\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most \n",
    "                    relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "        Output: Average Precision\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5db8cb2dfd3a77554f4147b409f47f38",
     "grade": true,
     "grade_id": "cell-8a1f7ec98571e58b",
     "locked": true,
     "points": 12,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Function check\n",
    "qid = queries[20][0]\n",
    "qtext = queries[20][1]\n",
    "print(f'query:{qtext}')\n",
    "results = bm25_search(qtext, 2)\n",
    "mean_ap = average_precision(results, qrels[qid])\n",
    "print(f'MAP = {mean_ap}')\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb645b79d04cc3eed33ca060a5e7bf0f",
     "grade": false,
     "grade_id": "cell-1da18f0fe6f6d7be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 4.5 Expected Reciprocal Rank (12 points)\n",
    "Implement the `err` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ac94db728e23ea1f5dc0d509473c6fb",
     "grade": false,
     "grade_id": "cell-64262889f9b267ea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (12 points)\n",
    "def err(results, relevant_docs):\n",
    "    \"\"\"\n",
    "        Compute the expected reciprocal rank.\n",
    "        Hint: https://dl.acm.org/doi/pdf/10.1145/1645953.1646033?download=true\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most \n",
    "                    relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "        Output: ERR\n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb97e6d89a0b992b642e4e0ff36ff9f5",
     "grade": true,
     "grade_id": "cell-071e3970ff1afae4",
     "locked": true,
     "points": 12,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Function check\n",
    "qid = queries[30][0]\n",
    "qtext = queries[30][1]\n",
    "print(f'query:{qtext}')\n",
    "results = bm25_search(qtext, 2)\n",
    "ERR = err(results, qrels[qid])\n",
    "print(f'ERR = {ERR}')\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bd94caf77cfa5f34675df758d91002d",
     "grade": false,
     "grade_id": "cell-43709a765f353946",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### 4.6 Evaluate Search Functions\n",
    "\n",
    "Let's define some metrics@k using [partial functions](https://docs.python.org/3/library/functools.html#functools.partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49ccc158e5fb7340ace55e90eeb9d62a",
     "grade": false,
     "grade_id": "cell-dab560e18e340da8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### metrics@k functions\n",
    "\n",
    "recall_at_1 = partial(recall_k, k=1)\n",
    "recall_at_5 = partial(recall_k, k=5)\n",
    "recall_at_10 = partial(recall_k, k=10)\n",
    "precision_at_1 = partial(precision_k, k=1)\n",
    "precision_at_5 = partial(precision_k, k=5)\n",
    "precision_at_10 = partial(precision_k, k=10)\n",
    "\n",
    "\n",
    "list_of_metrics = [\n",
    "    (\"ERR\", err),\n",
    "    (\"MAP\", average_precision),\n",
    "    (\"Recall@1\",recall_at_1),\n",
    "    (\"Recall@5\", recall_at_5),\n",
    "    (\"Recall@10\", recall_at_10),\n",
    "    (\"Precision@1\", precision_at_1),\n",
    "    (\"Precision@5\", precision_at_5),\n",
    "    (\"Precision@10\", precision_at_10)]\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb541002c03221b453b8936290020ea5",
     "grade": false,
     "grade_id": "cell-580a2bdc66d03b47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "The following function evaluates a `search_fn` using the `metric_fn`. Note that the final number is averaged over all the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Evaluate a search function\n",
    "\n",
    "list_of_search_fns = [\n",
    "    (\"BOW\", bow_search),\n",
    "    (\"TF-IDF\", tfidf_search),\n",
    "    (\"NaiveQL\", naive_ql_search),\n",
    "    (\"QL\", ql_search),\n",
    "    (\"BM25\", bm25_search)\n",
    "]\n",
    "\n",
    "def evaluate_search_fn(search_fn, metric_fns, index_set=None):\n",
    "    # build a dict query_id -> query \n",
    "    queries_by_id = dict((q[0], q[1]) for q in queries)\n",
    "    \n",
    "    metrics = {}\n",
    "    for metric, metric_fn in metric_fns:\n",
    "        metrics[metric] = np.zeros(len(qrels), dtype=np.float32)\n",
    "    \n",
    "    for i, (query_id, relevant_docs) in enumerate(qrels.items()):\n",
    "        query = queries_by_id[query_id]\n",
    "        if index_set:\n",
    "            results = search_fn(query, index_set)\n",
    "        else:\n",
    "            results = search_fn(query)\n",
    "        \n",
    "        for metric, metric_fn in metric_fns:\n",
    "            metrics[metric][i] = metric_fn(results, relevant_docs)\n",
    "\n",
    "    \n",
    "    \n",
    "    final_dict = {}\n",
    "    for metric, metric_vals in metrics.items():\n",
    "        final_dict[metric] = metric_vals.mean()\n",
    "    \n",
    "    return final_dict\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ea67534f74a8f81e1f504794f641709",
     "grade": false,
     "grade_id": "cell-b156d83a0649cbb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 5: Analysis (30 points) <a class=\"anchor\" id=\"analysis\"></a>\n",
    "\n",
    "[Back to Part 1](#part1)\n",
    "\n",
    "In the final section of Part1, we will compare the different term-based IR algorithms and different preprocessing configurations and analyze their advantages and disadvantages.\n",
    "\n",
    "### Section 5.1: Plot (20 points)\n",
    "\n",
    "First, gather the results. The results should consider the index set, the different search functions and different metrics. Plot the results in bar charts, per metric, with clear labels.\n",
    "\n",
    "**Rubric:**\n",
    "- Each Metric is plotted: 7 points\n",
    "- Each Method is plotted: 7 points\n",
    "- Clear titles, x label, y labels and legends (if applicable): 6 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e2588a925d13ddf588abe8311dc9cfc",
     "grade": true,
     "grade_id": "cell-46fda42a25863a04",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e88c444a0acf4e398c65e289169b75f7",
     "grade": false,
     "grade_id": "cell-8aabe3bcf265deb0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "---\n",
    "### Section 5.2: Summary (10 points)\n",
    "Write a summary of what you observe in the results.\n",
    "Your summary should compare results across the 2 indices and the methods being used. State what you expected to see in the results, followed by either supporting evidence *or* justify why the results did not support your expectations.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3eb77be74eecca205fc7b47316d1627",
     "grade": false,
     "grade_id": "cell-bb60dd5c092d0f2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "---\n",
    "# Part 2: Semantic-based Matching (85 points) <a class=\"anchor\" id=\"part2\"></a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "We will now experiment with methods that go beyond lexical methods like TF-IDF, which operate at the word level and are high dimensional and sparse, and look at methods which constructs low dimensional dense representations of queries and documents. \n",
    "\n",
    "Since these low-dimensional methods have a higher time complexity, they are typically used in conjunction with methods like BM-25. That is, instead of searching through potentially million documents to find matches using low dimensional vectors, a list of K documents are retrieved using BM25, and then **re-ranked** using the other method. This is the method that is going to be applied in the following exercises. \n",
    "\n",
    "LSI/LDA takes documents that are similar on a semantic level - for instance, if they are describing the same topic - and projects them into nearby vectors, despite having low lexical overlap.\n",
    "\n",
    "In this assignment, you will use `gensim` to create LSI/LDA models and use them in re-ranking. \n",
    "\n",
    "**Note**: The following exercises only uses `doc_repr_2` and `config_2`\n",
    "\n",
    "Table of contents:\n",
    "- [Section 6: LSI](#lsi) (15 points)\n",
    "- [Section 7: LDA](#lda) (10 points)\n",
    "- [Section 8: Word2Vec/Doc2Vec](#2vec) (20 points)\n",
    "- [Section 8: Re-ranking](#reranking) (10 points)\n",
    "- [Section 9: Re-ranking Evaluation](#reranking_eval) (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7c7b2cab82f576ed0acf836ca57171c",
     "grade": false,
     "grade_id": "cell-6b2c81e7a8abd180",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Section 6: Latent Semantic Indexing (LSI) (15 points) <a class=\"anchor\" id=\"lsi\"></a>\n",
    "\n",
    "[Back to Part 2](#part2)\n",
    "\n",
    "LSI is one of the methods to embed the queries and documents into vectors. It is based on a method similar to Principal Component Analysis (PCA) for obtaining a dense concept matrix out of the sparse term-document matrix.\n",
    "\n",
    "See [wikipedia](https://en.wikipedia.org/wiki/Latent_semantic_analysis), particularly [#Mathematics_of_LSI](https://en.wikipedia.org/wiki/Latent_semantic_analysis#Mathematics_of_LSI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c17ee75319cb517e2bf48ec3d9efc329",
     "grade": false,
     "grade_id": "cell-59913daee47f680d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, LsiModel, Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import downloader as g_downloader\n",
    "# gensim uses logging, so set it up \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fce140c546759b54a9fc060901ae77c",
     "grade": false,
     "grade_id": "cell-3644faff4976598a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### Section 6.1: Cosine Similarity (5 points)<a class=\"anchor\" id=\"cosing_sim\"></a>\n",
    "Before we begin, let us first define our method of similarity for the LSI model, the cosine similarity:\n",
    "\n",
    "$$\\text{similarity} = \\cos(\\theta) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} }$$\n",
    "\n",
    "Since we are using gensim, the types of vectors returned by their classes are of the form defined below (they are not just simple vectors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e340e1a1d546f430c018fd0760e707a",
     "grade": false,
     "grade_id": "cell-3995a50f951314d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1, 2, 3 are either latent dimensions (LSI), or topics (LDA)\n",
    "# The second value in each tuple is a number (LSI) or a probability (LDA)  \n",
    "example_vec_1 = [(1, 0.2), (2, 0.3), (3, 0.4)]\n",
    "example_vec_2 = [(1, 0.2), (2, 0.7), (3, 0.4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20832fd4f579f49ae204b0efee02edd1",
     "grade": false,
     "grade_id": "cell-5e54d581858dc8f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "**Implementation (2+3 points):**\n",
    "Now, implement the `dot product` operation on these types of vectors and using this operator, implement the `cosine similarity` (don't forget: two functions to implement!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06a712ee75fc213a21c5f0067fd8fe28",
     "grade": false,
     "grade_id": "cell-0e8189f5f93de33f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (2 points)\n",
    "def dot(vec_1,vec_2): \n",
    "    \"\"\"\n",
    "        vec_1 and vec_2 are of the form: [(int, float), (int, float), ...]\n",
    "        Return the dot product of two such vectors, computed only on the floats\n",
    "        You can assume that the lengths of the vectors are the same, and the dimensions are aligned \n",
    "            i.e you won't get: vec_1 = [(1, 0.2)] ; vec_2 = [(2, 0.3)] \n",
    "                                (dimensions are unaligned and lengths are different)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Implement this! (3 points)\n",
    "def cosine_sim(vec_1, vec_2):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d22a4a7325ba7815a808390388f534a1",
     "grade": true,
     "grade_id": "cell-b25d04ed6b79fd35",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Function check\n",
    "print(f'vectors: {(example_vec_1,example_vec_2)}')\n",
    "print(f'dot product = {dot(example_vec_1,example_vec_2)}')\n",
    "print(f'cosine similarity = {cosine_sim(example_vec_1,example_vec_2)}')\n",
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0744131724ce76b1b3f163b4bae5f700",
     "grade": true,
     "grade_id": "cell-ae3c4466866ace77",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b123f599f9ea372d14676e23f1c6a52",
     "grade": false,
     "grade_id": "cell-4b2534067c44fcdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### Section 6.2: LSI Retrieval (10 points)<a class=\"anchor\" id=\"lsi_retrieval\"></a>\n",
    "LSI retrieval is simply ranking the documents based on their cosine similarity to the query vector.\n",
    "First, let's write a parent class for vector-based retrieval models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecc111d58182570e2252b8ef5d6b02af",
     "grade": false,
     "grade_id": "cell-937936cea18711ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class VectorSpaceRetrievalModel:\n",
    "    \"\"\"\n",
    "        Parent class for Dense Vector Retrieval models\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_repr):\n",
    "        \"\"\"\n",
    "            document_collection: \n",
    "                [\n",
    "                    (doc_id_1, [token 1, token 2, ...]), \n",
    "                    (doc_id_2, [token 1, token 2, ....]) \n",
    "                    ...\n",
    "                ]\n",
    "\n",
    "        \"\"\"\n",
    "        self.doc_repr = doc_repr\n",
    "        self.documents = [_[1] for _ in self.doc_repr]\n",
    "        \n",
    "        # construct a dictionary\n",
    "        self.dictionary = Dictionary(self.documents)\n",
    "        # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "        self.dictionary.filter_extremes(no_below=10)\n",
    "        self.corpus = [self.dictionary.doc2bow(doc) for doc in self.documents]\n",
    "    \n",
    "        # Make a index to word dictionary.\n",
    "        temp = self.dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "        self.id2word = self.dictionary.id2token\n",
    "        \n",
    "        # this is set by the train_model function\n",
    "        self.model = None\n",
    "        \n",
    "        \n",
    "    def vectorize_documents(self):\n",
    "        \"\"\"\n",
    "            Returns a doc_id -> vector dictionary\n",
    "        \"\"\"\n",
    "        vectors = {}\n",
    "        for (doc_id, _), cc in zip(self.doc_repr, self.corpus):\n",
    "            vectors[doc_id] = self.model[cc]\n",
    "        return vectors\n",
    "\n",
    "    def vectorize_query(self, query):\n",
    "        # Note the use of config_2 here!\n",
    "        query = process_text(query, **config_2)\n",
    "        query_vector = self.dictionary.doc2bow(query)\n",
    "        return self.model[query_vector]\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "            Trains a model and sets the 'self.model' variable. \n",
    "            Make sure to use the variables created in the __init__ method.\n",
    "            e.g the variables which may be useful: {corpus, dictionary, id2word}\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff161eefd9b81b768cd6361bc1a502b0",
     "grade": false,
     "grade_id": "cell-704a18c2f80cd60c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "**Implementation (5 points):**\n",
    "Implement the `train_model` method in the following class (note that this is only one line of code in `gensim`!). Ensure that the parameters defined in the `__init__` method are not changed, and are *used in the `train_method` function*. Normally, the hyperaparameter space will be searched using grid search / other methods - in this assignment we have provided the hyperparameters for you.\n",
    "\n",
    "The last two lines of code train an LSI model on the list of documents which have been stemmed, lower-cased and have stopwords removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e90eedc27c248bc1ae050518a46a46c",
     "grade": false,
     "grade_id": "cell-307682c9089f15d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "class LsiRetrievalModel(VectorSpaceRetrievalModel):\n",
    "    def __init__(self, doc_repr):\n",
    "        super().__init__(doc_repr)\n",
    "        \n",
    "        self.num_topics = 100\n",
    "        self.chunksize = 2000\n",
    "    \n",
    "    def train_model(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00399cfe13d60cb4beed1271e36004b0",
     "grade": true,
     "grade_id": "cell-5ce512650c1b2dfb",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Function check\n",
    "lsi = LsiRetrievalModel(doc_repr_2)\n",
    "lsi.train_model()\n",
    "\n",
    "# you can now get an LSI vector for a given query in the following way:\n",
    "lsi.vectorize_query(\"report\")\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7116bb9f576c5bb04934e1d59c51d729",
     "grade": false,
     "grade_id": "cell-4c5eeb557b4fca2f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1068a108600b8c1539117d543e720354",
     "grade": false,
     "grade_id": "cell-c4e50296cd17a555",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "Next, implement a basic ranking class for vector space retrieval (used for all semantic methods): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a8389d2f0635c3405e2b0b27ed9f327",
     "grade": false,
     "grade_id": "cell-250515d288e80cdc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "class DenseRetrievalRanker:\n",
    "    def __init__(self, vsrm, similarity_fn):\n",
    "        \"\"\"\n",
    "            vsrm: instance of `VectorSpaceRetrievalModel`\n",
    "            similarity_fn: function instance that takes in two vectors \n",
    "                            and returns a similarity score e.g cosine_sim defined earlier\n",
    "        \"\"\"\n",
    "        self.vsrm = vsrm \n",
    "        self.vectorized_documents = self.vsrm.vectorize_documents()\n",
    "        self.similarity_fn = similarity_fn\n",
    "    \n",
    "    def _compute_sim(self, query_vector):\n",
    "        \"\"\"\n",
    "            Compute the similarity of `query_vector` to documents in \n",
    "            `self.vectorized_documents` using `self.similarity_fn`\n",
    "            Returns a list of (doc_id, score) tuples\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def search(self, query):\n",
    "        scores = self._compute_sim(self.vsrm.vectorize_query(query))\n",
    "        scores.sort(key=lambda _:-_[1])\n",
    "        return scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f237dd1ef6c1783c06797f4b514421f5",
     "grade": true,
     "grade_id": "cell-b73068b3e77a8e31",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Function check\n",
    "drm_lsi = DenseRetrievalRanker(lsi, cosine_sim)\n",
    "drm_lsi.search(\"report\")[:5]\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b3f19fdcaa585d263706d5a26038799",
     "grade": false,
     "grade_id": "cell-034c755a6502b868",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcafef6e037033c46304b914f7c78bdf",
     "grade": false,
     "grade_id": "cell-d1df23f497d5ed6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "Now, you can test your LSI model in the following cell: try finding queries which are lexically different to documents, but semantically similar - does LSI work well for these queries?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11734bc7674377b340ad51297a8e8bb5",
     "grade": false,
     "grade_id": "cell-efd1d08dfc04ec3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your LSI model\n",
    "search_fn = drm_lsi.search\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "def make_results_2(query, search_fn):\n",
    "    results = []\n",
    "    for doc_id, score in search_fn(query):\n",
    "        highlight = highlight_text(docs_by_id[doc_id], query)\n",
    "        if len(highlight.strip()) == 0:\n",
    "            highlight = docs_by_id[doc_id]\n",
    "        results.append(ResultRow(doc_id, highlight, score))\n",
    "    return results\n",
    "\n",
    "def handle_submit_2(sender):\n",
    "    print(f\"Searching for: '{sender.value}' (SEARCH FN: {search_fn})\")\n",
    "    \n",
    "    results = make_results_2(sender.value, search_fn)\n",
    "    \n",
    "    # display only the top 5\n",
    "    results = results[:5]\n",
    "    \n",
    "    body = \"\"\n",
    "    for idx, r in enumerate(results):\n",
    "        body += f\"<li>Document #{r.doc_id}({r.score}): {r.snippet}</li>\"\n",
    "    display(HTML(f\"<ul>{body}</ul>\"))\n",
    "    \n",
    "\n",
    "text.on_submit(handle_submit_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d074ce1ca48384cdda78742741c938be",
     "grade": false,
     "grade_id": "cell-3a86cef264d8f6cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Section 7: Latent Dirichlet Allocation (LDA) (10 points) <a class=\"anchor\" id=\"lda\"></a>\n",
    "\n",
    "[Back to Part 2](#part2)\n",
    "\n",
    "The specifics of LDA is out of the scope of this assignment, but we will use the `gensim` implementation to perform search using LDA over our small document collection. The key thing to remember is that LDA, unlike LSI, outputs a topic **distribution**, not a vector. With that in mind, let's first define a similarity measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db01092373b18f0c9dfed1bb17db4ad9",
     "grade": false,
     "grade_id": "cell-6b78ad22c2d60ba7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### Section 7.1: Jenson-Shannon divergence (5 points) <a class=\"anchor\" id=\"js_sim\"></a>\n",
    "\n",
    "The Jenson-Shannon divergence is a symmetric and finite measure on two probability distributions (unlike the KL, which is neither). For identical distributions, the JSD is equal to 0, and since our code uses 0 as irrelevant and higher scores as relevant, we use `(1 - JSD)` as the score or 'similarity' in our setup\n",
    "\n",
    "**Note**: the JSD is bounded to \\[0,1\\] only if we use log base 2. So please ensure that you're using `np.log2` instead of `np.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a579e6cd7a24a3516bc9a84528b392d3",
     "grade": false,
     "grade_id": "cell-d2376a85a4841e98",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## TODO: Implement this! (5 points)\n",
    "def jenson_shannon_divergence(vec_1, vec_2, assert_prob=False):\n",
    "    \"\"\"\n",
    "        Computes the Jensen-Shannon divergence between two probability distributions. \n",
    "        NOTE: DO NOT RETURN 1 - JSD here, that is handled by the next function which is already implemented! \n",
    "        The inputs are *gensim* vectors - same as the vectors for the cosine_sim function\n",
    "        assert_prob is a flag that checks if the inputs are proper probability distributions \n",
    "            i.e they sum to 1 and are positive - use this to check your inputs if needed. \n",
    "                (This is optional to implement, but recommended - \n",
    "                you can the default to False to save a few ms off the runtime)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def jenson_shannon_sim(vec_1, vec_2, assert_prob=False):\n",
    "    return 1 - jenson_shannon_divergence(vec_1, vec_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab340aa941c9fb5c89b3fd0a9139e246",
     "grade": true,
     "grade_id": "cell-487c6d2933f38053",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Function check\n",
    "vec_1 = [(1, 0.3), (2, 0.4), (3, 0.3)]\n",
    "vec_2 = [(1, 0.1), (2, 0.7), (3, 0.2)]\n",
    "jenson_shannon_sim(vec_1, vec_2, assert_prob=True)\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a1583a5f23e3390038331cce67f5d8e",
     "grade": false,
     "grade_id": "cell-4535cc67a50b80fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "### Section 7.2: LDA retrieval (5 points) <a class=\"anchor\" id=\"lda_ret\"></a>\n",
    "\n",
    "Implement the `train_model` method in the following class (note that this is only one line of code in `gensim`!). Ensure that the parameters defined in the `__init__` method are not changed, and are *used in the `train_method` function*. You do not need to set this. Normally, the hyperaparameter space will be searched using grid search / other methods. Note that training the LDA model might take some time\n",
    "\n",
    "The last two lines of code train an LDA model on the list of documents which have been stemmed, lower-cased and have stopwords removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27de8e4fa85536bb396b73bfc51b3f50",
     "grade": false,
     "grade_id": "cell-021a48dff4a8bb91",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "class LdaRetrievalModel(VectorSpaceRetrievalModel):\n",
    "    def __init__(self, doc_repr):\n",
    "        super().__init__(doc_repr)\n",
    "        \n",
    "        # use these parameters in the train_model method\n",
    "        self.num_topics = 100\n",
    "        self.chunksize = 2000\n",
    "        self.passes = 20\n",
    "        self.iterations = 400\n",
    "        self.eval_every = 10\n",
    "        # this is need to get full vectors\n",
    "        self.minimum_probability=0.0\n",
    "        self.alpha='auto'\n",
    "        self.eta='auto'\n",
    "    \n",
    "    \n",
    "    def train_model(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be70fcb8098d0b6ce64cd2a10e6a05b7",
     "grade": true,
     "grade_id": "cell-86750b715f0345fd",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Function check\n",
    "lda = LdaRetrievalModel(doc_repr_2)\n",
    "lda.train_model()\n",
    "\n",
    "# you can now get an LDA vector for a given query in the following way:\n",
    "lda.vectorize_query(\"report\")\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32d83b6ea79ca3ddb789a7f8805a1b25",
     "grade": false,
     "grade_id": "cell-0e24b727d5908c0e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "114a934f0b9ce696e6cf09d3b8da6a3d",
     "grade": false,
     "grade_id": "cell-b1bffcb970b18aeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "Now we can use the `DenseRetrievalModel` class to obtain an LDA search function.\n",
    "You can test your LDA model in the following cell: Try finding queries which are lexically different to documents, but semantically similar - does LDA work well for these queries?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drm_lda = DenseRetrievalRanker(lda, jenson_shannon_sim)\n",
    "\n",
    "# test your LDA model\n",
    "search_fn = drm_lda.search\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "\n",
    "text.on_submit(handle_submit_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d7f15863b655119b45f4d89354e5661",
     "grade": false,
     "grade_id": "cell-190cd0854b2791cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 8: Word2Vec/Doc2Vec (20 points) <a class=\"anchor\" id=\"2vec\"></a>\n",
    "\n",
    "[Back to Part 2](#part2)\n",
    "\n",
    "We will implement two other methods here, the Word2Vec model and the Doc2Vec model, also using `gensim`. Word2Vec creates representations of words, not documents, so the word level vectors need to be aggregated to obtain a representation for the document. Here, we will simply take the mean of the vectors. \n",
    "\n",
    "\n",
    "A drawback of these models is that they need a lot of training data. Our dataset is tiny, so in addition to using a model trained on the data, we will also use a pre-trained model for Word2Vec (this will be automatically downloaded).     \n",
    "\n",
    "*Note*:\n",
    "1. The code in vectorize_documents / vectorize_query should return gensim-like vectors i.e `[(dim, val), .. (dim, val)]`. \n",
    "2. For Word2Vec: You should also handle the following two cases: (a) A word in the query is not present in the vocabulary of the model and (b) none of the words in the query are present in the model - you can return 0 scores for all documents in this case. For either of these, you can check if a `word` is present in the vocab by using `word in self.model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83ab733608ed14c29c09b36b4e1b6daa",
     "grade": false,
     "grade_id": "cell-2b73759f9baf688f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "class W2VRetrievalModel(VectorSpaceRetrievalModel):\n",
    "    def __init__(self, doc_repr):\n",
    "        super().__init__(doc_repr)\n",
    "        \n",
    "        # the dimensionality of the vectors\n",
    "        self.size = 100 \n",
    "        self.min_count = 1\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Trains the W2V model\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def vectorize_documents(self):\n",
    "        \"\"\"\n",
    "            Returns a doc_id -> vector dictionary\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def vectorize_query(self, query):\n",
    "        \"\"\"\n",
    "        Vectorizes the query using the W2V model\n",
    "        \"\"\"\n",
    "        query = process_text(query, **config_2)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "class W2VPretrainedRetrievalModel(W2VRetrievalModel):\n",
    "    def __init__(self, doc_repr):\n",
    "        super().__init__(doc_repr)\n",
    "        self.model_name = \"word2vec-google-news-300\"\n",
    "        self.size = 300\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Loads the pretrained model\n",
    "        \"\"\"\n",
    "        self.model = g_downloader.load(self.model_name)\n",
    "\n",
    "w2v = W2VRetrievalModel(doc_repr_2)\n",
    "w2v.train_model()\n",
    "\n",
    "# you can now get a W2V vector for a given query in the following way:\n",
    "w2v.vectorize_query(\"report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f92b5c5a8c6c4b80652b94223209ab0b",
     "grade": true,
     "grade_id": "cell-b31c0f8d214b8bdf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(w2v.vectorize_query(\"report\")) == 100\n",
    "assert len(w2v.vectorize_query(\"this is a sentence that is not mellifluous\")) == 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8dfaabebcb06f308a7ca61fdc5d369e7",
     "grade": false,
     "grade_id": "cell-c2614fa067386384",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_pretrained = W2VPretrainedRetrievalModel(doc_repr_2)\n",
    "w2v_pretrained.train_model()\n",
    "\n",
    "# you can now get an W2V vector for a given query in the following way:\n",
    "w2v_pretrained.vectorize_query(\"report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0822845afb5eafe5ddb1ffeaa4f4942a",
     "grade": true,
     "grade_id": "cell-1b1466f8ce516f42",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Function check\n",
    "\n",
    "print(len(w2v_pretrained.vectorize_query(\"report\")))\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drm_w2v = DenseRetrievalRanker(w2v, cosine_sim)\n",
    "\n",
    "# test your LDA model\n",
    "search_fn = drm_w2v.search\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "\n",
    "text.on_submit(handle_submit_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drm_w2v_pretrained = DenseRetrievalRanker(w2v_pretrained, cosine_sim)\n",
    "\n",
    "# test your LDA model\n",
    "search_fn = drm_w2v_pretrained.search\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "\n",
    "text.on_submit(handle_submit_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51b778984fd60757974f51047c61eb15",
     "grade": false,
     "grade_id": "cell-b92f701cbc706108",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (10 points):**\n",
    "For Doc2Vec, you will need to create a list of `TaggedDocument` instead of using the `self.corpus` or `self.documents` variable. Use the document id as the 'tag'.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f60fdeb97febb7f4a6fd5bf109aac20",
     "grade": false,
     "grade_id": "cell-680facdcc98a19ab",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "class D2VRetrievalModel(VectorSpaceRetrievalModel):\n",
    "    def __init__(self, doc_repr):\n",
    "        super().__init__(doc_repr)\n",
    "        \n",
    "        self.vector_size= 100\n",
    "        self.min_count = 1\n",
    "        self.epochs = 20\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def train_model(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def vectorize_documents(self):\n",
    "        \"\"\"\n",
    "            Returns a doc_id -> vector dictionary\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def vectorize_query(self, query):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "d2v = D2VRetrievalModel(doc_repr_2)\n",
    "d2v.train_model()\n",
    "\n",
    "\n",
    "# # you can now get an LSI vector for a given query in the following way:\n",
    "d2v.vectorize_query(\"report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e83a363a9d4f136efbdde1426a83925e",
     "grade": true,
     "grade_id": "cell-5e2c5e0c9a2e8cb5",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bb46bf6b9be1e0ca66f0b0bc6260ecb",
     "grade": false,
     "grade_id": "cell-8a49d414f798a595",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drm_d2v = DenseRetrievalRanker(d2v, cosine_sim)\n",
    "\n",
    "# test your LDA model\n",
    "search_fn = drm_d2v.search\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "\n",
    "text.on_submit(handle_submit_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "363ec36c1d03d9f9e1c2045a6e193c14",
     "grade": false,
     "grade_id": "cell-3529ae29eece7b97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Section 9: Re-ranking (10 points) <a class=\"anchor\" id=\"reranking\"></a>\n",
    "\n",
    "[Back to Part 2](#part2)\n",
    "\n",
    "To motivate the re-ranking perspective (i.e retrieve with lexical method + rerank with a semantic method), let's search using semantic methods and compare it to BM25's performance, along with their runtime:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5755f70e3eb28abc65d14d80125338af",
     "grade": false,
     "grade_id": "cell-f8f43bf5ae383128",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "query = \"algebraic functions\"\n",
    "print(\"BM25: \")\n",
    "%timeit bm25_search(query, 2)\n",
    "print(\"LSI: \")\n",
    "%timeit drm_lsi.search(query)\n",
    "print(\"LDA: \")\n",
    "%timeit drm_lda.search(query)\n",
    "print(\"W2V: \")\n",
    "%timeit drm_w2v.search(query)\n",
    "print(\"W2V(Pretrained): \")\n",
    "%timeit drm_w2v_pretrained.search(query)\n",
    "print(\"D2V:\")\n",
    "%timeit drm_d2v.search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae398da0a8c23c95bcbb0023b7ec6f34",
     "grade": false,
     "grade_id": "cell-db5ff09f97841af7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "**Implementation (10 points):**\n",
    "Re-ranking involves retrieving a small set of documents using simple but fast methods like BM25 and then re-ranking them with the aid of semantic methods such as LDA or LSI. Implement the following class, which takes in an `initial_retrieval_fn` - the initial retrieval function and `vsrm` - an instance of the `VectorSpaceRetrievalModel` class (i.e LSI/LDA) as input. The search function should first retrieve an initial list of K documents, and then these documents are re-ranked using a semantic method. This not only makes retrieval faster, but semantic methods perform poorly when used in isolation, as you will find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63b6b05a676a2ae3f08d8bed1bc59428",
     "grade": false,
     "grade_id": "cell-5bf47600d1a0c507",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "class DenseRerankingModel:\n",
    "    def __init__(self, initial_retrieval_fn, vsrm, similarity_fn):\n",
    "        \"\"\"\n",
    "            initial_retrieval_fn: takes in a query and returns a list of [(doc_id, score)] (sorted)\n",
    "            vsrm: instance of `VectorSpaceRetrievalModel`\n",
    "            similarity_fn: function instance that takes in two vectors \n",
    "                            and returns a similarity score e.g cosine_sim defined earlier\n",
    "        \"\"\"\n",
    "        self.ret = initial_retrieval_fn\n",
    "        self.vsrm = vsrm\n",
    "        self.similarity_fn = similarity_fn\n",
    "        self.vectorized_documents = vsrm.vectorize_documents()\n",
    "        \n",
    "        assert len(self.vectorized_documents) == len(doc_repr_2)\n",
    "    \n",
    "    def search(self, query, K=50):\n",
    "        \"\"\"\n",
    "            First, retrieve the top K results using the retrieval function\n",
    "            Then, re-rank the results using the VSRM instance\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "334ab5af96976265cace682ab82a7387",
     "grade": true,
     "grade_id": "cell-52c6d18a4c0b4882",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Function check\n",
    "bm25_search_2 = partial(bm25_search, index_set=2)\n",
    "lsi_rerank = DenseRerankingModel(bm25_search_2, lsi, cosine_sim)\n",
    "lda_rerank = DenseRerankingModel(bm25_search_2, lda, jenson_shannon_sim)\n",
    "w2v_rerank = DenseRerankingModel(bm25_search_2, w2v, cosine_sim)\n",
    "w2v_pretrained_rerank = DenseRerankingModel(bm25_search_2, w2v_pretrained, cosine_sim)\n",
    "d2v_rerank = DenseRerankingModel(bm25_search_2, d2v, cosine_sim)\n",
    "\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd904253f45f84e63bab3a69729058fc",
     "grade": false,
     "grade_id": "cell-93215dfe6bcf7cff",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b592e60292bfe3d9ef2930a354c4077a",
     "grade": false,
     "grade_id": "cell-aa694ff55fa91e7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "Now, let us time the new search functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "338c7e3528cba266a865a061287c0e38",
     "grade": false,
     "grade_id": "cell-5edbd481562ad91f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "query = \"algebraic functions\"\n",
    "print(\"BM25: \")\n",
    "%timeit bm25_search(query, 2)\n",
    "print(\"LSI: \")\n",
    "%timeit lsi_rerank.search(query)\n",
    "print(\"LDA: \")\n",
    "%timeit lda_rerank.search(query)\n",
    "print(\"W2V: \")\n",
    "%timeit w2v_rerank.search(query)\n",
    "print(\"W2V(Pretrained): \")\n",
    "%timeit w2v_pretrained_rerank.search(query)\n",
    "print(\"D2V:\")\n",
    "%timeit d2v_rerank.search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c45c5e3f015b2de89d9d39ae3766368b",
     "grade": false,
     "grade_id": "cell-85c50f2ab9eec301",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "As you can see, it is much faster (but BM25 is still orders of magnitude faster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e2f3388e3807659f303fe31a75a010e",
     "grade": false,
     "grade_id": "cell-5071bb99b2af61cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Section 10: Evaluation & Analysis (30 points) <a class=\"anchor\" id=\"reranking_eval\"></a>\n",
    "\n",
    "[Back to Part 2](#part2)\n",
    "\n",
    "[Previously](#evaluation) we have implemented some evaluation metrics and used them for measuring the ranking performance of term-based IR algorithms. In this section, we will do the same for semantic methods, both with and without re-ranking.\n",
    "\n",
    "### Section 10.1: Plot (10 points)\n",
    "\n",
    "First, gather the results. The results should consider the index set, the different search functions and different metrics. Plot the results in bar charts, per metric, with clear labels.\n",
    "\n",
    "Then, gather only the re-ranking models, and plot and compare them with the results obtained in part 1 (only index set 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fe81520ac6413a803838913fd64de03",
     "grade": false,
     "grade_id": "cell-b672fe6dfae0b1ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "list_of_sem_search_fns = [\n",
    "    (\"lda\", drm_lda.search),\n",
    "    (\"lsi\", drm_lsi.search),\n",
    "    (\"w2v\", drm_w2v.search),\n",
    "    (\"w2v_pretrained\", drm_w2v_pretrained.search),\n",
    "    (\"d2v\", drm_d2v.search),\n",
    "    (\"lsi_rr\", lsi_rerank.search),\n",
    "    (\"lda_rr\", lda_rerank.search),\n",
    "    (\"w2v_rr\", w2v_rerank.search),\n",
    "    (\"w2v_pretrained_rr\", w2v_pretrained_rerank.search),\n",
    "    (\"d2v_rr\", d2v_rerank.search),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54707c4afac084299aeefa047259b4a9",
     "grade": true,
     "grade_id": "cell-7dd8273b0f5a3c22",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8a3b6189bdde66704c694d85e38d049",
     "grade": false,
     "grade_id": "cell-deb2ef3daa306e82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 10.2: Summary (20 points)\n",
    "\n",
    "Your summary should compare methods from Part 1 and Part 2 (only for index set 2). State what you expected to see in the results, followed by either supporting evidence *or* justify why the results did not support your expectations. Consider the availability of data, scalability, domain/type of data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff97c43837d10bff6aaffa75e1492887",
     "grade": true,
     "grade_id": "cell-ec5dd7d9cf59dd86",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
